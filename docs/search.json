[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Replication Reports",
    "section": "",
    "text": "This companion website presents the replication reports for the paper Judging a Pitch by its Cover: Evidence on the Roles of Visual Fluency and Substantive Quality in Startup Pitch Decks From Field and Online Experiments. There are five replication reports:\n\nVC and BA Survey\nPitch Deck Benchmarking Study\nManipulation Checks\nStudy 1 (Field Experiment)\nStudy 2 (Online Experiments)\n\nThe reports are created from the raw data using R and Quarto, and they reproduce all results, tables, and figures from the paper. For the sake of readability, the code itself is folded by default in the reports and only the results relevant for the questions at hand are shown.\nEverything you see on this website is fully reproducible from the project’s GitHub repository. In addition, the pre-registrations, detailed materials, data, and code for the entire project are available at the paper’s ResearchBox."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#bar",
    "href": "about.html#bar",
    "title": "About",
    "section": "bar",
    "text": "bar\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_Online_Experiments.html",
    "href": "replication_reports/Replication_Report_3_Online_Experiments.html",
    "title": "Judging a Pitch by its Cover",
    "section": "",
    "text": "For both our fictitious startups (Software: PerkSouq; Healthcare: Brachytix), we ran online experiments to determine how the (stated) likelihood to invest depends on the substantive quality and visual fluency of a pitch deck.\nSpecifically, we first ran a 2x2 between-subjects experiment for our software startup. To make sure that the instruction we used when measuring investment likelihood (“Based on the information at hand, what is the probability […]”) did not bias the results, we ran a replication of this first experiment with a modified instruction (“Based on the pitch deck, what is the probability […]”). To ensure generalizability across domains, we then ran the same experiment for our healthcare startup.\nWe ran all online experiments on Qualtrics, hosted the pitch decks on DocSend, and recruited the participants via Prolific. For details, see the corresponding AsPredicted pre-registrations listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registrations\n\n\n\n\n\nStartup\nPre-Reg Date\nAsPredicted #\nData Collection Start\n\n\n\n\nSoftware startup\n28-11-2022\n114380\n30-11-2022\n\n\nSoftware startup (Replication)\n16-12-2022\n116872\n17-12-2022\n\n\nHealthcare startup\n20-12-2022\n117215\n21-12-2022\n\n\n\n\n\n\nIn what follows, we will give an overview of the results and robustness checks, followed by figures that summarize the results of the experiments. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\noptions(knitr.kable.NA = '')\n\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(ggsignif)\nlibrary(ggtext)\nlibrary(patchwork)\nlibrary(kableExtra)\noptions(knitr.kable.NA = '',\n        kable_styling_bootstrap_options = c(\"striped\", \"condensed\", \"responsive\"))\n\n# further packages that are loaded on demand are:\n# - supernova\n# - car\n# - rstatix\n# - forcats\n# - weights\n# - broom\n# - scales\n# - readr\n# - tidyr\n# - hrbrthemes\n# - emmeans\n# - grid\n\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract t-test results and Cohen's d and put the results together as a table\nttest_tbl &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", weights::rd(tres$p.value, 3))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return df\n  df = data.frame(DV = NA, condition=rep(NA, 2), N = NA, Mean = NA, SD = NA, test_statistic = NA, p = NA, d = NA)\n  # fill values\n  df$DV[1] &lt;- stringr::str_to_sentence(dres$`.y.`)\n  df$condition &lt;- c(dres$group1, dres$group2)\n  df$N &lt;- c(dres$n1, dres$n2)\n  df$Mean &lt;- weights::rd(aggregate(formula, data = data, FUN = mean)[,2], 2)\n  df$SD &lt;- weights::rd(aggregate(formula, data = data, FUN = sd)[,2], 3)\n  df$test_statistic[1] &lt;- paste0(\"t(\",\n                                 ifelse(var.equal == TRUE, tres$parameter,\n                                        weights::rd(tres$parameter, 1)),\n                                 \") = \",\n                                 sprintf('%.2f', tres$statistic))\n  df$p[1] &lt;- pval\n  df$d[1] &lt;- weights::rd(dres$effsize, 2)\n  return(df)\n}\n#\n# extract ANOVA results including eta squared and put the results together as a table\n# -Note: needs the supernova library installed\nanova_tbl &lt;- function(formula, data, type = 3, ...){\n  # perform ANOVA\n  d &lt;- supernova::supernova(lm(as.formula(deparse(formula)), data = data), type = type)$tbl\n  # check whether ANOVA is univariate or factorial\n  univariate &lt;- all(d$term %in% c(\"Model\", \"Error\", \"Total\"))\n  # get rows of interest\n  if(univariate) {\n    effect_rows &lt;- d$term %notin% c(\"Error\", \"Total\")\n  } else {\n    effect_rows &lt;- d$term %notin% c(\"Model\", \"Error\", \"Total\")\n  }\n  # extract key parameters\n  effect &lt;- d$term[effect_rows]\n  MSE &lt;- round(d$MS[effect_rows], 2)\n  df &lt;- d$df[effect_rows]\n  df_res &lt;- d$df[d$term == \"Error\"]\n  statistic &lt;- round(d$F[effect_rows], 2)\n  pval &lt;- ifelse(d$p[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$p[effect_rows], 3))\n  eta &lt;- ifelse(d$PRE[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$PRE[effect_rows], 3))\n  # construct return df\n  return(data.frame(effect, MSE, df, df_res, statistic, pval, eta))\n}\n# extract GLM results and put the results together as a table\nglm_tbl &lt;- function(model, coef_digits = 2, coef_bold = TRUE, p_threshold = 0.05, ...){\n  # extract model parameters\n  if(\"tobit\" %in% class(model)){ # tobit model -&gt; broom::tidy does not work\n    res &lt;- parameters::model_parameters(model)\n    res &lt;- res[c(\"Parameter\", \"Coefficient\", \"SE\", \"z\", \"p\")]\n    names(res) &lt;- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n    # res[] &lt;- lapply(res, function(x) { attributes(x) &lt;- NULL; x })\n  } else {\n    res &lt;- broom::tidy(model)\n  }\n  pvals &lt;- res$p.value\n  res$estimate &lt;- sprintf(paste0(\"%.\", coef_digits, \"f\"), res$estimate)\n  res$std.error &lt;- sprintf(\"%.3f\", res$std.error)\n  res$statistic &lt;- sprintf(\"%.2f\", res$statistic)\n  # format p value\n  res$p.value &lt;- ifelse(res$p.value &lt; .001, \" &lt; .001\", weights::rd(res$p.value, 3))\n  # make estimates bold if below critical p value\n  if(coef_bold){\n    res$estimate[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$estimate[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$std.error[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$std.error[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$statistic[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$statistic[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$p.value[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$p.value[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n  }\n  # bind R2 and Adj. R2 to model parameters\n  r2 &lt;- performance::r2(model) # extract R2\n  end &lt;- nrow(res) + seq_len(length(r2))\n  res[end,\"term\"] &lt;- names(r2)\n  res[end,\"estimate\"] &lt;- weights::rd(unlist(r2), digits = 3)\n  # return result\n  return(res)\n}"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_Online_Experiments.html#main-analyses",
    "href": "replication_reports/Replication_Report_3_Online_Experiments.html#main-analyses",
    "title": "Judging a Pitch by its Cover",
    "section": "4.1 Main analyses",
    "text": "4.1 Main analyses\nTable 5 shows for each experiment the results of a factorial ANOVA that models stated investment likelihood as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\n\n\nCode\n# combine data\nd &lt;- bind_rows(list(\n  Software = d_sw,\n  `Software Replication` = d_sw_repl,\n  Healthcare = d_hc),\n  .id = \"Startup\")\n\n# convert Startup to factor, change order of levels\nd$Startup &lt;- factor(d$Startup,\n                    levels = c(\"Software\", \"Software Replication\", \"Healthcare\"),\n                    labels = c(\"Software\", \"Software (replication)\", \"Healthcare\"))\n\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n\n# combine results\ntemp &lt;- bind_rows(\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software\")),\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software (replication)\")),\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Healthcare\"))\n  )\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 3)) |&gt;\n  kbl(col.names = c(\"\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling() |&gt; \n  pack_rows(index = c(\"Software Startup\" = 3,\n                      \"Software Startup (Replication)\" = 3,\n                      \"Healthcare Startup\" = 3),\n            label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0,3,6,9), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 5: ANOVA results for stated investment likelihood\n\n\n\n\n\n\n\n\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nSoftware Startup\n\n\nFluency\n559.81\n1\n193\n0.77\n.380\n.004\n\n\nQuality\n3055.37\n1\n193\n4.22\n.041\n.021\n\n\nFluency × Quality\n232.63\n1\n193\n0.32\n.571\n.002\n\n\nSoftware Startup (Replication)\n\n\nFluency\n202.33\n1\n196\n0.27\n.604\n.001\n\n\nQuality\n8433.54\n1\n196\n11.28\n&lt; .001\n.054\n\n\nFluency × Quality\n30.14\n1\n196\n0.04\n.841\n&lt; .001\n\n\nHealthcare Startup\n\n\nFluency\n2451.79\n1\n189\n3.35\n.069\n.017\n\n\nQuality\n3813.02\n1\n189\n5.21\n.024\n.027\n\n\nFluency × Quality\n1104.61\n1\n189\n1.51\n.221\n.008"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_Online_Experiments.html#sec-mc",
    "href": "replication_reports/Replication_Report_3_Online_Experiments.html#sec-mc",
    "title": "Judging a Pitch by its Cover",
    "section": "4.2 Manipulation checks",
    "text": "4.2 Manipulation checks\nIn this section, we report the results of the manipulation checks for visual fluency and substantive quality (software startup replication and healthcare startup experiments). We conducted t-tests for each manipulation check, with either perceived fluency or perceived quality as the dependent variable and the visual fluency condition or the substantive quality condition as independent variables. The results are shown in Table 6 (a) for perceived fluency and Table 6 (b) for perceived quality. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\n# perceived fluency\nbind_rows(\n  ttest_tbl(fluency_perception ~ fluency, d |&gt; filter(Startup == \"Software (replication)\")),\n  ttest_tbl(fluency_perception ~ fluency, d |&gt; filter(Startup == \"Healthcare\"))\n) |&gt;\n  mutate(DV = rep(c(\"Perceived fluency\", \"\"), 2)) |&gt;\n  kbl(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"), escape = FALSE) |&gt;\n  kable_styling() |&gt;\n  pack_rows(index = c(\"Software Startup (Replication)\" = 2, \"Healthcare Startup\" = 2), label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0, 2), extra_css = \"border-bottom: 1px solid\")\n# perceived quality\nbind_rows(\n  ttest_tbl(quality_perception ~ quality, d |&gt; filter(Startup == \"Software (replication)\")),\n  ttest_tbl(quality_perception ~ quality, d |&gt; filter(Startup == \"Healthcare\"))\n) |&gt;\n  mutate(DV = rep(c(\"Perceived quality\", \"\"), 2)) |&gt;\n  kbl(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"), escape = FALSE) |&gt;\n  kable_styling() |&gt;\n  pack_rows(index = c(\"Software Startup (Replication)\" = 2, \"Healthcare Startup\" = 2), label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0, 2), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 6: Manipulation checks (software startup replication and healthcare startup)\n\n\n\n\n\n\n\n(a) Perceived fluency\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen's d\n\n\n\n\nSoftware Startup (Replication)\n\n\nPerceived fluency\nhigh\n101\n63.80\n26.711\nt(198) = 2.03\n.043\n.29\n\n\n\nlow\n99\n55.83\n28.749\n\n\n\n\n\nHealthcare Startup\n\n\nPerceived fluency\nhigh\n103\n58.79\n28.051\nt(191) = 1.99\n.048\n.29\n\n\n\nlow\n90\n50.33\n30.871\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Perceived quality\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen's d\n\n\n\n\nSoftware Startup (Replication)\n\n\nPerceived quality\nhigh\n107\n4.73\n1.225\nt(198) = 2.34\n.020\n.33\n\n\n\nlow\n93\n4.33\n1.155\n\n\n\n\n\nHealthcare Startup\n\n\nPerceived quality\nhigh\n97\n5.23\n.952\nt(191) = 3.48\n&lt; .001\n.50\n\n\n\nlow\n96\n4.70\n1.153"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_Online_Experiments.html#sec-robustness",
    "href": "replication_reports/Replication_Report_3_Online_Experiments.html#sec-robustness",
    "title": "Judging a Pitch by its Cover",
    "section": "4.3 Robustness checks",
    "text": "4.3 Robustness checks\nAs a robustness check, we estimated linear regression models in which we predicted the stated investment likelihood by visual fluency, substantive quality, age, gender, investment experience and the aesthetic sensitivity (CVPA composite score). In this regression, we again used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1. Table 7 shows the results of this regression.\n\n\nCode\n# -- Note: We comment out Levene's tests of homogeneous group variances here for\n#          readability. However, we adapt the function call of the regression\n#          based on the result of Levene's tests.\n\n\n# Levene's test of homogeneous group variances -&gt; if not, use robust::lmRob() below\n# \n# # software startup\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Software\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Software\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software\")) # not significant\n# # software startup replication\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# # healthcare startup\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n\n# recode Likert text scores into numeric values for the CVPA score\nd |&gt; mutate_at(vars(cvpa_v_1:cvpa_r_3), dplyr::recode,\n                   'Strongly disagree' = 1,\n                   'Disagree' = 2,\n                   'Neither agree nor disagree' = 3,\n                   'Agree' = 4,\n                   'Strongly agree' = 5) -&gt; d\n# create CVPA composite score\nd &lt;- d |&gt; rowwise() |&gt; mutate(CVPA = mean(cvpa_v_1:cvpa_r_3))\n\n# group \"Non-binary\" and \"Prefer not to say\" gender categories into one\nd$gender &lt;- forcats::fct_collapse(d$gender, Female = \"Female\", Male = \"Male\", other_level = \"Other\")\n\n# run regression\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Software\"))) -&gt; lm_rob_sw\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Software (replication)\"))) -&gt; lm_rob_sw_repl\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Healthcare\"))) -&gt; lm_rob_hc\n\n# add empty row for `genderOther` in the first and third regression\nlm_rob_sw |&gt; add_row(.before = 6)  -&gt; lm_rob_sw\nlm_rob_hc |&gt; add_row(.before = 6)  -&gt; lm_rob_hc\n\n\ntemp &lt;- bind_cols(lm_rob_sw, lm_rob_sw_repl[,-1], lm_rob_hc[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Age\",\n               \"Gender [Male]\", \"Gender [Other]\",\n               \"Investment experience\", \"Aesthetic sensitivity\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 3))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Software Startup\" = 4, \"Software Startup (Replication)\" = 4, \"Healthcare Startup\" = 4)) |&gt;\n  row_spec(c(0,9), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 7: Robustness checks: Linear regressions for stated investment likelihood with individual-level control variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Startup\n\n\nSoftware Startup (Replication)\n\n\nHealthcare Startup\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n34.99\n10.432\n3.35\n&lt; .001\n4.86\n10.538\n0.46\n.645\n37.19\n10.670\n3.49\n&lt; .001\n\n\nFluency\n-2.12\n1.781\n-1.19\n.236\n1.54\n1.831\n0.84\n.401\n3.59\n1.875\n1.91\n.057\n\n\nQuality\n4.48\n1.790\n2.50\n.013\n6.18\n1.831\n3.38\n&lt; .001\n4.27\n1.891\n2.26\n.025\n\n\nFluency × Quality\n-0.01\n1.804\n-0.01\n.996\n-0.34\n1.847\n-0.18\n.855\n1.51\n1.883\n0.80\n.423\n\n\nAge\n-0.47\n0.165\n-2.82\n.005\n-0.01\n0.165\n-0.03\n.975\n-0.30\n0.182\n-1.65\n.102\n\n\nGender [Male]\n-3.90\n3.703\n-1.05\n.294\n0.32\n4.029\n0.08\n.938\n-2.06\n4.045\n-0.51\n.611\n\n\nGender [Other]\n\n\n\n\n-2.19\n13.260\n-0.17\n.869\n\n\n\n\n\n\nInvestment experience\n-0.12\n0.299\n-0.39\n.695\n-0.06\n0.294\n-0.21\n.830\n-0.14\n0.399\n-0.34\n.732\n\n\nAesthetic sensitivity\n8.80\n1.981\n4.44\n&lt; .001\n11.79\n2.028\n5.82\n&lt; .001\n8.58\n2.047\n4.19\n&lt; .001\n\n\nR2\n.188\n\n\n\n.202\n\n\n\n.164\n\n\n\n\n\nR2adj.\n.157\n\n\n\n.168\n\n\n\n.133"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html",
    "title": "Judging a Pitch by its Cover",
    "section": "",
    "text": "In our main study, we ran a field experiment in which we varied the visual fluency and the substantive quality of pitch decks of our two fictitious startups (Software: PerkSouq; Healthcare: Brachytix).\nSpecifically, we sent out a standardized email to 39,977 potential investors (24,961 for the software startup and 15,016 for healthcare startup), and tracked whether the potential investor clicks on the link to the pitch deck. Every investor who clicked the link within 21 days of receiving the email was potentially part of our sample (pending exclusion restrictions). All data that was recorded within 21 days after an investor clicked on the link to the pitch deck was considered for analysis.\nDepending on their previous investment record, investors were matched to either the software startup or healthcare startup, then randomly assigned to one of the four experimental conditions. We pretested all the manipulations. We tracked whether an investor clicked on the link to the pitch deck, the cumulative time the pitch deck remained open, the percentage of slides viewed, and whether there was a positive reaction to the email. A reaction was considered positive if a meeting appointment was scheduled over a link in the deck and / or an email reply has been sent that clearly demonstrated investor interest.1 Participants were not aware that they took part in a scientific study and that the startups were fictitious.\nFor more details, e.g., on the exclusion criteria that we used, see the corresponding AsPredicted pre-registration listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registration\n\n\n\n\n\nStartup\nPre-Reg Date\nAsPredicted #\nData Collection Start\n\n\n\n\nPerkSouq & Brachytix\n13-01-2023\n118675\n16-01-2023\n\n\n\n\n\n\nIn what follows, we will give an overview of the results, separately for each startup, followed by figures that summarize the results of the field experiment. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(ggsankey)\nlibrary(ggsignif)\nlibrary(patchwork)\nlibrary(kableExtra)\noptions(knitr.kable.NA = '',\n        kable_styling_bootstrap_options = c(\"striped\", \"condensed\", \"responsive\"))\n\n# further packages that are loaded on demand are:\n# - supernova\n# - weights\n# - parameters\n# - broom\n# - scales\n# - performance\n# - readr\n# - lubridate\n# - stringr\n# - tidyr\n# - irr\n# - AER\n# - hrbrthemes\n# - grid\n# - gridExtra\n\n\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract ANOVA results including eta squared and put the results together as a table\n# -Note: needs the supernova library installed\nanova_tbl &lt;- function(formula, data, type = 3, ...){\n  # perform ANOVA\n  d &lt;- supernova::supernova(lm(as.formula(deparse(formula)), data = data), type = type)$tbl\n  # check whether ANOVA is univariate or factorial\n  univariate &lt;- all(d$term %in% c(\"Model\", \"Error\", \"Total\"))\n  # get rows of interest\n  if(univariate) {\n    effect_rows &lt;- d$term %notin% c(\"Error\", \"Total\")\n  } else {\n    effect_rows &lt;- d$term %notin% c(\"Model\", \"Error\", \"Total\")\n  }\n  # extract key parameters\n  effect &lt;- d$term[effect_rows]\n  MSE &lt;- round(d$MS[effect_rows], 2)\n  df &lt;- d$df[effect_rows]\n  df_res &lt;- d$df[d$term == \"Error\"]\n  statistic &lt;- round(d$F[effect_rows], 2)\n  pval &lt;- ifelse(d$p[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$p[effect_rows], 3))\n  eta &lt;- ifelse(d$PRE[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$PRE[effect_rows], 3))\n  # construct return df\n  return(data.frame(effect, MSE, df, df_res, statistic, pval, eta))\n}\n# extract GLM results and put the results together as a table\nglm_tbl &lt;- function(model, coef_digits = 2, coef_bold = TRUE, p_threshold = 0.05, ...){\n  # extract model parameters\n  if(\"tobit\" %in% class(model)){ # tobit model -&gt; broom::tidy does not work\n    res &lt;- parameters::model_parameters(model)\n    res &lt;- res[c(\"Parameter\", \"Coefficient\", \"SE\", \"z\", \"p\")]\n    names(res) &lt;- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n    # res[] &lt;- lapply(res, function(x) { attributes(x) &lt;- NULL; x })\n  } else {\n    res &lt;- broom::tidy(model)\n  }\n  pvals &lt;- res$p.value\n  res$estimate &lt;- sprintf(paste0(\"%.\", coef_digits, \"f\"), res$estimate)\n  res$std.error &lt;- sprintf(\"%.3f\", res$std.error)\n  res$statistic &lt;- sprintf(\"%.2f\", res$statistic)\n  # format p value\n  res$p.value &lt;- ifelse(res$p.value &lt; .001, \" &lt; .001\", weights::rd(res$p.value, 3))\n  # make estimates bold if below critical p value\n  if(coef_bold){\n    res$estimate[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$estimate[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$std.error[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$std.error[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$statistic[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$statistic[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$p.value[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$p.value[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n  }\n  # bind R2 and Adj. R2 to model parameters\n  r2 &lt;- performance::r2(model) # extract R2\n  end &lt;- nrow(res) + seq_len(length(r2))\n  res[end,\"term\"] &lt;- names(r2)\n  res[end,\"estimate\"] &lt;- weights::rd(unlist(r2), digits = 3)\n  # return result\n  return(res)\n}"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#descriptives-1",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#descriptives-1",
    "title": "Judging a Pitch by its Cover",
    "section": "4.1 Descriptives",
    "text": "4.1 Descriptives\nTable 4 shows a descriptive breakdown of cumulative opening duration, completion percentage, and positive reactions by visual fluency and substantive quality conditions.\n\n\nCode\nd &lt;- d_sw\n\nd |&gt; group_by(fluency, quality) |&gt; summarize(N = n(), Mean = mean(duration),\n      SD = sd(duration)) -&gt; temp_duration\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(completion), SD = sd(completion)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_completion\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(positive), SD = sd(positive)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_positive\n\ntemp &lt;- bind_cols(temp_duration, temp_completion, temp_positive)\nnames(temp) &lt;- c(\"Fluency\", \"Quality\", \"N\", \"Mean\", \"SD\", \"Mean\", \"SD\", \"Mean\", \"SD\")\ntemp |&gt; kbl(digits = 3, format.args = list(decimal.mark = \".\", big.mark = \",\")) |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 3, \"Opening Duration\" = 2, \"Completion Percentage\" = 2, \"Positive Reaction\" = 2))\n\n\n\n\nTable 4: Descriptive statistics (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\nPositive Reaction\n\n\n\nFluency\nQuality\nN\nMean\nSD\nMean\nSD\nMean\nSD\n\n\n\n\nhigh\nhigh\n1,091\n181.742\n167.906\n0.728\n0.278\n0.122\n0.327\n\n\nhigh\nlow\n1,080\n146.706\n127.484\n0.690\n0.272\n0.084\n0.278\n\n\nlow\nhigh\n1,126\n148.829\n131.286\n0.687\n0.283\n0.083\n0.277\n\n\nlow\nlow\n1,146\n94.798\n84.112\n0.613\n0.313\n0.024\n0.154"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage",
    "title": "Judging a Pitch by its Cover",
    "section": "4.2 Opening duration, completion percentage",
    "text": "4.2 Opening duration, completion percentage\nTable 5 shows the result of two factorial ANOVAs that model cumulative opening duration and completion percentage as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\nAdditionally, Table 6 shows the results of robustness check regressions in which we included investment experience, investor type, gender, and country as control variables in addition to fluency, quality, and their interaction (significant values with p &lt; .05 are printed in boldface). In all analyses, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n# combine results\ntemp &lt;- bind_rows(`Opening duration` = anova_tbl(duration ~ fluency * quality, d),\n                  `Completion percentage` = anova_tbl(completion ~ fluency * quality, d),\n                  .id = \"Measure\")\n\n# keep only first occurrence of each measure\ntemp$Measure[duplicated(temp$Measure)] &lt;- NA\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 2)) |&gt;\n  kbl(col.names = c(\"Measure\", \"Effect\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling()\n\n\n\n\nTable 5: ANOVA results for opening duration and completion (software startup)\n\n\n\n\n\n\n\nMeasure\nEffect\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nOpening duration\nFluency\n1996756.35\n1\n4439\n117.00\n&lt; .001\n.026\n\n\n\nQuality\n2201670.64\n1\n4439\n129.01\n&lt; .001\n.028\n\n\n\nFluency × Quality\n100126.67\n1\n4439\n5.87\n.015\n.001\n\n\nCompletion percentage\nFluency\n3.89\n1\n4439\n47.15\n&lt; .001\n.011\n\n\n\nQuality\n3.47\n1\n4439\n42.04\n&lt; .001\n.009\n\n\n\nFluency × Quality\n0.35\n1\n4439\n4.28\n.039\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# set reference levels\nd$location &lt;- relevel(as.factor(d$location), \"United States\")\nd$gender &lt;- relevel(as.factor(d$gender), \"female\")\nd$type &lt;- relevel(as.factor(d$type), \"Angel\")\n\n\n# run regressions\nglm_tbl(lm(duration ~ fluency * quality + investments + type + gender + location, d)) -&gt; temp_duration\nglm_tbl(lm(completion ~ fluency * quality + investments + type + gender + location, d), coef_digits = 3) -&gt; temp_completion\n\ntemp &lt;- bind_cols(temp_duration, temp_completion[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Opening Duration\" = 4, \"Completion Percentage\" = 4)) |&gt;\n  row_spec(16, extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 6: Robustness check regressions for opening duration and completion percentage with control variables (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n150.54\n6.614\n22.76\n&lt; .001\n0.671\n0.015\n46.15\n&lt; .001\n\n\nFluency\n21.32\n1.961\n10.87\n&lt; .001\n0.030\n0.004\n6.89\n&lt; .001\n\n\nQuality\n22.36\n1.964\n11.38\n&lt; .001\n0.028\n0.004\n6.48\n&lt; .001\n\n\nFluency × Quality\n-4.89\n1.962\n-2.49\n.013\n-0.009\n0.004\n-2.08\n.037\n\n\nInvestment experience\n0.13\n0.545\n0.24\n.809\n0.001\n0.001\n0.83\n.405\n\n\nInvestor type [Venture Capital]\n1.81\n5.206\n0.35\n.728\n-0.002\n0.011\n-0.19\n.852\n\n\nGender [Male]\n-8.22\n6.724\n-1.22\n.221\n0.004\n0.015\n0.30\n.761\n\n\nCountry [Brazil]\n-4.54\n25.722\n-0.18\n.860\n0.039\n0.057\n0.69\n.491\n\n\nCountry [Canada]\n8.39\n13.662\n0.61\n.539\n0.022\n0.030\n0.73\n.465\n\n\nCountry [China]\n-26.29\n28.027\n-0.94\n.348\n-0.050\n0.062\n-0.81\n.416\n\n\nCountry [France]\n-9.67\n15.473\n-0.63\n.532\n0.024\n0.034\n0.69\n.489\n\n\nCountry [Germany]\n-0.27\n13.134\n-0.02\n.983\n0.057\n0.029\n1.96\n.050\n\n\nCountry [India]\n12.18\n10.931\n1.11\n.265\n0.017\n0.024\n0.70\n.487\n\n\nCountry [Israel]\n-23.51\n17.968\n-1.31\n.191\n-0.014\n0.040\n-0.36\n.723\n\n\nCountry [Singapore]\n-43.21\n18.088\n-2.39\n.017\n-0.030\n0.040\n-0.75\n.455\n\n\nCountry [United Kingdom]\n-1.83\n7.581\n-0.24\n.809\n0.003\n0.017\n0.17\n.864\n\n\nR2\n.057\n\n\n\n.023\n\n\n\n\n\nR2adj.\n.054\n\n\n\n.019"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#positive-reactions",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#positive-reactions",
    "title": "Judging a Pitch by its Cover",
    "section": "4.3 Positive reactions",
    "text": "4.3 Positive reactions\nA logistic regression model was estimated to analyze the effects of visual fluency, substantive quality, and their interaction on whether there was a positive reaction to the emails. Table 7 shows the result of this regression model, next to several robustness check models that included control variables and / or were specified as Tobit models.\nIn all regressions, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# run regressions\nglm_tbl(glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive\nglm_tbl(glm(positive ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive_controls\nglm_tbl(AER::tobit(positive ~ fluency * quality, data = d), coef_digits = 3) -&gt; temp_positive_tobit\nglm_tbl(AER::tobit(positive ~ fluency * quality + investments + type + gender + location, data = d), coef_digits = 3) -&gt; temp_positive_controls_tobit\n\n\n# add empty rows to models w/o controls to enable column binding\nrows_control &lt;- nrow(temp_positive_controls)\nrows_simple &lt;- nrow(temp_positive)\ntemp_positive[(rows_simple+1):rows_control,] &lt;- NA\ntemp_positive_tobit[(rows_simple+1):rows_control,] &lt;- NA\n# put interaction and R2 row to the end\ntemp_positive[rows_control-1,] &lt;- temp_positive[rows_simple-1,]\ntemp_positive[rows_control,] &lt;- temp_positive[rows_simple,]\ntemp_positive[rows_simple-1,] &lt;- NA\ntemp_positive[rows_simple,] &lt;- NA\ntemp_positive_tobit[rows_control-1,] &lt;- temp_positive_tobit[rows_simple-1,]\ntemp_positive_tobit[rows_control,] &lt;- temp_positive_tobit[rows_simple,]\ntemp_positive_tobit[rows_simple-1,] &lt;- NA\ntemp_positive_tobit[rows_simple,] &lt;- NA\n\n# table binary logit\ntemp &lt;- bind_cols(temp_positive, temp_positive_controls[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Binary Logit\" = 4, \"Binary Logit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n# table tobit\ntemp &lt;- bind_cols(temp_positive_tobit, temp_positive_controls_tobit[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Nagelkerke's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Tobit Model\" = 4, \"Tobit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 7: Binary logit and Tobit regressions for positive reactions (software startup)\n\n\n\n\n\n\n\n(a) Binary logit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logit\n\n\nBinary Logit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.611\n0.066\n-39.82\n&lt; .001\n-2.634\n0.197\n-13.38\n&lt; .001\n\n\nFluency\n0.431\n0.066\n6.57\n&lt; .001\n0.431\n0.066\n6.56\n&lt; .001\n\n\nQuality\n0.426\n0.066\n6.49\n&lt; .001\n0.426\n0.066\n6.49\n&lt; .001\n\n\nFluency × Quality\n-0.220\n0.066\n-3.35\n&lt; .001\n-0.219\n0.066\n-3.34\n&lt; .001\n\n\nInvestment experience\n\n\n\n\n0.014\n0.012\n1.12\n.263\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.031\n0.147\n0.21\n.835\n\n\nGender [Male]\n\n\n\n\n0.013\n0.198\n0.07\n.947\n\n\nCountry [Brazil]\n\n\n\n\n-0.731\n1.026\n-0.71\n.476\n\n\nCountry [Canada]\n\n\n\n\n0.062\n0.379\n0.16\n.869\n\n\nCountry [China]\n\n\n\n\n-0.752\n1.030\n-0.73\n.466\n\n\nCountry [France]\n\n\n\n\n-0.410\n0.522\n-0.79\n.432\n\n\nCountry [Germany]\n\n\n\n\n0.129\n0.358\n0.36\n.718\n\n\nCountry [India]\n\n\n\n\n-0.406\n0.371\n-1.09\n.274\n\n\nCountry [Israel]\n\n\n\n\n0.445\n0.416\n1.07\n.284\n\n\nCountry [Singapore]\n\n\n\n\n0.580\n0.416\n1.39\n.164\n\n\nCountry [United Kingdom]\n\n\n\n\n-0.286\n0.241\n-1.19\n.236\n\n\nTjur's R2\n.017\n\n\n\n.020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tobit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTobit Model\n\n\nTobit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.708\n0.154\n-17.56\n&lt; .001\n-2.746\n0.229\n-11.98\n&lt; .001\n\n\nFluency\n0.370\n0.057\n6.49\n&lt; .001\n0.372\n0.057\n6.51\n&lt; .001\n\n\nQuality\n0.366\n0.057\n6.41\n&lt; .001\n0.364\n0.057\n6.38\n&lt; .001\n\n\nFluency × Quality\n-0.176\n0.055\n-3.18\n.001\n-0.176\n0.055\n-3.19\n.001\n\n\nInvestment experience\n\n\n\n\n0.012\n0.012\n0.98\n.326\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.017\n0.133\n0.13\n.900\n\n\nGender [Male]\n\n\n\n\n0.042\n0.179\n0.23\n.816\n\n\nCountry [Brazil]\n\n\n\n\n-0.631\n0.837\n-0.75\n.451\n\n\nCountry [Canada]\n\n\n\n\n0.084\n0.344\n0.24\n.808\n\n\nCountry [China]\n\n\n\n\n-0.625\n0.847\n-0.74\n.460\n\n\nCountry [France]\n\n\n\n\n-0.379\n0.451\n-0.84\n.400\n\n\nCountry [Germany]\n\n\n\n\n0.193\n0.322\n0.60\n.549\n\n\nCountry [India]\n\n\n\n\n-0.376\n0.320\n-1.18\n.240\n\n\nCountry [Israel]\n\n\n\n\n0.391\n0.404\n0.97\n.333\n\n\nCountry [Singapore]\n\n\n\n\n0.547\n0.402\n1.36\n.173\n\n\nCountry [United Kingdom]\n\n\n\n\n-0.237\n0.210\n-1.13\n.260\n\n\nNagelkerke's R2\n.040\n\n\n\n.044"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#percentage-increases",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#percentage-increases",
    "title": "Judging a Pitch by its Cover",
    "section": "4.4 Percentage increases",
    "text": "4.4 Percentage increases\nTo facilitate the interpretation of the results, we calculated the percentage increase in the dependent variables for the high level of visual fluency and substantive quality compared to the respective low level. Table 8 shows these percentage increases for the opening duration, completion percentage, and positive reactions.\n\n\nCode\n# calculate percentage increases based on marginal means\n#\n# compute marginal means using the `marginaleffects` package\n#\n# duration\nm_dur &lt;- aov(duration ~ fluency * quality, d)\nmm_dur_flu &lt;- marginaleffects::predictions(m_dur, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_dur_qual &lt;- marginaleffects::predictions(m_dur, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# completion\nm_compl &lt;- aov(completion ~ fluency * quality, d)\nmm_compl_flu &lt;- marginaleffects::predictions(m_compl, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_compl_qual &lt;- marginaleffects::predictions(m_compl, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# positive reactions\nm_pos &lt;- glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), data = d)\nmm_pos_flu &lt;- marginaleffects::predictions(m_pos, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_pos_qual &lt;- marginaleffects::predictions(m_pos, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n\n# compute effect sizes as percentage increases for high vs. low levels\n#\n# we use the formula: 100 * (high / low - 1)\n# That is, we multiply by 100 to get the percentage increase and then\n# subtract 1 to get the percentage increase relative to the low level.\n#\n# duration: fluency\nperc_incr_dur_flu &lt;- 100 * (mm_dur_flu$estimate[mm_dur_flu$fluency == \"high\"] / mm_dur_flu$estimate[mm_dur_flu$fluency == \"low\"] - 1)\n# duration: quality\nperc_incr_dur_qual &lt;- 100 * (mm_dur_qual$estimate[mm_dur_qual$quality == \"high\"] / mm_dur_qual$estimate[mm_dur_qual$quality == \"low\"] - 1)\n\n# completion: fluency\nperc_incr_compl_flu &lt;- 100 * (mm_compl_flu$estimate[mm_compl_flu$fluency == \"high\"] / mm_compl_flu$estimate[mm_compl_flu$fluency == \"low\"] - 1)\n# completion: quality\nperc_incr_compl_qual &lt;- 100 * (mm_compl_qual$estimate[mm_compl_qual$quality == \"high\"] / mm_compl_qual$estimate[mm_compl_qual$quality == \"low\"] - 1)\n\n# positive: fluency\nperc_incr_pos_flu &lt;- 100 * (mm_pos_flu$estimate[mm_pos_flu$fluency == \"high\"] / mm_pos_flu$estimate[mm_pos_flu$fluency == \"low\"] - 1)\n# positive: quality\nperc_incr_pos_qual &lt;- 100 * (mm_pos_qual$estimate[mm_pos_qual$quality == \"high\"] / mm_pos_qual$estimate[mm_pos_qual$quality == \"low\"] - 1)\n\n# create table\ndata.frame(\n  Measure = c(\"Opening duration\", \"Completion percentage\", \"Positive reaction\"),\n  Fluency = c(sprintf('%.1f%%', perc_incr_dur_flu), sprintf('%.1f%%', perc_incr_compl_flu), sprintf('%.1f%%', perc_incr_pos_flu)),\n  Quality = c(sprintf('%.1f%%', perc_incr_dur_qual), sprintf('%.1f%%', perc_incr_compl_qual), sprintf('%.1f%%', perc_incr_pos_qual))\n) |&gt; kbl() |&gt; kable_styling()\n\n\n\n\nTable 8: Percentage increases for high vs. low levels of visual fluency and substantive quality (software startup)\n\n\n\n\n\n\n\nMeasure\nFluency\nQuality\n\n\n\n\nOpening duration\n34.8%\n36.9%\n\n\nCompletion percentage\n9.1%\n8.6%\n\n\nPositive reaction\n122.8%\n120.7%"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#descriptives-2",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#descriptives-2",
    "title": "Judging a Pitch by its Cover",
    "section": "5.1 Descriptives",
    "text": "5.1 Descriptives\nTable 9 shows a descriptive breakdown of cumulative opening duration, completion percentage, and positive reactions by visual fluency and substantive quality conditions.\n\n\nCode\nd &lt;- d_hc\n\nd |&gt; group_by(fluency, quality) |&gt; summarize(N = n(), Mean = mean(duration),\n      SD = sd(duration)) -&gt; temp_duration\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(completion), SD = sd(completion)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_completion\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(positive), SD = sd(positive)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_positive\n\ntemp &lt;- bind_cols(temp_duration, temp_completion, temp_positive)\nnames(temp) &lt;- c(\"Fluency\", \"Quality\", \"N\", \"Mean\", \"SD\", \"Mean\", \"SD\", \"Mean\", \"SD\")\ntemp |&gt; kbl(digits = 3) |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 3, \"Opening Duration\" = 2, \"Completion Percentage\" = 2, \"Positive Reaction\" = 2))\n\n\n\n\nTable 9: Descriptive statistics (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\nPositive Reaction\n\n\n\nFluency\nQuality\nN\nMean\nSD\nMean\nSD\nMean\nSD\n\n\n\n\nhigh\nhigh\n589\n125.431\n90.226\n0.775\n0.228\n0.171\n0.377\n\n\nhigh\nlow\n605\n100.198\n88.423\n0.688\n0.245\n0.094\n0.292\n\n\nlow\nhigh\n593\n99.042\n77.849\n0.691\n0.253\n0.099\n0.300\n\n\nlow\nlow\n620\n69.044\n64.892\n0.586\n0.294\n0.037\n0.189"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage-1",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage-1",
    "title": "Judging a Pitch by its Cover",
    "section": "5.2 Opening duration, completion percentage",
    "text": "5.2 Opening duration, completion percentage\nTable 10 shows the result of two factorial ANOVAs that model cumulative opening duration and completion percentage as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\nAdditionally, Table 11 shows the results of robustness check regressions in which we included investment experience, investor type, gender, and country as control variables in addition to fluency, quality, and their interaction (significant values with p &lt; .05 are printed in boldface). In all analyses, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n# combine results\ntemp &lt;- bind_rows(`Opening duration` = anova_tbl(duration ~ fluency * quality, d),\n                  `Completion percentage` = anova_tbl(completion ~ fluency * quality, d),\n                  .id = \"Measure\")\n\n# keep only first occurrence of each measure\ntemp$Measure[duplicated(temp$Measure)] &lt;- NA\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 2)) |&gt;\n  kbl(col.names = c(\"Measure\", \"Effect\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling()\n\n\n\n\nTable 10: ANOVA results for opening duration and completion (healthcare startup)\n\n\n\n\n\n\n\nMeasure\nEffect\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nOpening duration\nFluency\n497944.86\n1\n2403\n76.20\n&lt; .001\n.031\n\n\n\nQuality\n458729.52\n1\n2403\n70.20\n&lt; .001\n.028\n\n\n\nFluency × Quality\n3415.38\n1\n2403\n0.52\n.470\n&lt; .001\n\n\nCompletion percentage\nFluency\n5.21\n1\n2403\n79.11\n&lt; .001\n.032\n\n\n\nQuality\n5.56\n1\n2403\n84.53\n&lt; .001\n.034\n\n\n\nFluency × Quality\n0.05\n1\n2403\n0.83\n.364\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# set reference levels\nd$location &lt;- relevel(as.factor(d$location), \"United States\")\nd$gender &lt;- relevel(as.factor(d$gender), \"female\")\nd$type &lt;- relevel(as.factor(d$type), \"Angel\")\n\n\n# run regressions\nglm_tbl(lm(duration ~ fluency * quality + investments + type + gender + location, d)) -&gt; temp_duration\nglm_tbl(lm(completion ~ fluency * quality + investments + type + gender + location, d), coef_digits = 3) -&gt; temp_completion\n\ntemp &lt;- bind_cols(temp_duration, temp_completion[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Opening Duration\" = 4, \"Completion Percentage\" = 4)) |&gt;\n  row_spec(16, extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 11: Robustness check regressions for opening duration and completion percentage with control variables (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n95.01\n5.426\n17.51\n&lt; .001\n0.680\n0.017\n39.44\n&lt; .001\n\n\nFluency\n14.32\n1.656\n8.65\n&lt; .001\n0.046\n0.005\n8.83\n&lt; .001\n\n\nQuality\n13.91\n1.655\n8.40\n&lt; .001\n0.048\n0.005\n9.15\n&lt; .001\n\n\nFluency × Quality\n-1.19\n1.651\n-0.72\n.471\n-0.005\n0.005\n-0.86\n.389\n\n\nInvestment experience\n0.50\n0.941\n0.54\n.592\n0.004\n0.003\n1.43\n.153\n\n\nInvestor type [Venture Capital]\n-1.86\n3.812\n-0.49\n.626\n-0.017\n0.012\n-1.44\n.151\n\n\nGender [Male]\n3.21\n5.231\n0.61\n.539\n0.000\n0.017\n0.02\n.986\n\n\nCountry [Brazil]\n28.17\n20.980\n1.34\n.180\n0.056\n0.067\n0.84\n.402\n\n\nCountry [Canada]\n13.84\n11.540\n1.20\n.231\n0.032\n0.037\n0.88\n.377\n\n\nCountry [China]\n-2.59\n12.788\n-0.20\n.839\n-0.031\n0.041\n-0.76\n.449\n\n\nCountry [France]\n-7.84\n10.027\n-0.78\n.434\n-0.012\n0.032\n-0.36\n.716\n\n\nCountry [Germany]\n5.27\n10.580\n0.50\n.618\n0.018\n0.034\n0.55\n.585\n\n\nCountry [India]\n8.41\n10.985\n0.77\n.444\n0.021\n0.035\n0.61\n.541\n\n\nCountry [Israel]\n28.49\n14.909\n1.91\n.056\n0.004\n0.047\n0.08\n.935\n\n\nCountry [Singapore]\n-11.03\n13.430\n-0.82\n.412\n0.056\n0.043\n1.31\n.189\n\n\nCountry [United Kingdom]\n-4.62\n6.178\n-0.75\n.455\n0.023\n0.020\n1.18\n.236\n\n\nR2\n.062\n\n\n\n.068\n\n\n\n\n\nR2adj.\n.056\n\n\n\n.062"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#positive-reactions-1",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#positive-reactions-1",
    "title": "Judging a Pitch by its Cover",
    "section": "5.3 Positive reactions",
    "text": "5.3 Positive reactions\nA logistic regression model was estimated to analyze the effects of visual fluency, substantive quality, and their interaction on whether there was a positive reaction to the emails. Table 12 shows the result of this regression model, next to several robustness check models that included control variables and / or were specified as Tobit models.\nIn all regressions, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# run regressions\nglm_tbl(glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive\nglm_tbl(glm(positive ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive_controls\nglm_tbl(AER::tobit(positive ~ fluency * quality, data = d), coef_digits = 3) -&gt; temp_positive_tobit\nglm_tbl(AER::tobit(positive ~ fluency * quality + investments + type + gender + location, data = d), coef_digits = 3) -&gt; temp_positive_controls_tobit\n\n\n# add empty rows to models w/o controls to enable column binding\nrows_control &lt;- nrow(temp_positive_controls)\nrows_simple &lt;- nrow(temp_positive)\ntemp_positive[(rows_simple+1):rows_control,] &lt;- NA\ntemp_positive_tobit[(rows_simple+1):rows_control,] &lt;- NA\n# put interaction and R2 row to the end\ntemp_positive[rows_control-1,] &lt;- temp_positive[rows_simple-1,]\ntemp_positive[rows_control,] &lt;- temp_positive[rows_simple,]\ntemp_positive[rows_simple-1,] &lt;- NA\ntemp_positive[rows_simple,] &lt;- NA\ntemp_positive_tobit[rows_control-1,] &lt;- temp_positive_tobit[rows_simple-1,]\ntemp_positive_tobit[rows_control,] &lt;- temp_positive_tobit[rows_simple,]\ntemp_positive_tobit[rows_simple-1,] &lt;- NA\ntemp_positive_tobit[rows_simple,] &lt;- NA\n\n# table binary logit\ntemp &lt;- bind_cols(temp_positive, temp_positive_controls[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Binary Logit\" = 4, \"Binary Logit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n# table tobit\ntemp &lt;- bind_cols(temp_positive_tobit, temp_positive_controls_tobit[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Nagelkerke's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Tobit Model\" = 4, \"Tobit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 12: Binary logit and Tobit regressions for positive reactions (healthcare startup)\n\n\n\n\n\n\n\n(a) Binary logit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logit\n\n\nBinary Logit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.324\n0.077\n-30.12\n&lt; .001\n-2.068\n0.214\n-9.65\n&lt; .001\n\n\nFluency\n0.405\n0.077\n5.25\n&lt; .001\n0.401\n0.078\n5.16\n&lt; .001\n\n\nQuality\n0.435\n0.077\n5.64\n&lt; .001\n0.448\n0.078\n5.76\n&lt; .001\n\n\nFluency × Quality\n-0.091\n0.077\n-1.18\n.236\n-0.093\n0.078\n-1.20\n.229\n\n\nInvestment experience\n\n\n\n\n-0.020\n0.049\n-0.41\n.679\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.059\n0.165\n0.36\n.721\n\n\nGender [Male]\n\n\n\n\n-0.360\n0.201\n-1.79\n.074\n\n\nCountry [Brazil]\n\n\n\n\n-0.503\n1.045\n-0.48\n.630\n\n\nCountry [Canada]\n\n\n\n\n0.452\n0.425\n1.06\n.287\n\n\nCountry [China]\n\n\n\n\n-0.205\n0.545\n-0.38\n.707\n\n\nCountry [France]\n\n\n\n\n0.783\n0.328\n2.39\n.017\n\n\nCountry [Germany]\n\n\n\n\n0.608\n0.380\n1.60\n.110\n\n\nCountry [India]\n\n\n\n\n-0.324\n0.532\n-0.61\n.542\n\n\nCountry [Israel]\n\n\n\n\n0.099\n0.624\n0.16\n.874\n\n\nCountry [Singapore]\n\n\n\n\n0.618\n0.462\n1.34\n.181\n\n\nCountry [United Kingdom]\n\n\n\n\n0.131\n0.251\n0.52\n.602\n\n\nTjur's R2\n.025\n\n\n\n.033\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tobit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTobit Model\n\n\nTobit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.287\n0.162\n-14.15\n&lt; .001\n-2.012\n0.230\n-8.73\n&lt; .001\n\n\nFluency\n0.344\n0.066\n5.23\n&lt; .001\n0.337\n0.066\n5.14\n&lt; .001\n\n\nQuality\n0.370\n0.066\n5.60\n&lt; .001\n0.377\n0.066\n5.70\n&lt; .001\n\n\nFluency × Quality\n-0.063\n0.063\n-0.99\n.323\n-0.062\n0.063\n-0.98\n.326\n\n\nInvestment experience\n\n\n\n\n-0.017\n0.041\n-0.42\n.676\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.022\n0.142\n0.15\n.878\n\n\nGender [Male]\n\n\n\n\n-0.337\n0.179\n-1.88\n.060\n\n\nCountry [Brazil]\n\n\n\n\n-0.371\n0.836\n-0.44\n.657\n\n\nCountry [Canada]\n\n\n\n\n0.414\n0.382\n1.08\n.279\n\n\nCountry [China]\n\n\n\n\n-0.141\n0.466\n-0.30\n.762\n\n\nCountry [France]\n\n\n\n\n0.689\n0.308\n2.23\n.026\n\n\nCountry [Germany]\n\n\n\n\n0.557\n0.346\n1.61\n.107\n\n\nCountry [India]\n\n\n\n\n-0.238\n0.436\n-0.55\n.586\n\n\nCountry [Israel]\n\n\n\n\n0.019\n0.555\n0.03\n.973\n\n\nCountry [Singapore]\n\n\n\n\n0.483\n0.433\n1.11\n.266\n\n\nCountry [United Kingdom]\n\n\n\n\n0.143\n0.217\n0.66\n.510\n\n\nNagelkerke's R2\n.047\n\n\n\n.056"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#percentage-increases-1",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#percentage-increases-1",
    "title": "Judging a Pitch by its Cover",
    "section": "5.4 Percentage increases",
    "text": "5.4 Percentage increases\nTo facilitate the interpretation of the results, we calculated the percentage increase in the dependent variables for the high level of visual fluency and substantive quality compared to the respective low level. Table 13 shows these percentage increases for the opening duration, completion percentage, and positive reactions.\n\n\nCode\n# calculate percentage increases based on marginal means\n#\n# compute marginal means using the `marginaleffects` package\n#\n# duration\nm_dur &lt;- aov(duration ~ fluency * quality, d)\nmm_dur_flu &lt;- marginaleffects::predictions(m_dur, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_dur_qual &lt;- marginaleffects::predictions(m_dur, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# completion\nm_compl &lt;- aov(completion ~ fluency * quality, d)\nmm_compl_flu &lt;- marginaleffects::predictions(m_compl, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_compl_qual &lt;- marginaleffects::predictions(m_compl, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# positive reactions\nm_pos &lt;- glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), data = d)\nmm_pos_flu &lt;- marginaleffects::predictions(m_pos, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_pos_qual &lt;- marginaleffects::predictions(m_pos, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n\n# compute effect sizes as percentage increases for high vs. low levels\n#\n# we use the formula: 100 * (high / low - 1)\n# That is, we multiply by 100 to get the percentage increase and then\n# subtract 1 to get the percentage increase relative to the low level.\n#\n# duration: fluency\nperc_incr_dur_flu &lt;- 100 * (mm_dur_flu$estimate[mm_dur_flu$fluency == \"high\"] / mm_dur_flu$estimate[mm_dur_flu$fluency == \"low\"] - 1)\n# duration: quality\nperc_incr_dur_qual &lt;- 100 * (mm_dur_qual$estimate[mm_dur_qual$quality == \"high\"] / mm_dur_qual$estimate[mm_dur_qual$quality == \"low\"] - 1)\n\n# completion: fluency\nperc_incr_compl_flu &lt;- 100 * (mm_compl_flu$estimate[mm_compl_flu$fluency == \"high\"] / mm_compl_flu$estimate[mm_compl_flu$fluency == \"low\"] - 1)\n# completion: quality\nperc_incr_compl_qual &lt;- 100 * (mm_compl_qual$estimate[mm_compl_qual$quality == \"high\"] / mm_compl_qual$estimate[mm_compl_qual$quality == \"low\"] - 1)\n\n# positive: fluency\nperc_incr_pos_flu &lt;- 100 * (mm_pos_flu$estimate[mm_pos_flu$fluency == \"high\"] / mm_pos_flu$estimate[mm_pos_flu$fluency == \"low\"] - 1)\n# positive: quality\nperc_incr_pos_qual &lt;- 100 * (mm_pos_qual$estimate[mm_pos_qual$quality == \"high\"] / mm_pos_qual$estimate[mm_pos_qual$quality == \"low\"] - 1)\n\n# create table\ndata.frame(\n  Measure = c(\"Opening duration\", \"Completion percentage\", \"Positive reaction\"),\n  Fluency = c(sprintf('%.1f%%', perc_incr_dur_flu), sprintf('%.1f%%', perc_incr_compl_flu), sprintf('%.1f%%', perc_incr_pos_flu)),\n  Quality = c(sprintf('%.1f%%', perc_incr_dur_qual), sprintf('%.1f%%', perc_incr_compl_qual), sprintf('%.1f%%', perc_incr_pos_qual))\n) |&gt; kbl() |&gt; kable_styling()\n\n\n\n\nTable 13: Percentage increases for high vs. low levels of visual fluency and substantive quality (healthcare startup)\n\n\n\n\n\n\n\nMeasure\nFluency\nQuality\n\n\n\n\nOpening duration\n34.2%\n32.6%\n\n\nCompletion percentage\n14.6%\n15.1%\n\n\nPositive reaction\n108.9%\n120.6%"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#sankey-plots",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#sankey-plots",
    "title": "Judging a Pitch by its Cover",
    "section": "6.1 Sankey plots",
    "text": "6.1 Sankey plots\nFigure 1 (a) and Figure 1 (b) show the flow of the field experiment for the software and healthcare startups, respectively, as Sankey diagram.\n\n\nCode\n# aesthetics\n#\n# define colors\nblue_main &lt;- \"#297FB8\"\nblue_dark &lt;- \"#2D3E50\"\nblue_light &lt;- \"#A0ABBF\"\n\n# define custom geom\ngeom_sankey_label_richtext &lt;- function (mapping = NULL, data = NULL, position = \"identity\", \n  na.rm = FALSE, show.legend = NA, space = NULL, type = \"sankey\", width = 0.1,\n  inherit.aes = TRUE, family =\n    if (.Platform$OS.type == \"windows\") \"Roboto Condensed\" else \"Roboto Condensed Light\", ...)  # added font family\n{\n  label.aes &lt;- list(...)\n  list(label = ggplot2::layer(stat = ggsankey:::StatSankeyText, data = data, \n    mapping = mapping, geom = \"richtext\", position = position, # changed: geom = \"label\"\n    show.legend = show.legend, inherit.aes = inherit.aes, \n    params = purrr::flatten(list(na.rm = na.rm, width = width, \n      space = space, type = type, label.aes, family = family))))\n}\n\n# change ipsum theme to work for sankey plot\ntheme_sankey_ipsum_rc &lt;- function (base_family = \"Roboto Condensed\", base_size = 11.5, ...)\n{\n  {\n    hrbrthemes::theme_ipsum_rc(base_family = base_family, base_size = base_size, ...) %+replace% \n      ggplot2::theme(panel.border = ggplot2::element_blank(), \n        panel.grid.major = ggplot2::element_blank(), \n        panel.grid.minor = ggplot2::element_blank(), \n        axis.line = ggplot2::element_line(colour = \"black\", \n          linewidth = ggplot2::rel(1)), legend.key = ggplot2::element_blank(), \n        strip.background = ggplot2::element_rect(fill = \"white\", \n          colour = \"transparent\", linewidth = ggplot2::rel(2)), \n        complete = TRUE, axis.line.y = ggplot2::element_blank(), \n        axis.line.x = ggplot2::element_blank(),\n        axis.text.x = ggplot2::element_blank(), \n        axis.text.y = ggplot2::element_blank(), \n        axis.ticks.y = ggplot2::element_blank(),\n        axis.ticks.x = ggplot2::element_blank(),\n        # set background to white for quarto to avoid transparency issues\n        panel.background = ggplot2::element_rect(fill='white', color='white'),\n        plot.background = ggplot2::element_rect(fill='white', color='white')\n      )\n  }\n}\n\n\n# Software startup --------------------------------------------------\n#\n# prepare sankey data (the data is already loaded at the beginning of the script)\n#\n# create binary indicators for the different stages\nsankey_soft |&gt; mutate(\n  `Sent mails` = case_when(\n    hard_bounce == 1 ~ \"Hard bounce\",\n    hard_bounce == 0 ~ \"Delivered\"\n  ),\n  Delivered = case_when(\n    hard_bounce == 0 & is.na(date_first_click) ~ \"Recipient did not open deck\",\n    hard_bounce == 0 & !is.na(date_first_click) ~ \"Recipient opened deck\"\n  ),\n  `Recipient opened deck` = case_when(\n    hard_bounce == 0 & !is.na(date_first_click) & is.na(date_reply) ~ \"Recipient did not reply\",\n    hard_bounce == 0 & !is.na(date_reply) ~ \"Recipient replied\"\n  ),\n  `Recipient replied` = case_when(\n    positive == 0 ~ \"Negative reply\",\n    positive == 1 ~ \"Positive reply\"\n  ),\n  Category = category,\n  `Initial sample` = \"Software&lt;br&gt;startup\"\n) -&gt; sankey_soft\n\n# make data long for sankey graph (for simplicity, just called `df`)\nsankey_soft |&gt; make_long(`Initial sample`, `Sent mails`, Delivered,\n                         `Recipient opened deck`, `Recipient replied`,\n                         Category) -&gt; df\n\n# add group_nodes (needed for calculation of group percentages later)\ndf |&gt; mutate(\n  group_node = case_when(\n    node == \"Hard bounce\" | node == \"Delivered\" ~ \"Software&lt;br&gt;startup\",\n    node == \"Recipient did not open deck\" | node == \"Recipient opened deck\" ~ \"Delivered\",\n    node == \"Recipient did not reply\" | node == \"Recipient replied\" ~ \"Recipient opened deck\",\n    node == \"Negative reply\" | node == \"Positive reply\" ~ \"Recipient replied\",\n    node == \"other\" | node == \"geography\" | node == \"no investments\" | node == \"stage\" | node == \"investment strategy\" | node == \"industry\" | node == \"no specific reason\" ~ \"Negative reply\",\n    node == \"more info/clarification\" | node == \"meeting\" | node == \"formal application\" | node == \"updates\" | node == \"referral\" ~ \"Positive reply\"\n  )  \n) -&gt; df\n\n# add information about node N and group_node N and calculate percentages\ndf |&gt;\n  # count obs per node\n  group_by(node) |&gt; mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  # count obs per group_node\n  group_by(group_node) |&gt; mutate(n_group = ifelse(is.na(group_node), NA, n())) |&gt; \n  ungroup() |&gt;\n  # add percentages\n  mutate(pct = n/n_group) -&gt; df\n\n# manually change order of nodes\ndf |&gt; mutate(\n  node = factor(node,\n                levels = c(\"Software&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  ),\n  next_node = factor(next_node,\n                levels = c(\"Software&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  )\n) -&gt; df\n\n# make sankey plot\n#\n# first change the data so that percentages are displayed in the node texts\ndf |&gt; tidyr::drop_na(node) |&gt;\n  mutate(\n    pct = ifelse(is.na(pct), 9999, pct), # dummy-replace NA with 9999, otherwise later ifelse does not work\n    node_text = ifelse(\n      # group_node and thus pct empty (here: 9999)\n      pct == 9999,\n      # yes, pct empty -&gt; no percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s&lt;/span&gt;\",\n                        node, scales::comma(n)),\n      # no, pct not empty -&gt; add percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s (%s%%)&lt;/span&gt;\",\n                        node,\n                        scales::comma(n),\n                 ifelse(pct&lt;.01, paste0(\"0\",weights::rd(pct*100,2)), weights::rd(pct*100,2)))\n      )) |&gt;\n  # now create the plot\n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node),\n             label = node_text,\n             color = factor(node)\n             )) +\n  geom_sankey(flow.alpha = 0.65, show.legend = FALSE, node.color = blue_dark, node.fill = blue_dark) +\n  geom_sankey_label_richtext(size = 3, color = \"black\", fill = \"white\") +\n  labs(x = element_blank()) +\n  # apply customized theme\n  theme_sankey_ipsum_rc(base_size = 11, plot_margin = margin(5, 5, 5, 5)) -&gt; p\n\n# more customizing\n#\n# now: change the color of the segments\n# to this end, first decompose the plot into its parts using `ggplot_build`\nq &lt;- ggplot_build(p)\n\n# first data layer is for line color of flows\nl1 &lt;- q$data[[1]]$colour\n# second data layer is for line color of nodes\nl2 &lt;- q$data[[2]]$colour\n\n# fill colors\nf1 &lt;- q$data[[1]]$fill # flows\nf2 &lt;- q$data[[2]]$fill # nodes\n\n# relevant flows are all of length 600, and only starting color value is relevant\n# thus, color change points (ccp) are\nccp &lt;- seq(1, length(f1), by = 600)\nq$data[[1]]$fill[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$fill[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$fill[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$fill[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$fill[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$fill[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$fill[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$fill[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$fill[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$fill[ccp[16:20]] &lt;- blue_main # positive categories\n\nq$data[[1]]$colour[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$colour[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$colour[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$colour[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$colour[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$colour[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$colour[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$colour[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$colour[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$colour[ccp[16:20]] &lt;- blue_main # positive categories\n\n# put all back together and plot the modified, final plot\np_mod &lt;- ggplot_gtable(q)\nplot(p_mod)\n\n\n# Healthcare startup ------------------------------------------------\n#\n# prepare sankey data (the data is already loaded at the beginning of the script)\n#\n# create binary indicators for the different stages\nsankey_health |&gt; mutate(\n  `Sent mails` = case_when(\n    hard_bounce == 1 ~ \"Hard bounce\",\n    hard_bounce == 0 ~ \"Delivered\"\n  ),\n  Delivered = case_when(\n    hard_bounce == 0 & is.na(date_first_click) ~ \"Recipient did not open deck\",\n    hard_bounce == 0 & !is.na(date_first_click) ~ \"Recipient opened deck\"\n  ),\n  `Recipient opened deck` = case_when(\n    hard_bounce == 0 & !is.na(date_first_click) & is.na(date_reply) ~ \"Recipient did not reply\",\n    hard_bounce == 0 & !is.na(date_reply) ~ \"Recipient replied\"\n  ),\n  `Recipient replied` = case_when(\n    positive == 0 ~ \"Negative reply\",\n    positive == 1 ~ \"Positive reply\"\n  ),\n  Category = category,\n  `Initial sample` = \"Healthcare&lt;br&gt;startup\"\n) -&gt; sankey_health\n\n# make data long for sankey graph (for simplicity, just called `df`)\nsankey_health |&gt; make_long(`Initial sample`, `Sent mails`, Delivered,\n                         `Recipient opened deck`, `Recipient replied`,\n                         Category) -&gt; df\n\n# add group_nodes (needed for calculation of group percentages later)\ndf |&gt; mutate(\n  group_node = case_when(\n    node == \"Hard bounce\" | node == \"Delivered\" ~ \"Healthcare&lt;br&gt;startup\",\n    node == \"Recipient did not open deck\" | node == \"Recipient opened deck\" ~ \"Delivered\",\n    node == \"Recipient did not reply\" | node == \"Recipient replied\" ~ \"Recipient opened deck\",\n    node == \"Negative reply\" | node == \"Positive reply\" ~ \"Recipient replied\",\n    node == \"other\" | node == \"geography\" | node == \"no investments\" | node == \"stage\" | node == \"investment strategy\" | node == \"industry\" | node == \"no specific reason\" ~ \"Negative reply\",\n    node == \"more info/clarification\" | node == \"meeting\" | node == \"formal application\" | node == \"updates\" | node == \"referral\" ~ \"Positive reply\"\n  )  \n) -&gt; df\n\n# add information about node N and group_node N and calculate percentages\ndf |&gt;\n  # count obs per node\n  group_by(node) |&gt; mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  # count obs per group_node\n  group_by(group_node) |&gt; mutate(n_group = ifelse(is.na(group_node), NA, n())) |&gt; \n  ungroup() |&gt;\n  # add percentages\n  mutate(pct = n/n_group) -&gt; df\n\n# manually change order of nodes\ndf |&gt; mutate(\n  node = factor(node,\n                levels = c(\"Healthcare&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  ),\n  next_node = factor(next_node,\n                levels = c(\"Healthcare&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  )\n) -&gt; df\n\n# make sankey plot\n#\n# first change the data so that percentages are displayed in the node texts\ndf |&gt; tidyr::drop_na(node) |&gt;\n  mutate(\n    pct = ifelse(is.na(pct), 9999, pct), # dummy-replace NA with 9999, otherwise later ifelse does not work\n    node_text = ifelse(\n      # group_node and thus pct empty (here: 9999)\n      pct == 9999,\n      # yes, pct empty -&gt; no percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s&lt;/span&gt;\",\n                        node, scales::comma(n)),\n      # no, pct not empty -&gt; add percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s (%s%%)&lt;/span&gt;\",\n                        node,\n                        scales::comma(n),\n                 ifelse(pct&lt;.01, paste0(\"0\",weights::rd(pct*100,2)), weights::rd(pct*100,2)))\n      )) |&gt;\n  # now create the plot\n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node),\n             label = node_text,\n             color = factor(node)\n             )) +\n  geom_sankey(flow.alpha = 0.65, show.legend = FALSE, node.color = blue_dark, node.fill = blue_dark) +\n  geom_sankey_label_richtext(size = 3, color = \"black\", fill = \"white\") +\n  labs(x = element_blank()) +\n  # apply customized theme\n  theme_sankey_ipsum_rc(base_size = 11, plot_margin = margin(5, 5, 5, 5)) -&gt; p\n\n# more customizing\n#\n# now: change the color of the segments\n# to this end, first decompose the plot into its parts using `ggplot_build`\nq &lt;- ggplot_build(p)\n\n# first data layer is for line color of flows\nl1 &lt;- q$data[[1]]$colour\n# second data layer is for line color of nodes\nl2 &lt;- q$data[[2]]$colour\n\n# fill colors\nf1 &lt;- q$data[[1]]$fill # flows\nf2 &lt;- q$data[[2]]$fill # nodes\n\n# relevant flows are all of length 600, and only starting color value is relevant\n# thus, color change points (ccp) are\nccp &lt;- seq(1, length(f1), by = 600)\nq$data[[1]]$fill[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$fill[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$fill[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$fill[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$fill[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$fill[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$fill[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$fill[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$fill[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$fill[ccp[16:20]] &lt;- blue_main # positive categories\n\nq$data[[1]]$colour[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$colour[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$colour[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$colour[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$colour[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$colour[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$colour[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$colour[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$colour[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$colour[ccp[16:20]] &lt;- blue_main # positive categories\n\n# put all back together and plot the modified, final plot\np_mod &lt;- ggplot_gtable(q)\nplot(p_mod)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Software startup\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Healthcare startup\n\n\n\n\n\n\n\nFigure 1: Sankey diagrams of the field experiment flow"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#main-results",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#main-results",
    "title": "Judging a Pitch by its Cover",
    "section": "6.2 Main results",
    "text": "6.2 Main results\nFigure 2 shows the main results visually. Figure 2 (a) shows the results for the opening duration of the pitch decks, Figure 2 (b) for the percentage of pitch decks slides that were viewed, and Figure 2 (c) for the share of positive investor reactions. Note that the significance brackets represent post-hoc contrasts with Holm (1979) correction (** p &lt; .01; *** p &lt; .001).\n\n\nCode\n# Helper functions and aesthetics\n#\n# pval function\npval &lt;- function(p, stars = TRUE){\n  if(stars){\n    if(p &lt; .001) return(\"***\")\n    if(p &lt; .01) return(\"**\")\n    if(p &lt; .05) return(\"*\")\n    if(p &gt;= .05) return(\"NS\")\n  } else{\n    scales::pvalue(p, prefix = c(\"p &lt; \", \"p = \", \"p &gt; \"))\n  }\n}\n\n# theme settings\nmy_style &lt;- list(hrbrthemes::theme_ipsum_rc(),\n                 scale_fill_manual(values=c(blue_light, blue_dark)))\nmy_theme &lt;- theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = .5),\n        axis.title.y = element_text(size = 12, hjust = .5, margin = margin(t = 0, r = 10, b = 0, l = 0)),\n        axis.title.x = element_text(size = 12, hjust = .5, margin = margin(t = 5, r = 0, b = 0, l = 0)),\n        )\n# set up titles, axis names etc.\nmy_labs &lt;- labs(\n  x = \"Quality\", \n  shape='Fluency'\n)\n\n\n# Main figures: software startup\n#\n# data prep\n#\n# for convenience and to not interfere with later code, we work on a copy of\n# the data\nd_soft &lt;- d_sw\n# convert fluency and quality to factor vars\nd_soft$fluency &lt;- factor(d_soft$fluency, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\nd_soft$quality &lt;- factor(d_soft$quality, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\n\n# create dataset for post-hoc contrasts\n# --Note: We create a special dataset since we need to change the reference\n#         level of the factor variables before switching to effect coding to \n#         keep the direction of the effects as intended\nd_soft_analysis &lt;- d_soft\nd_soft_analysis$fluency &lt;- relevel(d_soft_analysis$fluency, ref = 2)\nd_soft_analysis$quality &lt;- relevel(d_soft_analysis$quality, ref = 2)\n# switch to effect coding\ncontrasts(d_soft_analysis$fluency) &lt;- contr.sum # High = 1, Low = -1\ncontrasts(d_soft_analysis$quality) &lt;- contr.sum\n\n# FIGURE FOR DURATION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(duration ~ fluency * quality, d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\nduration_1 &lt;- ggplot(d_soft, aes(x=quality, y=duration, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Duration (in seconds)\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  coord_cartesian(ylim=c(0, 220)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = 175,\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = c(210),\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR COMPLETION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(completion ~ fluency * quality, d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\ncompletion_1 &lt;- ggplot(d_soft, aes(x=quality, y=completion, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Completion percentage\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, 1)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .8,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .85,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR POSITIVE REACTIONS\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(glm(positive ~ fluency * quality,\n                           family = binomial(link = \"logit\"),\n                           data = d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\npositive_1 &lt;- ggplot(d_soft, aes(x=quality, y=positive, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Positive reactions\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, .25)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .125,\n    tip_length = .002,\n    textsize = 6,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .16,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n# Main figures: healthcare startup\n#\n# data prep\n#\n# for convenience and to not interfere with later code, we work on a copy of\n# the data\nd_health &lt;- d_hc\n# convert fluency and quality to factor vars\nd_health$fluency &lt;- factor(d_health$fluency, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\nd_health$quality &lt;- factor(d_health$quality, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\n\n# create dataset for post-hoc contrasts\n# --Note: We create a special dataset since we need to change the reference\n#         level of the factor variables before switching to effect coding to \n#         keep the direction of the effects as intended\nd_health_analysis &lt;- d_health\nd_health_analysis$fluency &lt;- relevel(d_health_analysis$fluency, ref = 2)\nd_health_analysis$quality &lt;- relevel(d_health_analysis$quality, ref = 2)\n# switch to effect coding\ncontrasts(d_health_analysis$fluency) &lt;- contr.sum # High = 1, Low = -1\ncontrasts(d_health_analysis$quality) &lt;- contr.sum\n\n# FIGURE FOR DURATION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(duration ~ fluency * quality, d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\nduration_2 &lt;- ggplot(d_health, aes(x=quality, y=duration, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Duration (in seconds)\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  coord_cartesian(ylim=c(0, 220)) +\n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = 130,\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = c(155),\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR COMPLETION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(completion ~ fluency * quality, d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\ncompletion_2 &lt;- ggplot(d_health, aes(x=quality, y=completion, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Completion percentage\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, 1)) +\n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .8,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .875,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR POSITIVE REACTIONS\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(glm(positive ~ fluency * quality,\n                           family = binomial(link = \"logit\"),\n                           data = d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\npositive_2 &lt;- ggplot(d_health, aes(x=quality, y=positive, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Positive reactions\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, .25)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .1425,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .225,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n\n# Final (combined) figures\n#\n# duration\nduration_1 + duration_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Opening duration of the pitch decks\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n# completion\ncompletion_1 + completion_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Percentage of pitch deck slides being viewed\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n  # tag_levels = 'A'\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n# positive reactions\npositive_1 + positive_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Share of positive investor reactions\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n  # tag_levels = 'A'\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Opening duration. *** p &lt; .001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Completion percentage. ** p &lt; .01; *** p &lt; .001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Positive reactions. ** p &lt; .01; *** p &lt; .001.\n\n\n\n\n\n\n\nFigure 2: Results of the field experiment"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#descriptives-3",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#descriptives-3",
    "title": "Judging a Pitch by its Cover",
    "section": "7.1 Descriptives",
    "text": "7.1 Descriptives\nAfter having applied the exclusion restrictions, there were 944 replies to the 21,152 emails we sent (21.25%) for the software startup. Of these 944 replies, 346 replies were positive (36.65% of replies).\nFor the healthcare startup, there were 757 replies to the 13,335 emails we sent (31.45%). Of these 757 replies, 240 replies were positive (31.70% of replies).\nTable 14 shows a breakdown of the reply rate and the positive reactions per fluency and quality condition.\n\n\nCode\n# combine data\npos_given_reply_data &lt;- bind_rows(list(\n  Software = d_sw |&gt; select(Fluency = fluency, Quality = quality, reply, positive_given_reply, investments, type, gender, location) |&gt; mutate(n_mails = n_mails_sw),\n  Healthcare = d_hc |&gt; select(Fluency = fluency, Quality = quality, reply, positive_given_reply, investments, type, gender, location) |&gt; mutate(n_mails = n_mails_hc)),\n  .id = \"Startup\")\n# convert Startup to factor, change order of levels\npos_given_reply_data$Startup &lt;- factor(pos_given_reply_data$Startup, levels = c(\"Software\", \"Healthcare\"))\n\n# create table\npos_given_reply_data |&gt; \n  group_by(Startup, Fluency, Quality) |&gt; \n  summarize(\n    `Clicks` = scales::comma(n()),\n    `Replies ` = scales::comma(sum(reply)),\n    `% Reply Rate ` = sprintf('%.2f', mean(reply)*100),\n    `Pos. Reaction ` = scales::comma(sum(positive_given_reply, na.rm=T)),\n    `% Pos. Reaction Rate ` = sprintf('%.2f', mean(positive_given_reply, na.rm=T)*100),\n  ) |&gt; kbl() |&gt; kable_styling() |&gt;\n  row_spec(c(4), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 14: Breakdown of positive email reactions per startup, fluency, and quality condition\n\n\n\n\n\n\n\nStartup\nFluency\nQuality\nClicks\nReplies\n% Reply Rate\nPos. Reaction\n% Pos. Reaction Rate\n\n\n\n\nSoftware\nhigh\nhigh\n1,091\n245\n22.46\n133\n54.29\n\n\nSoftware\nhigh\nlow\n1,080\n238\n22.04\n91\n38.24\n\n\nSoftware\nlow\nhigh\n1,126\n233\n20.69\n94\n40.34\n\n\nSoftware\nlow\nlow\n1,146\n228\n19.90\n28\n12.28\n\n\nHealthcare\nhigh\nhigh\n589\n209\n35.48\n101\n48.33\n\n\nHealthcare\nhigh\nlow\n605\n196\n32.40\n57\n29.08\n\n\nHealthcare\nlow\nhigh\n593\n180\n30.35\n59\n32.78\n\n\nHealthcare\nlow\nlow\n620\n172\n27.74\n23\n13.37"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#results",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#results",
    "title": "Judging a Pitch by its Cover",
    "section": "7.2 Results",
    "text": "7.2 Results\nWe re-estimated the binary logit models (with control variables) reported in Table 7 and Table 12, now predicting whether there was a positive reaction to the emails conditional on investors having replied. Thus, for this analysis, the sample is a subset of the full sample (i.e., only investors who replied to the emails, N = 944 software startup, N = 757 healthcare startup). Table 15 shows the results of these models for the software and healthcare startup.\n\n\nCode\nd &lt;- pos_given_reply_data\n\n# 2 tables witch 2 colmuns each version\n\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$Fluency)\nd$quality &lt;- as.factor(d$Quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n\n# run regressions\nglm_tbl(glm(positive_given_reply ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d |&gt; filter(Startup == \"Software\")), coef_digits = 3) -&gt; temp_positive_given_reply_sw\nglm_tbl(glm(positive_given_reply ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d |&gt; filter(Startup == \"Healthcare\")), coef_digits = 3) -&gt; temp_positive_given_reply_hc\n\n# put results together\ntemp &lt;- bind_cols(temp_positive_given_reply_sw,\n                  temp_positive_given_reply_hc[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\"))\n\n# change order of rows: put interaction after main effects\ntemp |&gt; arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\")) -&gt; temp\n# change column names\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\n\n# create final table\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Software Startup\" = 4, \"Healthcare Startup\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 15: Binary logit models of positive reactions conditional on investors having replied\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Startup\n\n\nHealthcare Startup\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-1.664\n1.145\n-1.45\n.146\n0.328\n1.465\n0.22\n.823\n\n\nFluency\n0.519\n0.077\n6.75\n&lt; .001\n0.407\n0.087\n4.66\n&lt; .001\n\n\nQuality\n0.558\n0.077\n7.26\n&lt; .001\n0.504\n0.087\n5.77\n&lt; .001\n\n\nFluency × Quality\n-0.236\n0.077\n-3.08\n.002\n-0.079\n0.087\n-0.91\n.364\n\n\nInvestment experience\n0.005\n0.016\n0.30\n.766\n-0.038\n0.064\n-0.59\n.555\n\n\nInvestor type [Venture Capital]\n-0.111\n0.187\n-0.59\n.553\n-0.140\n0.198\n-0.71\n.478\n\n\nGender [Male]\n0.063\n0.248\n0.26\n.798\n-0.628\n0.253\n-2.49\n.013\n\n\nCountry [Brazil]\n1.098\n1.231\n0.89\n.372\n-0.263\n1.521\n-0.17\n.863\n\n\nCountry [Canada]\n1.375\n1.830\n0.75\n.452\n-0.766\n1.563\n-0.49\n.624\n\n\nCountry [China]\n0.580\n1.292\n0.45\n.654\n-0.162\n1.491\n-0.11\n.913\n\n\nCountry [France]\n1.295\n1.220\n1.06\n.288\n-0.269\n1.505\n-0.18\n.858\n\n\nCountry [Germany]\n0.644\n1.214\n0.53\n.596\n-0.733\n1.567\n-0.47\n.640\n\n\nCountry [India]\n1.055\n1.247\n0.85\n.397\n0.053\n1.678\n0.03\n.975\n\n\nCountry [Israel]\n1.636\n1.280\n1.28\n.201\n-0.327\n1.538\n-0.21\n.832\n\n\nCountry [Singapore]\n0.818\n1.166\n0.70\n.483\n-0.582\n1.464\n-0.40\n.691\n\n\nCountry [United Kingdom]\n0.956\n1.133\n0.84\n.399\n-0.565\n1.443\n-0.39\n.695\n\n\nTjur's R2\n.102\n\n\n\n.084"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Field_Experiment.html#footnotes",
    "href": "replication_reports/Replication_Report_2_Field_Experiment.html#footnotes",
    "title": "Judging a Pitch by its Cover",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTwo independent raters blind to our hypotheses rated all email replies as to whether the content demonstrates investor interest. For the software startup, the interrater agreement was 97.16% (Cohen’s κ = 0.939). For the healthcare startup, the interrater agreement was 96.45% (Cohen’s κ = 0.919). All cases for which the raters did not agree were resolved by the authors of this paper.↩︎"
  },
  {
    "objectID": "replication_reports/Replication_Report_1_MCs.html",
    "href": "replication_reports/Replication_Report_1_MCs.html",
    "title": "Judging a Pitch by its Cover",
    "section": "",
    "text": "For both our fictitious startups (Software: PerkSouq; Healthcare: Brachytix), we ran manipulation checks of the proposed pitch decks. Specifically, we ran four online experiments in which either design (i.e., visual fluency) or substantive quality was manipulated and their impact on several measures was tested.\nWe ran all online experiments on Qualtrics, hosted the pitch decks on DocSend, and recruited the participants via Prolific. For details, see the corresponding AsPredicted pre-registrations listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registrations\n\n\n\n\n\nStartup\nManipulation\nPre-Reg Date\nAsPredicted #\nTarget N\nData Collection Start\n\n\n\n\nSoftware\nDesign\n03-11-2022\n111740\n160\n04-11-2022\n\n\n\nQuality\n11-11-2022\n112721\n160\n12-11-2022\n\n\nHealthcare\nDesign\n18-12-2022\n116999\n160\n19-12-2022\n\n\n\nQuality\n18-12-2022\n117000\n160\n19-12-2022\n\n\n\n\n\n\nIn what follows, we will give an overview of the results, separately for each startup. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\noptions(knitr.kable.NA = '')\n\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\n# further packages that are loaded on demand are:\n# - rstatix\n# - weights\n# - stringr\n# - readr\n# - car\n# - tidyr\n# - hrbrthemes\n# - grid\n\n# set option to disable showing the column types when loading data with `readr`\noptions(\"readr.show_col_types\" = FALSE)\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract t-test results and Cohen's d and put the results together as a string\nttest_str &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", paste0(\" = \",weights::rd(tres$p.value, 3)))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return string\n  return(paste0(stringr::str_to_sentence(dv),\n                \"\\nt(\",\n                ifelse(var.equal == TRUE, tres$parameter, weights::rd(tres$parameter, 1)),\n                \") = \", sprintf('%.2f', tres$statistic),\n                \", p\", pval,\n                \"; d = \", weights::rd(dres$effsize, 2)))\n}\n#\n# extract t-test results and Cohen's d and put the results together as a table\nttest_tbl &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", weights::rd(tres$p.value, 3))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return df\n  df = data.frame(DV = NA, condition=rep(NA, 2), N = NA, Mean = NA, SD = NA, test_statistic = NA, p = NA, d = NA)\n  # fill values\n  df$DV[1] &lt;- stringr::str_to_sentence(dres$`.y.`)\n  df$condition &lt;- c(dres$group1, dres$group2)\n  df$N &lt;- c(dres$n1, dres$n2)\n  df$Mean &lt;- weights::rd(aggregate(formula, data = data, FUN = mean)[,2], 2)\n  df$SD &lt;- weights::rd(aggregate(formula, data = data, FUN = sd)[,2], 3)\n  df$test_statistic[1] &lt;- paste0(\"t(\",\n                                 ifelse(var.equal == TRUE, tres$parameter,\n                                        weights::rd(tres$parameter, 1)),\n                                 \") = \",\n                                 sprintf('%.2f', tres$statistic))\n  df$p[1] &lt;- pval\n  df$d[1] &lt;- weights::rd(dres$effsize, 2)\n  return(df)\n}"
  },
  {
    "objectID": "replication_reports/Replication_Report_1_MCs.html#sec-design-sw",
    "href": "replication_reports/Replication_Report_1_MCs.html#sec-design-sw",
    "title": "Judging a Pitch by its Cover",
    "section": "4.1 Design manipulation (visual fluency)",
    "text": "4.1 Design manipulation (visual fluency)\nIn this between-subjects experiment, we presented participants one of two pitch decks that varied only in their visual fluency. The content (i.e., substantive quality) was held constant across conditions. Specifically, the pitch deck’s design was systematically varied by a design agency with the instruction that four dimensions of processing fluency (contrast, clarity, symmetry, simplicity) should be each either relatively high or relatively low. The goal was to create a high fluency and a low fluency pitch deck.\nIn the online experiment, participants were randomly assigned to one of the two visual fluency conditions, had to open and carefully study the pitch deck, and answer questions on their perceived contrast, clarity, simplicity, symmetry, processing fluency, and venture quality.\n\n4.1.1 Results\nTable 3 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two visual fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- design_sw\n\n# convert fluency_condition to factor\nd$fluency_condition &lt;- as.factor(d$fluency_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Contrast\nres_contr &lt;- ttest_tbl(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 2. Clarity\nres_clar &lt;- ttest_tbl(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 3. Symmetry\nres_sym &lt;- ttest_tbl(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 4. Simplicity\nres_simpl &lt;- ttest_tbl(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 5. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 6. Venture Quality\nres_qual &lt;- ttest_tbl(quality ~ fluency_condition, data = d)\n\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\n\n# put all results together\nbind_rows(res_contr, res_clar, res_sym, res_simpl, res_pf, res_qual) |&gt;\n  kable(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 3: Manipulation checks, visual fluency (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nContrast\nhigh\n49\n4.80\n1.224\nt(90.3) = 5.30\n&lt; .001\n1.06\n\n\n\nlow\n51\n3.22\n1.724\n\n\n\n\n\nClarity\nhigh\n49\n5.45\n1.138\nt(89.2) = 5.07\n&lt; .001\n1.01\n\n\n\nlow\n51\n4.02\n1.643\n\n\n\n\n\nSymmetry\nhigh\n49\n5.76\n.969\nt(80.0) = 5.59\n&lt; .001\n1.11\n\n\n\nlow\n51\n4.22\n1.701\n\n\n\n\n\nSimplicity\nhigh\n49\n4.27\n1.366\nt(98) = -0.03\n.974\n-.01\n\n\n\nlow\n51\n4.27\n1.401\n\n\n\n\n\nProcessing fluency\nhigh\n49\n66.27\n26.701\nt(98) = 3.65\n&lt; .001\n.73\n\n\n\nlow\n51\n46.69\n26.951\n\n\n\n\n\nVenture quality\nhigh\n49\n4.94\n1.069\nt(98) = 1.50\n.136\n.30\n\n\n\nlow\n51\n4.61\n1.133\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Plots\nFigure 1 summarizes the results of this manipulation check visually.\n\n\nCode\n# change factor labels for fluency\nd$fluency_condition &lt;- factor(d$fluency_condition, levels = c(\"high\", \"low\"), labels = c(\"High\", \"Low\"))\n\n# create long dataset for plot\nd_long &lt;- d |&gt; select(contrast:symmetry, fluency, quality, fluency_condition) |&gt; \n  tidyr::pivot_longer(contrast:quality, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_contrast &lt;- ttest_str(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_clarity &lt;- ttest_str(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_symmetry &lt;- ttest_str(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_simplicity &lt;- ttest_str(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ fluency_condition, data = d)\n\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"contrast\", \"clarity\", \"symmetry\", \"simplicity\", \"fluency\", \"quality\"),\n                          labels = c(str_contrast, str_clarity, str_symmetry, str_simplicity, str_fluency, str_quality))\n\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n                 )) -&gt; d_long\n                                  \n# plot result\nggplot(d_long, aes(x=fluency_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +  \n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Visual fluency (software startup)\",\n       subtitle = \"Effect of the low vs. high fluency pitch deck versions on various outcomes\",\n       x = \"Pitch deck visual fluency\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 1: Summary of the fluency manipulation checks for the software startup"
  },
  {
    "objectID": "replication_reports/Replication_Report_1_MCs.html#sec-quality-sw",
    "href": "replication_reports/Replication_Report_1_MCs.html#sec-quality-sw",
    "title": "Judging a Pitch by its Cover",
    "section": "4.2 Quality manipulation",
    "text": "4.2 Quality manipulation\nIn this between-subjects experiment, we presented participants one of two pitch decks that varied only in their substantive quality. The design (i.e., visual fluency) was held constant across conditions. Participants were randomly assigned to one of the two substantive quality conditions, had to open and carefully study the pitch deck, and rate the startup’s intellectual property, human capital, commercialization opportunity, legitimacy, and venture quality. They further had to rate the perceived processing fluency of the pitch deck.\n\n4.2.1 Results\nTable 3 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two quality conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- quality_sw\n\n# convert quality_condition to factor\nd$quality_condition &lt;- as.factor(d$quality_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Intellectual Property\nres_intell &lt;- ttest_tbl(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 2. Human Capital\nres_hum &lt;- ttest_tbl(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 3. Commercialization opportunity\nres_commerc &lt;- ttest_tbl(commerc ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 4. Organizational legitimacy\nres_legitim &lt;- ttest_tbl(legitim ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 5. Overall Venture Quality / Potential\nres_qual &lt;- ttest_tbl(quality ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 6. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ quality_condition, data = d)\n\nres_intell[1,1] &lt;- stringr::str_replace(res_intell[1,1], \"Intell_prop\", \"Intellectual property\")\nres_hum[1,1] &lt;- stringr::str_replace(res_hum[1,1], \"Hum_cap\", \"Human capital\")\nres_commerc[1,1] &lt;- stringr::str_replace(res_commerc[1,1], \"Commerc\", \"Commercialization opportunity\")\nres_legitim[1,1] &lt;- stringr::str_replace(res_legitim[1,1], \"Legitim\", \"Organizational legitimacy\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\n\n# put all results together\nbind_rows(res_intell, res_hum, res_commerc, res_legitim, res_qual, res_pf) |&gt;\n  kable(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 4: Manipulation checks, substantive quality (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nIntellectual property\nhigh\n56\n4.61\n1.303\nt(111) = 2.70\n.008\n.51\n\n\n\nlow\n57\n3.96\n1.224\n\n\n\n\n\nHuman capital\nhigh\n56\n5.54\n.990\nt(111) = 3.61\n&lt; .001\n.68\n\n\n\nlow\n57\n4.88\n.946\n\n\n\n\n\nCommercialization opportunity\nhigh\n56\n5.11\n1.155\nt(111) = 1.21\n.227\n.23\n\n\n\nlow\n57\n4.86\n1.008\n\n\n\n\n\nOrganizational legitimacy\nhigh\n56\n5.02\n1.104\nt(111) = 2.94\n.004\n.55\n\n\n\nlow\n57\n4.40\n1.116\n\n\n\n\n\nVenture quality\nhigh\n56\n4.80\n1.212\nt(111) = 2.82\n.006\n.53\n\n\n\nlow\n57\n4.21\n1.013\n\n\n\n\n\nProcessing fluency\nhigh\n56\n54.39\n26.222\nt(111) = 1.42\n.158\n.27\n\n\n\nlow\n57\n47.12\n28.041\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Plots\nFigure 2 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(intell_prop:legitim, quality, fluency, quality_condition) |&gt; \n  tidyr::pivot_longer(intell_prop:fluency, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_intell_prop &lt;- ttest_str(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\nstr_hum_cap &lt;- ttest_str(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\nstr_commerc &lt;- ttest_str(commerc ~ quality_condition, data = d) # alternative = \"greater\"\nstr_legitim &lt;- ttest_str(legitim ~ quality_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ quality_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ quality_condition, data = d)\n\nstr_intell_prop &lt;- stringr::str_replace(str_intell_prop, \"Intell_prop\", \"Intellectual property\")\nstr_hum_cap &lt;- stringr::str_replace(str_hum_cap, \"Hum_cap\", \"Human capital\")\nstr_commerc &lt;- stringr::str_replace(str_commerc, \"Commerc\", \"Commercialization opportunity\")\nstr_legitim &lt;- stringr::str_replace(str_legitim, \"Legitim\", \"Organizational legitimacy\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"intell_prop\", \"hum_cap\", \"commerc\", \"legitim\",\"quality\",  \"fluency\"),\n                         labels = c(str_intell_prop, str_hum_cap, str_commerc, str_legitim, str_quality, str_fluency))\n\n# create ymin and ymax for plot\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n)) -&gt; d_long\n\n\n# plot result\nggplot(d_long, aes(x=quality_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Substantive quality (software startup)\",\n       subtitle = \"Effect of the low vs. high quality pitch deck versions on various outcomes\",\n       x = \"Pitch deck substantive quality\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 2: Summary of the quality manipulation checks for the software startup"
  },
  {
    "objectID": "replication_reports/Replication_Report_1_MCs.html#sec-design-hc",
    "href": "replication_reports/Replication_Report_1_MCs.html#sec-design-hc",
    "title": "Judging a Pitch by its Cover",
    "section": "5.1 Design manipulation (visual fluency)",
    "text": "5.1 Design manipulation (visual fluency)\nAs before, we presented participants one of two pitch decks that varied only in their visual fluency. The content (i.e., substantive quality) was held constant across conditions. Participants were randomly assigned to the conditions. The dependent variables were the same as before (i.e., perceived contrast, clarity, symmetry, simplicity, processing fluency, and venture quality).\n\n5.1.1 Results\nTable 5 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two visual fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- design_hc\n\n# convert fluency_condition to factor\nd$fluency_condition &lt;- as.factor(d$fluency_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Contrast\nres_contr &lt;- ttest_tbl(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 2. Clarity\nres_clar &lt;- ttest_tbl(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 3. Symmetry\nres_sym &lt;- ttest_tbl(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 4. Simplicity\nres_simpl &lt;- ttest_tbl(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 5. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 6. Venture Quality\nres_qual &lt;- ttest_tbl(quality ~ fluency_condition, data = d)\n\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\n\n# put all results together\nbind_rows(res_contr, res_clar, res_sym, res_simpl, res_pf, res_qual) |&gt;\n  kable(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 5: Manipulation checks, visual fluency (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nContrast\nhigh\n53\n4.83\n1.139\nt(95.4) = 4.46\n&lt; .001\n.87\n\n\n\nlow\n52\n3.67\n1.491\n\n\n\n\n\nClarity\nhigh\n53\n5.49\n1.171\nt(92.8) = 5.12\n&lt; .001\n1.00\n\n\n\nlow\n52\n4.08\n1.619\n\n\n\n\n\nSymmetry\nhigh\n53\n5.36\n1.242\nt(103) = 4.96\n&lt; .001\n.97\n\n\n\nlow\n52\n4.02\n1.515\n\n\n\n\n\nSimplicity\nhigh\n53\n3.74\n1.546\nt(103) = 0.68\n.499\n.13\n\n\n\nlow\n52\n3.52\n1.721\n\n\n\n\n\nProcessing fluency\nhigh\n53\n53.26\n30.495\nt(99.8) = 2.14\n.034\n.42\n\n\n\nlow\n52\n41.62\n24.949\n\n\n\n\n\nVenture quality\nhigh\n53\n5.28\n1.007\nt(103) = 0.90\n.371\n.18\n\n\n\nlow\n52\n5.12\n.900\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Plots\nFigure 3 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(contrast:symmetry, fluency, quality, fluency_condition) |&gt; \n  tidyr::pivot_longer(contrast:quality, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_contrast &lt;- ttest_str(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_clarity &lt;- ttest_str(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_symmetry &lt;- ttest_str(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_simplicity &lt;- ttest_str(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ fluency_condition, data = d)\n\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"contrast\", \"clarity\", \"symmetry\", \"simplicity\", \"fluency\", \"quality\"),\n                          labels = c(str_contrast, str_clarity, str_symmetry, str_simplicity, str_fluency, str_quality))\n\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n                 )) -&gt; d_long\n\n# plot result\nggplot(d_long, aes(x=fluency_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Visual fluency (healthcare startup)\",\n       subtitle = \"Effect of the low vs. high fluency pitch deck versions on various outcomes\",\n       x = \"Pitch deck visual fluency\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 3: Summary of the fluency manipulation checks for the healthcare startup"
  },
  {
    "objectID": "replication_reports/Replication_Report_1_MCs.html#sec-quality-hc",
    "href": "replication_reports/Replication_Report_1_MCs.html#sec-quality-hc",
    "title": "Judging a Pitch by its Cover",
    "section": "5.2 Quality manipulation",
    "text": "5.2 Quality manipulation\nAs before, we presented participants one of two pitch decks that varied only in their substantive quality. The design was held constant across conditions. Participants were randomly assigned to the conditions. The dependent variables are the same as before (i.e., intellectual property, human capital, commercialization opportunity, legitimacy, venture quality, and processing fluency).\n\n5.2.1 Results\nTable 5 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances. \n\n\nCode\nd &lt;- quality_hc\n\n# convert quality_condition to factor\nd$quality_condition &lt;- as.factor(d$quality_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Intellectual Property\nres_intell &lt;- ttest_tbl(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 2. Human Capital\nres_hum &lt;- ttest_tbl(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 3. Commercialization opportunity\nres_commerc &lt;- ttest_tbl(commerc ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 4. Organizational legitimacy\nres_legitim &lt;- ttest_tbl(legitim ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 5. Overall Venture Quality / Potential\nres_qual &lt;- ttest_tbl(quality ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 6. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ quality_condition, data = d)\n\nres_intell[1,1] &lt;- stringr::str_replace(res_intell[1,1], \"Intell_prop\", \"Intellectual property\")\nres_hum[1,1] &lt;- stringr::str_replace(res_hum[1,1], \"Hum_cap\", \"Human capital\")\nres_commerc[1,1] &lt;- stringr::str_replace(res_commerc[1,1], \"Commerc\", \"Commercialization opportunity\")\nres_legitim[1,1] &lt;- stringr::str_replace(res_legitim[1,1], \"Legitim\", \"Organizational legitimacy\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\n\n# put all results together\nbind_rows(res_intell, res_hum, res_commerc, res_legitim, res_qual, res_pf) |&gt;\n  kable(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 6: Manipulation checks, substantive quality (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nIntellectual property\nhigh\n66\n5.65\n.903\nt(63.2) = 4.41\n&lt; .001\n.90\n\n\n\nlow\n43\n4.56\n1.452\n\n\n\n\n\nHuman capital\nhigh\n66\n5.86\n.959\nt(107) = 3.63\n&lt; .001\n.71\n\n\n\nlow\n43\n5.09\n1.250\n\n\n\n\n\nCommercialization opportunity\nhigh\n66\n5.52\n1.070\nt(107) = 3.32\n.001\n.65\n\n\n\nlow\n43\n4.74\n1.347\n\n\n\n\n\nOrganizational legitimacy\nhigh\n66\n5.36\n1.047\nt(107) = 3.44\n&lt; .001\n.67\n\n\n\nlow\n43\n4.63\n1.155\n\n\n\n\n\nVenture quality\nhigh\n66\n5.39\n.839\nt(107) = 3.01\n.003\n.59\n\n\n\nlow\n43\n4.84\n1.090\n\n\n\n\n\nProcessing fluency\nhigh\n66\n46.67\n26.018\nt(107) = -1.12\n.264\n-.22\n\n\n\nlow\n43\n52.58\n28.202\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Plots\nFigure 4 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(intell_prop:legitim, quality, fluency, quality_condition) |&gt; \n  tidyr::pivot_longer(intell_prop:fluency, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_intell_prop &lt;- ttest_str(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\nstr_hum_cap &lt;- ttest_str(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\nstr_commerc &lt;- ttest_str(commerc ~ quality_condition, data = d) # alternative = \"greater\"\nstr_legitim &lt;- ttest_str(legitim ~ quality_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ quality_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ quality_condition, data = d)\n\nstr_intell_prop &lt;- stringr::str_replace(str_intell_prop, \"Intell_prop\", \"Intellectual property\")\nstr_hum_cap &lt;- stringr::str_replace(str_hum_cap, \"Hum_cap\", \"Human capital\")\nstr_commerc &lt;- stringr::str_replace(str_commerc, \"Commerc\", \"Commercialization opportunity\")\nstr_legitim &lt;- stringr::str_replace(str_legitim, \"Legitim\", \"Organizational legitimacy\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"intell_prop\", \"hum_cap\", \"commerc\", \"legitim\",\"quality\",  \"fluency\"),\n                         labels = c(str_intell_prop, str_hum_cap, str_commerc, str_legitim, str_quality, str_fluency))\n\n# create ymin and ymax for plot\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n)) -&gt; d_long\n\n\n# plot result\nggplot(d_long, aes(x=quality_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Substantive quality (healthcare startup)\",\n       subtitle = \"Effect of the low vs. high quality pitch deck versions on various outcomes\",\n       x = \"Pitch deck substantive quality\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 4: Summary of the quality manipulation checks for the healthcare startup"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_1_MCs.html",
    "href": "replication_reports/backup/Replication_Report_1_MCs.html",
    "title": "Judging a Pitch by its Cover",
    "section": "",
    "text": "For both our fictitious startups (Software: PerkSouq; Healthcare: Brachytix), we ran manipulation checks of the proposed pitch decks. Specifically, we ran four online experiments in which either design (i.e., visual fluency) or substantive quality was manipulated and their impact on several measures was tested.\nWe ran all online experiments on Qualtrics, hosted the pitch decks on DocSend, and recruited the participants via Prolific. For details, see the corresponding AsPredicted pre-registrations listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registrations\n\n\n\n\n\nStartup\nManipulation\nPre-Reg Date\nAsPredicted #\nTarget N\nData Collection Start\n\n\n\n\nSoftware\nDesign\n03-11-2022\n111740\n160\n04-11-2022\n\n\n\nQuality\n11-11-2022\n112721\n160\n12-11-2022\n\n\nHealthcare\nDesign\n18-12-2022\n116999\n160\n19-12-2022\n\n\n\nQuality\n18-12-2022\n117000\n160\n19-12-2022\n\n\n\n\n\n\nIn what follows, we will give an overview of the results, separately for each startup. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\noptions(knitr.kable.NA = '')\n\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\n# further packages that are loaded on demand are:\n# - rstatix\n# - weights\n# - stringr\n# - readr\n# - car\n# - tidyr\n# - hrbrthemes\n# - grid\n\n# set option to disable showing the column types when loading data with `readr`\noptions(\"readr.show_col_types\" = FALSE)\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract t-test results and Cohen's d and put the results together as a string\nttest_str &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", paste0(\" = \",weights::rd(tres$p.value, 3)))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return string\n  return(paste0(stringr::str_to_sentence(dv),\n                \"\\nt(\",\n                ifelse(var.equal == TRUE, tres$parameter, weights::rd(tres$parameter, 1)),\n                \") = \", sprintf('%.2f', tres$statistic),\n                \", p\", pval,\n                \"; d = \", weights::rd(dres$effsize, 2)))\n}\n#\n# extract t-test results and Cohen's d and put the results together as a table\nttest_tbl &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", weights::rd(tres$p.value, 3))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return df\n  df = data.frame(DV = NA, condition=rep(NA, 2), N = NA, Mean = NA, SD = NA, test_statistic = NA, p = NA, d = NA)\n  # fill values\n  df$DV[1] &lt;- stringr::str_to_sentence(dres$`.y.`)\n  df$condition &lt;- c(dres$group1, dres$group2)\n  df$N &lt;- c(dres$n1, dres$n2)\n  df$Mean &lt;- weights::rd(aggregate(formula, data = data, FUN = mean)[,2], 2)\n  df$SD &lt;- weights::rd(aggregate(formula, data = data, FUN = sd)[,2], 3)\n  df$test_statistic[1] &lt;- paste0(\"t(\",\n                                 ifelse(var.equal == TRUE, tres$parameter,\n                                        weights::rd(tres$parameter, 1)),\n                                 \") = \",\n                                 sprintf('%.2f', tres$statistic))\n  df$p[1] &lt;- pval\n  df$d[1] &lt;- weights::rd(dres$effsize, 2)\n  return(df)\n}"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_1_MCs.html#sec-design-sw",
    "href": "replication_reports/backup/Replication_Report_1_MCs.html#sec-design-sw",
    "title": "Judging a Pitch by its Cover",
    "section": "4.1 Design manipulation (visual fluency)",
    "text": "4.1 Design manipulation (visual fluency)\nIn this between-subjects experiment, we presented participants one of two pitch decks that varied only in their visual fluency. The content (i.e., substantive quality) was held constant across conditions. Specifically, the pitch deck’s design was systematically varied by a design agency with the instruction that four dimensions of processing fluency (contrast, clarity, symmetry, simplicity) should be each either relatively high or relatively low. The goal was to create a high fluency and a low fluency pitch deck.\nIn the online experiment, participants were randomly assigned to one of the two visual fluency conditions, had to open and carefully study the pitch deck, and answer questions on their perceived contrast, clarity, simplicity, symmetry, processing fluency, and venture quality.\n\n4.1.1 Results\nTable 3 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two visual fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- design_sw\n\n# convert fluency_condition to factor\nd$fluency_condition &lt;- as.factor(d$fluency_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Contrast\nres_contr &lt;- ttest_tbl(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 2. Clarity\nres_clar &lt;- ttest_tbl(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 3. Symmetry\nres_sym &lt;- ttest_tbl(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 4. Simplicity\nres_simpl &lt;- ttest_tbl(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 5. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 6. Venture Quality\nres_qual &lt;- ttest_tbl(quality ~ fluency_condition, data = d)\n\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\n\n# put all results together\nbind_rows(res_contr, res_clar, res_sym, res_simpl, res_pf, res_qual) |&gt;\n  kable(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 3: Manipulation checks, visual fluency (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nContrast\nhigh\n49\n4.80\n1.224\nt(90.3) = 5.30\n&lt; .001\n1.06\n\n\n\nlow\n51\n3.22\n1.724\n\n\n\n\n\nClarity\nhigh\n49\n5.45\n1.138\nt(89.2) = 5.07\n&lt; .001\n1.01\n\n\n\nlow\n51\n4.02\n1.643\n\n\n\n\n\nSymmetry\nhigh\n49\n5.76\n.969\nt(80.0) = 5.59\n&lt; .001\n1.11\n\n\n\nlow\n51\n4.22\n1.701\n\n\n\n\n\nSimplicity\nhigh\n49\n4.27\n1.366\nt(98) = -0.03\n.974\n-.01\n\n\n\nlow\n51\n4.27\n1.401\n\n\n\n\n\nProcessing fluency\nhigh\n49\n66.27\n26.701\nt(98) = 3.65\n&lt; .001\n.73\n\n\n\nlow\n51\n46.69\n26.951\n\n\n\n\n\nVenture quality\nhigh\n49\n4.94\n1.069\nt(98) = 1.50\n.136\n.30\n\n\n\nlow\n51\n4.61\n1.133\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Plots\nFigure 1 summarizes the results of this manipulation check visually.\n\n\nCode\n# change factor labels for fluency\nd$fluency_condition &lt;- factor(d$fluency_condition, levels = c(\"high\", \"low\"), labels = c(\"High\", \"Low\"))\n\n# create long dataset for plot\nd_long &lt;- d |&gt; select(contrast:symmetry, fluency, quality, fluency_condition) |&gt; \n  tidyr::pivot_longer(contrast:quality, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_contrast &lt;- ttest_str(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_clarity &lt;- ttest_str(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_symmetry &lt;- ttest_str(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_simplicity &lt;- ttest_str(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ fluency_condition, data = d)\n\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"contrast\", \"clarity\", \"symmetry\", \"simplicity\", \"fluency\", \"quality\"),\n                          labels = c(str_contrast, str_clarity, str_symmetry, str_simplicity, str_fluency, str_quality))\n\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n                 )) -&gt; d_long\n                                  \n# plot result\nggplot(d_long, aes(x=fluency_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +  \n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Visual fluency (software startup)\",\n       subtitle = \"Effect of the low vs. high fluency pitch deck versions on various outcomes\",\n       x = \"Pitch deck visual fluency\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 1: Summary of the fluency manipulation checks for the software startup"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_1_MCs.html#sec-quality-sw",
    "href": "replication_reports/backup/Replication_Report_1_MCs.html#sec-quality-sw",
    "title": "Judging a Pitch by its Cover",
    "section": "4.2 Quality manipulation",
    "text": "4.2 Quality manipulation\nIn this between-subjects experiment, we presented participants one of two pitch decks that varied only in their substantive quality. The design (i.e., visual fluency) was held constant across conditions. Participants were randomly assigned to one of the two substantive quality conditions, had to open and carefully study the pitch deck, and rate the startup’s intellectual property, human capital, commercialization opportunity, legitimacy, and venture quality. They further had to rate the perceived processing fluency of the pitch deck.\n\n4.2.1 Results\nTable 3 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two quality conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- quality_sw\n\n# convert quality_condition to factor\nd$quality_condition &lt;- as.factor(d$quality_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Intellectual Property\nres_intell &lt;- ttest_tbl(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 2. Human Capital\nres_hum &lt;- ttest_tbl(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 3. Commercialization opportunity\nres_commerc &lt;- ttest_tbl(commerc ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 4. Organizational legitimacy\nres_legitim &lt;- ttest_tbl(legitim ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 5. Overall Venture Quality / Potential\nres_qual &lt;- ttest_tbl(quality ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 6. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ quality_condition, data = d)\n\nres_intell[1,1] &lt;- stringr::str_replace(res_intell[1,1], \"Intell_prop\", \"Intellectual property\")\nres_hum[1,1] &lt;- stringr::str_replace(res_hum[1,1], \"Hum_cap\", \"Human capital\")\nres_commerc[1,1] &lt;- stringr::str_replace(res_commerc[1,1], \"Commerc\", \"Commercialization opportunity\")\nres_legitim[1,1] &lt;- stringr::str_replace(res_legitim[1,1], \"Legitim\", \"Organizational legitimacy\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\n\n# put all results together\nbind_rows(res_intell, res_hum, res_commerc, res_legitim, res_qual, res_pf) |&gt;\n  kable(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 4: Manipulation checks, substantive quality (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nIntellectual property\nhigh\n56\n4.61\n1.303\nt(111) = 2.70\n.008\n.51\n\n\n\nlow\n57\n3.96\n1.224\n\n\n\n\n\nHuman capital\nhigh\n56\n5.54\n.990\nt(111) = 3.61\n&lt; .001\n.68\n\n\n\nlow\n57\n4.88\n.946\n\n\n\n\n\nCommercialization opportunity\nhigh\n56\n5.11\n1.155\nt(111) = 1.21\n.227\n.23\n\n\n\nlow\n57\n4.86\n1.008\n\n\n\n\n\nOrganizational legitimacy\nhigh\n56\n5.02\n1.104\nt(111) = 2.94\n.004\n.55\n\n\n\nlow\n57\n4.40\n1.116\n\n\n\n\n\nVenture quality\nhigh\n56\n4.80\n1.212\nt(111) = 2.82\n.006\n.53\n\n\n\nlow\n57\n4.21\n1.013\n\n\n\n\n\nProcessing fluency\nhigh\n56\n54.39\n26.222\nt(111) = 1.42\n.158\n.27\n\n\n\nlow\n57\n47.12\n28.041\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Plots\nFigure 2 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(intell_prop:legitim, quality, fluency, quality_condition) |&gt; \n  tidyr::pivot_longer(intell_prop:fluency, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_intell_prop &lt;- ttest_str(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\nstr_hum_cap &lt;- ttest_str(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\nstr_commerc &lt;- ttest_str(commerc ~ quality_condition, data = d) # alternative = \"greater\"\nstr_legitim &lt;- ttest_str(legitim ~ quality_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ quality_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ quality_condition, data = d)\n\nstr_intell_prop &lt;- stringr::str_replace(str_intell_prop, \"Intell_prop\", \"Intellectual property\")\nstr_hum_cap &lt;- stringr::str_replace(str_hum_cap, \"Hum_cap\", \"Human capital\")\nstr_commerc &lt;- stringr::str_replace(str_commerc, \"Commerc\", \"Commercialization opportunity\")\nstr_legitim &lt;- stringr::str_replace(str_legitim, \"Legitim\", \"Organizational legitimacy\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"intell_prop\", \"hum_cap\", \"commerc\", \"legitim\",\"quality\",  \"fluency\"),\n                         labels = c(str_intell_prop, str_hum_cap, str_commerc, str_legitim, str_quality, str_fluency))\n\n# create ymin and ymax for plot\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n)) -&gt; d_long\n\n\n# plot result\nggplot(d_long, aes(x=quality_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Substantive quality (software startup)\",\n       subtitle = \"Effect of the low vs. high quality pitch deck versions on various outcomes\",\n       x = \"Pitch deck substantive quality\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 2: Summary of the quality manipulation checks for the software startup"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_1_MCs.html#sec-design-hc",
    "href": "replication_reports/backup/Replication_Report_1_MCs.html#sec-design-hc",
    "title": "Judging a Pitch by its Cover",
    "section": "5.1 Design manipulation (visual fluency)",
    "text": "5.1 Design manipulation (visual fluency)\nAs before, we presented participants one of two pitch decks that varied only in their visual fluency. The content (i.e., substantive quality) was held constant across conditions. Participants were randomly assigned to the conditions. The dependent variables were the same as before (i.e., perceived contrast, clarity, symmetry, simplicity, processing fluency, and venture quality).\n\n5.1.1 Results\nTable 5 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two visual fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- design_hc\n\n# convert fluency_condition to factor\nd$fluency_condition &lt;- as.factor(d$fluency_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Contrast\nres_contr &lt;- ttest_tbl(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 2. Clarity\nres_clar &lt;- ttest_tbl(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 3. Symmetry\nres_sym &lt;- ttest_tbl(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 4. Simplicity\nres_simpl &lt;- ttest_tbl(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 5. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 6. Venture Quality\nres_qual &lt;- ttest_tbl(quality ~ fluency_condition, data = d)\n\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\n\n# put all results together\nbind_rows(res_contr, res_clar, res_sym, res_simpl, res_pf, res_qual) |&gt;\n  kable(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 5: Manipulation checks, visual fluency (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nContrast\nhigh\n53\n4.83\n1.139\nt(95.4) = 4.46\n&lt; .001\n.87\n\n\n\nlow\n52\n3.67\n1.491\n\n\n\n\n\nClarity\nhigh\n53\n5.49\n1.171\nt(92.8) = 5.12\n&lt; .001\n1.00\n\n\n\nlow\n52\n4.08\n1.619\n\n\n\n\n\nSymmetry\nhigh\n53\n5.36\n1.242\nt(103) = 4.96\n&lt; .001\n.97\n\n\n\nlow\n52\n4.02\n1.515\n\n\n\n\n\nSimplicity\nhigh\n53\n3.74\n1.546\nt(103) = 0.68\n.499\n.13\n\n\n\nlow\n52\n3.52\n1.721\n\n\n\n\n\nProcessing fluency\nhigh\n53\n53.26\n30.495\nt(99.8) = 2.14\n.034\n.42\n\n\n\nlow\n52\n41.62\n24.949\n\n\n\n\n\nVenture quality\nhigh\n53\n5.28\n1.007\nt(103) = 0.90\n.371\n.18\n\n\n\nlow\n52\n5.12\n.900\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Plots\nFigure 3 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(contrast:symmetry, fluency, quality, fluency_condition) |&gt; \n  tidyr::pivot_longer(contrast:quality, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_contrast &lt;- ttest_str(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_clarity &lt;- ttest_str(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_symmetry &lt;- ttest_str(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_simplicity &lt;- ttest_str(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ fluency_condition, data = d)\n\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"contrast\", \"clarity\", \"symmetry\", \"simplicity\", \"fluency\", \"quality\"),\n                          labels = c(str_contrast, str_clarity, str_symmetry, str_simplicity, str_fluency, str_quality))\n\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n                 )) -&gt; d_long\n\n# plot result\nggplot(d_long, aes(x=fluency_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Visual fluency (healthcare startup)\",\n       subtitle = \"Effect of the low vs. high fluency pitch deck versions on various outcomes\",\n       x = \"Pitch deck visual fluency\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 3: Summary of the fluency manipulation checks for the healthcare startup"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_1_MCs.html#sec-quality-hc",
    "href": "replication_reports/backup/Replication_Report_1_MCs.html#sec-quality-hc",
    "title": "Judging a Pitch by its Cover",
    "section": "5.2 Quality manipulation",
    "text": "5.2 Quality manipulation\nAs before, we presented participants one of two pitch decks that varied only in their substantive quality. The design was held constant across conditions. Participants were randomly assigned to the conditions. The dependent variables are the same as before (i.e., intellectual property, human capital, commercialization opportunity, legitimacy, venture quality, and processing fluency).\n\n5.2.1 Results\nTable 5 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances. \n\n\nCode\nd &lt;- quality_hc\n\n# convert quality_condition to factor\nd$quality_condition &lt;- as.factor(d$quality_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Intellectual Property\nres_intell &lt;- ttest_tbl(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 2. Human Capital\nres_hum &lt;- ttest_tbl(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 3. Commercialization opportunity\nres_commerc &lt;- ttest_tbl(commerc ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 4. Organizational legitimacy\nres_legitim &lt;- ttest_tbl(legitim ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 5. Overall Venture Quality / Potential\nres_qual &lt;- ttest_tbl(quality ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 6. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ quality_condition, data = d)\n\nres_intell[1,1] &lt;- stringr::str_replace(res_intell[1,1], \"Intell_prop\", \"Intellectual property\")\nres_hum[1,1] &lt;- stringr::str_replace(res_hum[1,1], \"Hum_cap\", \"Human capital\")\nres_commerc[1,1] &lt;- stringr::str_replace(res_commerc[1,1], \"Commerc\", \"Commercialization opportunity\")\nres_legitim[1,1] &lt;- stringr::str_replace(res_legitim[1,1], \"Legitim\", \"Organizational legitimacy\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\n\n# put all results together\nbind_rows(res_intell, res_hum, res_commerc, res_legitim, res_qual, res_pf) |&gt;\n  kable(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 6: Manipulation checks, substantive quality (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nIntellectual property\nhigh\n66\n5.65\n.903\nt(63.2) = 4.41\n&lt; .001\n.90\n\n\n\nlow\n43\n4.56\n1.452\n\n\n\n\n\nHuman capital\nhigh\n66\n5.86\n.959\nt(107) = 3.63\n&lt; .001\n.71\n\n\n\nlow\n43\n5.09\n1.250\n\n\n\n\n\nCommercialization opportunity\nhigh\n66\n5.52\n1.070\nt(107) = 3.32\n.001\n.65\n\n\n\nlow\n43\n4.74\n1.347\n\n\n\n\n\nOrganizational legitimacy\nhigh\n66\n5.36\n1.047\nt(107) = 3.44\n&lt; .001\n.67\n\n\n\nlow\n43\n4.63\n1.155\n\n\n\n\n\nVenture quality\nhigh\n66\n5.39\n.839\nt(107) = 3.01\n.003\n.59\n\n\n\nlow\n43\n4.84\n1.090\n\n\n\n\n\nProcessing fluency\nhigh\n66\n46.67\n26.018\nt(107) = -1.12\n.264\n-.22\n\n\n\nlow\n43\n52.58\n28.202\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Plots\nFigure 4 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(intell_prop:legitim, quality, fluency, quality_condition) |&gt; \n  tidyr::pivot_longer(intell_prop:fluency, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_intell_prop &lt;- ttest_str(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\nstr_hum_cap &lt;- ttest_str(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\nstr_commerc &lt;- ttest_str(commerc ~ quality_condition, data = d) # alternative = \"greater\"\nstr_legitim &lt;- ttest_str(legitim ~ quality_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ quality_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ quality_condition, data = d)\n\nstr_intell_prop &lt;- stringr::str_replace(str_intell_prop, \"Intell_prop\", \"Intellectual property\")\nstr_hum_cap &lt;- stringr::str_replace(str_hum_cap, \"Hum_cap\", \"Human capital\")\nstr_commerc &lt;- stringr::str_replace(str_commerc, \"Commerc\", \"Commercialization opportunity\")\nstr_legitim &lt;- stringr::str_replace(str_legitim, \"Legitim\", \"Organizational legitimacy\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"intell_prop\", \"hum_cap\", \"commerc\", \"legitim\",\"quality\",  \"fluency\"),\n                         labels = c(str_intell_prop, str_hum_cap, str_commerc, str_legitim, str_quality, str_fluency))\n\n# create ymin and ymax for plot\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n)) -&gt; d_long\n\n\n# plot result\nggplot(d_long, aes(x=quality_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Substantive quality (healthcare startup)\",\n       subtitle = \"Effect of the low vs. high quality pitch deck versions on various outcomes\",\n       x = \"Pitch deck substantive quality\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 4: Summary of the quality manipulation checks for the healthcare startup"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html",
    "title": "Judging a Pitch by its Cover",
    "section": "",
    "text": "In our main study, we ran a field experiment in which we varied the visual fluency and the substantive quality of pitch decks of our two fictitious startups (Software: PerkSouq; Healthcare: Brachytix).\nSpecifically, we sent out a standardized email to 39,977 potential investors (24,961 for the software startup and 15,016 for healthcare startup), and tracked whether the potential investor clicks on the link to the pitch deck. Every investor who clicked the link within 21 days of receiving the email was potentially part of our sample (pending exclusion restrictions). All data that was recorded within 21 days after an investor clicked on the link to the pitch deck was considered for analysis.\nDepending on their previous investment record, investors were matched to either the software startup or healthcare startup, then randomly assigned to one of the four experimental conditions. We pretested all the manipulations. We tracked whether an investor clicked on the link to the pitch deck, the cumulative time the pitch deck remained open, the percentage of slides viewed, and whether there was a positive reaction to the email. A reaction was considered positive if a meeting appointment was scheduled over a link in the deck and / or an email reply has been sent that clearly demonstrated investor interest.1 Participants were not aware that they took part in a scientific study and that the startups were fictitious.\nFor more details, e.g., on the exclusion criteria that we used, see the corresponding AsPredicted pre-registration listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registration\n\n\n\n\n\nStartup\nPre-Reg Date\nAsPredicted #\nData Collection Start\n\n\n\n\nPerkSouq & Brachytix\n13-01-2023\n118675\n16-01-2023\n\n\n\n\n\n\nIn what follows, we will give an overview of the results, separately for each startup, followed by figures that summarize the results of the field experiment. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(ggsankey)\nlibrary(ggsignif)\nlibrary(patchwork)\nlibrary(kableExtra)\noptions(knitr.kable.NA = '',\n        kable_styling_bootstrap_options = c(\"striped\", \"condensed\", \"responsive\"))\n\n# further packages that are loaded on demand are:\n# - supernova\n# - weights\n# - parameters\n# - broom\n# - scales\n# - performance\n# - readr\n# - lubridate\n# - stringr\n# - tidyr\n# - irr\n# - AER\n# - hrbrthemes\n# - grid\n# - gridExtra\n\n\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract ANOVA results including eta squared and put the results together as a table\n# -Note: needs the supernova library installed\nanova_tbl &lt;- function(formula, data, type = 3, ...){\n  # perform ANOVA\n  d &lt;- supernova::supernova(lm(as.formula(deparse(formula)), data = data), type = type)$tbl\n  # check whether ANOVA is univariate or factorial\n  univariate &lt;- all(d$term %in% c(\"Model\", \"Error\", \"Total\"))\n  # get rows of interest\n  if(univariate) {\n    effect_rows &lt;- d$term %notin% c(\"Error\", \"Total\")\n  } else {\n    effect_rows &lt;- d$term %notin% c(\"Model\", \"Error\", \"Total\")\n  }\n  # extract key parameters\n  effect &lt;- d$term[effect_rows]\n  MSE &lt;- round(d$MS[effect_rows], 2)\n  df &lt;- d$df[effect_rows]\n  df_res &lt;- d$df[d$term == \"Error\"]\n  statistic &lt;- round(d$F[effect_rows], 2)\n  pval &lt;- ifelse(d$p[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$p[effect_rows], 3))\n  eta &lt;- ifelse(d$PRE[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$PRE[effect_rows], 3))\n  # construct return df\n  return(data.frame(effect, MSE, df, df_res, statistic, pval, eta))\n}\n# extract GLM results and put the results together as a table\nglm_tbl &lt;- function(model, coef_digits = 2, coef_bold = TRUE, p_threshold = 0.05, ...){\n  # extract model parameters\n  if(\"tobit\" %in% class(model)){ # tobit model -&gt; broom::tidy does not work\n    res &lt;- parameters::model_parameters(model)\n    res &lt;- res[c(\"Parameter\", \"Coefficient\", \"SE\", \"z\", \"p\")]\n    names(res) &lt;- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n    # res[] &lt;- lapply(res, function(x) { attributes(x) &lt;- NULL; x })\n  } else {\n    res &lt;- broom::tidy(model)\n  }\n  pvals &lt;- res$p.value\n  res$estimate &lt;- sprintf(paste0(\"%.\", coef_digits, \"f\"), res$estimate)\n  res$std.error &lt;- sprintf(\"%.3f\", res$std.error)\n  res$statistic &lt;- sprintf(\"%.2f\", res$statistic)\n  # format p value\n  res$p.value &lt;- ifelse(res$p.value &lt; .001, \" &lt; .001\", weights::rd(res$p.value, 3))\n  # make estimates bold if below critical p value\n  if(coef_bold){\n    res$estimate[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$estimate[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$std.error[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$std.error[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$statistic[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$statistic[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$p.value[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$p.value[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n  }\n  # bind R2 and Adj. R2 to model parameters\n  r2 &lt;- performance::r2(model) # extract R2\n  end &lt;- nrow(res) + seq_len(length(r2))\n  res[end,\"term\"] &lt;- names(r2)\n  res[end,\"estimate\"] &lt;- weights::rd(unlist(r2), digits = 3)\n  # return result\n  return(res)\n}"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#descriptives-1",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#descriptives-1",
    "title": "Judging a Pitch by its Cover",
    "section": "4.1 Descriptives",
    "text": "4.1 Descriptives\nTable 4 shows a descriptive breakdown of cumulative opening duration, completion percentage, and positive reactions by visual fluency and substantive quality conditions.\n\n\nCode\nd &lt;- d_sw\n\nd |&gt; group_by(fluency, quality) |&gt; summarize(N = n(), Mean = mean(duration),\n      SD = sd(duration)) -&gt; temp_duration\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(completion), SD = sd(completion)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_completion\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(positive), SD = sd(positive)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_positive\n\ntemp &lt;- bind_cols(temp_duration, temp_completion, temp_positive)\nnames(temp) &lt;- c(\"Fluency\", \"Quality\", \"N\", \"Mean\", \"SD\", \"Mean\", \"SD\", \"Mean\", \"SD\")\ntemp |&gt; kbl(digits = 3, format.args = list(decimal.mark = \".\", big.mark = \",\")) |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 3, \"Opening Duration\" = 2, \"Completion Percentage\" = 2, \"Positive Reaction\" = 2))\n\n\n\n\nTable 4: Descriptive statistics (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\nPositive Reaction\n\n\n\nFluency\nQuality\nN\nMean\nSD\nMean\nSD\nMean\nSD\n\n\n\n\nhigh\nhigh\n1,091\n181.742\n167.906\n0.728\n0.278\n0.122\n0.327\n\n\nhigh\nlow\n1,080\n146.706\n127.484\n0.690\n0.272\n0.084\n0.278\n\n\nlow\nhigh\n1,126\n148.829\n131.286\n0.687\n0.283\n0.083\n0.277\n\n\nlow\nlow\n1,146\n94.798\n84.112\n0.613\n0.313\n0.024\n0.154"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage",
    "title": "Judging a Pitch by its Cover",
    "section": "4.2 Opening duration, completion percentage",
    "text": "4.2 Opening duration, completion percentage\nTable 5 shows the result of two factorial ANOVAs that model cumulative opening duration and completion percentage as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\nAdditionally, Table 6 shows the results of robustness check regressions in which we included investment experience, investor type, gender, and country as control variables in addition to fluency, quality, and their interaction (significant values with p &lt; .05 are printed in boldface). In all analyses, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n# combine results\ntemp &lt;- bind_rows(`Opening duration` = anova_tbl(duration ~ fluency * quality, d),\n                  `Completion percentage` = anova_tbl(completion ~ fluency * quality, d),\n                  .id = \"Measure\")\n\n# keep only first occurrence of each measure\ntemp$Measure[duplicated(temp$Measure)] &lt;- NA\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 2)) |&gt;\n  kbl(col.names = c(\"Measure\", \"Effect\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling()\n\n\n\n\nTable 5: ANOVA results for opening duration and completion (software startup)\n\n\n\n\n\n\n\nMeasure\nEffect\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nOpening duration\nFluency\n1996756.35\n1\n4439\n117.00\n&lt; .001\n.026\n\n\n\nQuality\n2201670.64\n1\n4439\n129.01\n&lt; .001\n.028\n\n\n\nFluency × Quality\n100126.67\n1\n4439\n5.87\n.015\n.001\n\n\nCompletion percentage\nFluency\n3.89\n1\n4439\n47.15\n&lt; .001\n.011\n\n\n\nQuality\n3.47\n1\n4439\n42.04\n&lt; .001\n.009\n\n\n\nFluency × Quality\n0.35\n1\n4439\n4.28\n.039\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# set reference levels\nd$location &lt;- relevel(as.factor(d$location), \"United States\")\nd$gender &lt;- relevel(as.factor(d$gender), \"female\")\nd$type &lt;- relevel(as.factor(d$type), \"Angel\")\n\n\n# run regressions\nglm_tbl(lm(duration ~ fluency * quality + investments + type + gender + location, d)) -&gt; temp_duration\nglm_tbl(lm(completion ~ fluency * quality + investments + type + gender + location, d), coef_digits = 3) -&gt; temp_completion\n\ntemp &lt;- bind_cols(temp_duration, temp_completion[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Opening Duration\" = 4, \"Completion Percentage\" = 4)) |&gt;\n  row_spec(16, extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 6: Robustness check regressions for opening duration and completion percentage with control variables (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n150.54\n6.614\n22.76\n&lt; .001\n0.671\n0.015\n46.15\n&lt; .001\n\n\nFluency\n21.32\n1.961\n10.87\n&lt; .001\n0.030\n0.004\n6.89\n&lt; .001\n\n\nQuality\n22.36\n1.964\n11.38\n&lt; .001\n0.028\n0.004\n6.48\n&lt; .001\n\n\nFluency × Quality\n-4.89\n1.962\n-2.49\n.013\n-0.009\n0.004\n-2.08\n.037\n\n\nInvestment experience\n0.13\n0.545\n0.24\n.809\n0.001\n0.001\n0.83\n.405\n\n\nInvestor type [Venture Capital]\n1.81\n5.206\n0.35\n.728\n-0.002\n0.011\n-0.19\n.852\n\n\nGender [Male]\n-8.22\n6.724\n-1.22\n.221\n0.004\n0.015\n0.30\n.761\n\n\nCountry [Brazil]\n-4.54\n25.722\n-0.18\n.860\n0.039\n0.057\n0.69\n.491\n\n\nCountry [Canada]\n8.39\n13.662\n0.61\n.539\n0.022\n0.030\n0.73\n.465\n\n\nCountry [China]\n-26.29\n28.027\n-0.94\n.348\n-0.050\n0.062\n-0.81\n.416\n\n\nCountry [France]\n-9.67\n15.473\n-0.63\n.532\n0.024\n0.034\n0.69\n.489\n\n\nCountry [Germany]\n-0.27\n13.134\n-0.02\n.983\n0.057\n0.029\n1.96\n.050\n\n\nCountry [India]\n12.18\n10.931\n1.11\n.265\n0.017\n0.024\n0.70\n.487\n\n\nCountry [Israel]\n-23.51\n17.968\n-1.31\n.191\n-0.014\n0.040\n-0.36\n.723\n\n\nCountry [Singapore]\n-43.21\n18.088\n-2.39\n.017\n-0.030\n0.040\n-0.75\n.455\n\n\nCountry [United Kingdom]\n-1.83\n7.581\n-0.24\n.809\n0.003\n0.017\n0.17\n.864\n\n\nR2\n.057\n\n\n\n.023\n\n\n\n\n\nR2adj.\n.054\n\n\n\n.019"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#positive-reactions",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#positive-reactions",
    "title": "Judging a Pitch by its Cover",
    "section": "4.3 Positive reactions",
    "text": "4.3 Positive reactions\nA logistic regression model was estimated to analyze the effects of visual fluency, substantive quality, and their interaction on whether there was a positive reaction to the emails. Table 7 shows the result of this regression model, next to several robustness check models that included control variables and / or were specified as Tobit models.\nIn all regressions, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# run regressions\nglm_tbl(glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive\nglm_tbl(glm(positive ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive_controls\nglm_tbl(AER::tobit(positive ~ fluency * quality, data = d), coef_digits = 3) -&gt; temp_positive_tobit\nglm_tbl(AER::tobit(positive ~ fluency * quality + investments + type + gender + location, data = d), coef_digits = 3) -&gt; temp_positive_controls_tobit\n\n\n# add empty rows to models w/o controls to enable column binding\nrows_control &lt;- nrow(temp_positive_controls)\nrows_simple &lt;- nrow(temp_positive)\ntemp_positive[(rows_simple+1):rows_control,] &lt;- NA\ntemp_positive_tobit[(rows_simple+1):rows_control,] &lt;- NA\n# put interaction and R2 row to the end\ntemp_positive[rows_control-1,] &lt;- temp_positive[rows_simple-1,]\ntemp_positive[rows_control,] &lt;- temp_positive[rows_simple,]\ntemp_positive[rows_simple-1,] &lt;- NA\ntemp_positive[rows_simple,] &lt;- NA\ntemp_positive_tobit[rows_control-1,] &lt;- temp_positive_tobit[rows_simple-1,]\ntemp_positive_tobit[rows_control,] &lt;- temp_positive_tobit[rows_simple,]\ntemp_positive_tobit[rows_simple-1,] &lt;- NA\ntemp_positive_tobit[rows_simple,] &lt;- NA\n\n# table binary logit\ntemp &lt;- bind_cols(temp_positive, temp_positive_controls[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Binary Logit\" = 4, \"Binary Logit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n# table tobit\ntemp &lt;- bind_cols(temp_positive_tobit, temp_positive_controls_tobit[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Nagelkerke's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Tobit Model\" = 4, \"Tobit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 7: Binary logit and Tobit regressions for positive reactions (software startup)\n\n\n\n\n\n\n\n(a) Binary logit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logit\n\n\nBinary Logit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.611\n0.066\n-39.82\n&lt; .001\n-2.634\n0.197\n-13.38\n&lt; .001\n\n\nFluency\n0.431\n0.066\n6.57\n&lt; .001\n0.431\n0.066\n6.56\n&lt; .001\n\n\nQuality\n0.426\n0.066\n6.49\n&lt; .001\n0.426\n0.066\n6.49\n&lt; .001\n\n\nFluency × Quality\n-0.220\n0.066\n-3.35\n&lt; .001\n-0.219\n0.066\n-3.34\n&lt; .001\n\n\nInvestment experience\n\n\n\n\n0.014\n0.012\n1.12\n.263\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.031\n0.147\n0.21\n.835\n\n\nGender [Male]\n\n\n\n\n0.013\n0.198\n0.07\n.947\n\n\nCountry [Brazil]\n\n\n\n\n-0.731\n1.026\n-0.71\n.476\n\n\nCountry [Canada]\n\n\n\n\n0.062\n0.379\n0.16\n.869\n\n\nCountry [China]\n\n\n\n\n-0.752\n1.030\n-0.73\n.466\n\n\nCountry [France]\n\n\n\n\n-0.410\n0.522\n-0.79\n.432\n\n\nCountry [Germany]\n\n\n\n\n0.129\n0.358\n0.36\n.718\n\n\nCountry [India]\n\n\n\n\n-0.406\n0.371\n-1.09\n.274\n\n\nCountry [Israel]\n\n\n\n\n0.445\n0.416\n1.07\n.284\n\n\nCountry [Singapore]\n\n\n\n\n0.580\n0.416\n1.39\n.164\n\n\nCountry [United Kingdom]\n\n\n\n\n-0.286\n0.241\n-1.19\n.236\n\n\nTjur's R2\n.017\n\n\n\n.020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tobit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTobit Model\n\n\nTobit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.708\n0.154\n-17.56\n&lt; .001\n-2.746\n0.229\n-11.98\n&lt; .001\n\n\nFluency\n0.370\n0.057\n6.49\n&lt; .001\n0.372\n0.057\n6.51\n&lt; .001\n\n\nQuality\n0.366\n0.057\n6.41\n&lt; .001\n0.364\n0.057\n6.38\n&lt; .001\n\n\nFluency × Quality\n-0.176\n0.055\n-3.18\n.001\n-0.176\n0.055\n-3.19\n.001\n\n\nInvestment experience\n\n\n\n\n0.012\n0.012\n0.98\n.326\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.017\n0.133\n0.13\n.900\n\n\nGender [Male]\n\n\n\n\n0.042\n0.179\n0.23\n.816\n\n\nCountry [Brazil]\n\n\n\n\n-0.631\n0.837\n-0.75\n.451\n\n\nCountry [Canada]\n\n\n\n\n0.084\n0.344\n0.24\n.808\n\n\nCountry [China]\n\n\n\n\n-0.625\n0.847\n-0.74\n.460\n\n\nCountry [France]\n\n\n\n\n-0.379\n0.451\n-0.84\n.400\n\n\nCountry [Germany]\n\n\n\n\n0.193\n0.322\n0.60\n.549\n\n\nCountry [India]\n\n\n\n\n-0.376\n0.320\n-1.18\n.240\n\n\nCountry [Israel]\n\n\n\n\n0.391\n0.404\n0.97\n.333\n\n\nCountry [Singapore]\n\n\n\n\n0.547\n0.402\n1.36\n.173\n\n\nCountry [United Kingdom]\n\n\n\n\n-0.237\n0.210\n-1.13\n.260\n\n\nNagelkerke's R2\n.040\n\n\n\n.044"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#percentage-increases",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#percentage-increases",
    "title": "Judging a Pitch by its Cover",
    "section": "4.4 Percentage increases",
    "text": "4.4 Percentage increases\nTo facilitate the interpretation of the results, we calculated the percentage increase in the dependent variables for the high level of visual fluency and substantive quality compared to the respective low level. Table 8 shows these percentage increases for the opening duration, completion percentage, and positive reactions.\n\n\nCode\n# calculate percentage increases based on marginal means\n#\n# compute marginal means using the `marginaleffects` package\n#\n# duration\nm_dur &lt;- aov(duration ~ fluency * quality, d)\nmm_dur_flu &lt;- marginaleffects::predictions(m_dur, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_dur_qual &lt;- marginaleffects::predictions(m_dur, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# completion\nm_compl &lt;- aov(completion ~ fluency * quality, d)\nmm_compl_flu &lt;- marginaleffects::predictions(m_compl, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_compl_qual &lt;- marginaleffects::predictions(m_compl, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# positive reactions\nm_pos &lt;- glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), data = d)\nmm_pos_flu &lt;- marginaleffects::predictions(m_pos, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_pos_qual &lt;- marginaleffects::predictions(m_pos, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n\n# compute effect sizes as percentage increases for high vs. low levels\n#\n# we use the formula: 100 * (high / low - 1)\n# That is, we multiply by 100 to get the percentage increase and then\n# subtract 1 to get the percentage increase relative to the low level.\n#\n# duration: fluency\nperc_incr_dur_flu &lt;- 100 * (mm_dur_flu$estimate[mm_dur_flu$fluency == \"high\"] / mm_dur_flu$estimate[mm_dur_flu$fluency == \"low\"] - 1)\n# duration: quality\nperc_incr_dur_qual &lt;- 100 * (mm_dur_qual$estimate[mm_dur_qual$quality == \"high\"] / mm_dur_qual$estimate[mm_dur_qual$quality == \"low\"] - 1)\n\n# completion: fluency\nperc_incr_compl_flu &lt;- 100 * (mm_compl_flu$estimate[mm_compl_flu$fluency == \"high\"] / mm_compl_flu$estimate[mm_compl_flu$fluency == \"low\"] - 1)\n# completion: quality\nperc_incr_compl_qual &lt;- 100 * (mm_compl_qual$estimate[mm_compl_qual$quality == \"high\"] / mm_compl_qual$estimate[mm_compl_qual$quality == \"low\"] - 1)\n\n# positive: fluency\nperc_incr_pos_flu &lt;- 100 * (mm_pos_flu$estimate[mm_pos_flu$fluency == \"high\"] / mm_pos_flu$estimate[mm_pos_flu$fluency == \"low\"] - 1)\n# positive: quality\nperc_incr_pos_qual &lt;- 100 * (mm_pos_qual$estimate[mm_pos_qual$quality == \"high\"] / mm_pos_qual$estimate[mm_pos_qual$quality == \"low\"] - 1)\n\n# create table\ndata.frame(\n  Measure = c(\"Opening duration\", \"Completion percentage\", \"Positive reaction\"),\n  Fluency = c(sprintf('%.1f%%', perc_incr_dur_flu), sprintf('%.1f%%', perc_incr_compl_flu), sprintf('%.1f%%', perc_incr_pos_flu)),\n  Quality = c(sprintf('%.1f%%', perc_incr_dur_qual), sprintf('%.1f%%', perc_incr_compl_qual), sprintf('%.1f%%', perc_incr_pos_qual))\n) |&gt; kbl() |&gt; kable_styling()\n\n\n\n\nTable 8: Percentage increases for high vs. low levels of visual fluency and substantive quality (software startup)\n\n\n\n\n\n\n\nMeasure\nFluency\nQuality\n\n\n\n\nOpening duration\n34.8%\n36.9%\n\n\nCompletion percentage\n9.1%\n8.6%\n\n\nPositive reaction\n122.8%\n120.7%"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#descriptives-2",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#descriptives-2",
    "title": "Judging a Pitch by its Cover",
    "section": "5.1 Descriptives",
    "text": "5.1 Descriptives\nTable 9 shows a descriptive breakdown of cumulative opening duration, completion percentage, and positive reactions by visual fluency and substantive quality conditions.\n\n\nCode\nd &lt;- d_hc\n\nd |&gt; group_by(fluency, quality) |&gt; summarize(N = n(), Mean = mean(duration),\n      SD = sd(duration)) -&gt; temp_duration\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(completion), SD = sd(completion)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_completion\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(positive), SD = sd(positive)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_positive\n\ntemp &lt;- bind_cols(temp_duration, temp_completion, temp_positive)\nnames(temp) &lt;- c(\"Fluency\", \"Quality\", \"N\", \"Mean\", \"SD\", \"Mean\", \"SD\", \"Mean\", \"SD\")\ntemp |&gt; kbl(digits = 3) |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 3, \"Opening Duration\" = 2, \"Completion Percentage\" = 2, \"Positive Reaction\" = 2))\n\n\n\n\nTable 9: Descriptive statistics (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\nPositive Reaction\n\n\n\nFluency\nQuality\nN\nMean\nSD\nMean\nSD\nMean\nSD\n\n\n\n\nhigh\nhigh\n589\n125.431\n90.226\n0.775\n0.228\n0.171\n0.377\n\n\nhigh\nlow\n605\n100.198\n88.423\n0.688\n0.245\n0.094\n0.292\n\n\nlow\nhigh\n593\n99.042\n77.849\n0.691\n0.253\n0.099\n0.300\n\n\nlow\nlow\n620\n69.044\n64.892\n0.586\n0.294\n0.037\n0.189"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage-1",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#opening-duration-completion-percentage-1",
    "title": "Judging a Pitch by its Cover",
    "section": "5.2 Opening duration, completion percentage",
    "text": "5.2 Opening duration, completion percentage\nTable 10 shows the result of two factorial ANOVAs that model cumulative opening duration and completion percentage as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\nAdditionally, Table 11 shows the results of robustness check regressions in which we included investment experience, investor type, gender, and country as control variables in addition to fluency, quality, and their interaction (significant values with p &lt; .05 are printed in boldface). In all analyses, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n# combine results\ntemp &lt;- bind_rows(`Opening duration` = anova_tbl(duration ~ fluency * quality, d),\n                  `Completion percentage` = anova_tbl(completion ~ fluency * quality, d),\n                  .id = \"Measure\")\n\n# keep only first occurrence of each measure\ntemp$Measure[duplicated(temp$Measure)] &lt;- NA\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 2)) |&gt;\n  kbl(col.names = c(\"Measure\", \"Effect\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling()\n\n\n\n\nTable 10: ANOVA results for opening duration and completion (healthcare startup)\n\n\n\n\n\n\n\nMeasure\nEffect\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nOpening duration\nFluency\n497944.86\n1\n2403\n76.20\n&lt; .001\n.031\n\n\n\nQuality\n458729.52\n1\n2403\n70.20\n&lt; .001\n.028\n\n\n\nFluency × Quality\n3415.38\n1\n2403\n0.52\n.470\n&lt; .001\n\n\nCompletion percentage\nFluency\n5.21\n1\n2403\n79.11\n&lt; .001\n.032\n\n\n\nQuality\n5.56\n1\n2403\n84.53\n&lt; .001\n.034\n\n\n\nFluency × Quality\n0.05\n1\n2403\n0.83\n.364\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# set reference levels\nd$location &lt;- relevel(as.factor(d$location), \"United States\")\nd$gender &lt;- relevel(as.factor(d$gender), \"female\")\nd$type &lt;- relevel(as.factor(d$type), \"Angel\")\n\n\n# run regressions\nglm_tbl(lm(duration ~ fluency * quality + investments + type + gender + location, d)) -&gt; temp_duration\nglm_tbl(lm(completion ~ fluency * quality + investments + type + gender + location, d), coef_digits = 3) -&gt; temp_completion\n\ntemp &lt;- bind_cols(temp_duration, temp_completion[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Opening Duration\" = 4, \"Completion Percentage\" = 4)) |&gt;\n  row_spec(16, extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 11: Robustness check regressions for opening duration and completion percentage with control variables (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n95.01\n5.426\n17.51\n&lt; .001\n0.680\n0.017\n39.44\n&lt; .001\n\n\nFluency\n14.32\n1.656\n8.65\n&lt; .001\n0.046\n0.005\n8.83\n&lt; .001\n\n\nQuality\n13.91\n1.655\n8.40\n&lt; .001\n0.048\n0.005\n9.15\n&lt; .001\n\n\nFluency × Quality\n-1.19\n1.651\n-0.72\n.471\n-0.005\n0.005\n-0.86\n.389\n\n\nInvestment experience\n0.50\n0.941\n0.54\n.592\n0.004\n0.003\n1.43\n.153\n\n\nInvestor type [Venture Capital]\n-1.86\n3.812\n-0.49\n.626\n-0.017\n0.012\n-1.44\n.151\n\n\nGender [Male]\n3.21\n5.231\n0.61\n.539\n0.000\n0.017\n0.02\n.986\n\n\nCountry [Brazil]\n28.17\n20.980\n1.34\n.180\n0.056\n0.067\n0.84\n.402\n\n\nCountry [Canada]\n13.84\n11.540\n1.20\n.231\n0.032\n0.037\n0.88\n.377\n\n\nCountry [China]\n-2.59\n12.788\n-0.20\n.839\n-0.031\n0.041\n-0.76\n.449\n\n\nCountry [France]\n-7.84\n10.027\n-0.78\n.434\n-0.012\n0.032\n-0.36\n.716\n\n\nCountry [Germany]\n5.27\n10.580\n0.50\n.618\n0.018\n0.034\n0.55\n.585\n\n\nCountry [India]\n8.41\n10.985\n0.77\n.444\n0.021\n0.035\n0.61\n.541\n\n\nCountry [Israel]\n28.49\n14.909\n1.91\n.056\n0.004\n0.047\n0.08\n.935\n\n\nCountry [Singapore]\n-11.03\n13.430\n-0.82\n.412\n0.056\n0.043\n1.31\n.189\n\n\nCountry [United Kingdom]\n-4.62\n6.178\n-0.75\n.455\n0.023\n0.020\n1.18\n.236\n\n\nR2\n.062\n\n\n\n.068\n\n\n\n\n\nR2adj.\n.056\n\n\n\n.062"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#positive-reactions-1",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#positive-reactions-1",
    "title": "Judging a Pitch by its Cover",
    "section": "5.3 Positive reactions",
    "text": "5.3 Positive reactions\nA logistic regression model was estimated to analyze the effects of visual fluency, substantive quality, and their interaction on whether there was a positive reaction to the emails. Table 12 shows the result of this regression model, next to several robustness check models that included control variables and / or were specified as Tobit models.\nIn all regressions, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# run regressions\nglm_tbl(glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive\nglm_tbl(glm(positive ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive_controls\nglm_tbl(AER::tobit(positive ~ fluency * quality, data = d), coef_digits = 3) -&gt; temp_positive_tobit\nglm_tbl(AER::tobit(positive ~ fluency * quality + investments + type + gender + location, data = d), coef_digits = 3) -&gt; temp_positive_controls_tobit\n\n\n# add empty rows to models w/o controls to enable column binding\nrows_control &lt;- nrow(temp_positive_controls)\nrows_simple &lt;- nrow(temp_positive)\ntemp_positive[(rows_simple+1):rows_control,] &lt;- NA\ntemp_positive_tobit[(rows_simple+1):rows_control,] &lt;- NA\n# put interaction and R2 row to the end\ntemp_positive[rows_control-1,] &lt;- temp_positive[rows_simple-1,]\ntemp_positive[rows_control,] &lt;- temp_positive[rows_simple,]\ntemp_positive[rows_simple-1,] &lt;- NA\ntemp_positive[rows_simple,] &lt;- NA\ntemp_positive_tobit[rows_control-1,] &lt;- temp_positive_tobit[rows_simple-1,]\ntemp_positive_tobit[rows_control,] &lt;- temp_positive_tobit[rows_simple,]\ntemp_positive_tobit[rows_simple-1,] &lt;- NA\ntemp_positive_tobit[rows_simple,] &lt;- NA\n\n# table binary logit\ntemp &lt;- bind_cols(temp_positive, temp_positive_controls[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Binary Logit\" = 4, \"Binary Logit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n# table tobit\ntemp &lt;- bind_cols(temp_positive_tobit, temp_positive_controls_tobit[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Nagelkerke's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Tobit Model\" = 4, \"Tobit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 12: Binary logit and Tobit regressions for positive reactions (healthcare startup)\n\n\n\n\n\n\n\n(a) Binary logit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logit\n\n\nBinary Logit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.324\n0.077\n-30.12\n&lt; .001\n-2.068\n0.214\n-9.65\n&lt; .001\n\n\nFluency\n0.405\n0.077\n5.25\n&lt; .001\n0.401\n0.078\n5.16\n&lt; .001\n\n\nQuality\n0.435\n0.077\n5.64\n&lt; .001\n0.448\n0.078\n5.76\n&lt; .001\n\n\nFluency × Quality\n-0.091\n0.077\n-1.18\n.236\n-0.093\n0.078\n-1.20\n.229\n\n\nInvestment experience\n\n\n\n\n-0.020\n0.049\n-0.41\n.679\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.059\n0.165\n0.36\n.721\n\n\nGender [Male]\n\n\n\n\n-0.360\n0.201\n-1.79\n.074\n\n\nCountry [Brazil]\n\n\n\n\n-0.503\n1.045\n-0.48\n.630\n\n\nCountry [Canada]\n\n\n\n\n0.452\n0.425\n1.06\n.287\n\n\nCountry [China]\n\n\n\n\n-0.205\n0.545\n-0.38\n.707\n\n\nCountry [France]\n\n\n\n\n0.783\n0.328\n2.39\n.017\n\n\nCountry [Germany]\n\n\n\n\n0.608\n0.380\n1.60\n.110\n\n\nCountry [India]\n\n\n\n\n-0.324\n0.532\n-0.61\n.542\n\n\nCountry [Israel]\n\n\n\n\n0.099\n0.624\n0.16\n.874\n\n\nCountry [Singapore]\n\n\n\n\n0.618\n0.462\n1.34\n.181\n\n\nCountry [United Kingdom]\n\n\n\n\n0.131\n0.251\n0.52\n.602\n\n\nTjur's R2\n.025\n\n\n\n.033\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tobit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTobit Model\n\n\nTobit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.287\n0.162\n-14.15\n&lt; .001\n-2.012\n0.230\n-8.73\n&lt; .001\n\n\nFluency\n0.344\n0.066\n5.23\n&lt; .001\n0.337\n0.066\n5.14\n&lt; .001\n\n\nQuality\n0.370\n0.066\n5.60\n&lt; .001\n0.377\n0.066\n5.70\n&lt; .001\n\n\nFluency × Quality\n-0.063\n0.063\n-0.99\n.323\n-0.062\n0.063\n-0.98\n.326\n\n\nInvestment experience\n\n\n\n\n-0.017\n0.041\n-0.42\n.676\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.022\n0.142\n0.15\n.878\n\n\nGender [Male]\n\n\n\n\n-0.337\n0.179\n-1.88\n.060\n\n\nCountry [Brazil]\n\n\n\n\n-0.371\n0.836\n-0.44\n.657\n\n\nCountry [Canada]\n\n\n\n\n0.414\n0.382\n1.08\n.279\n\n\nCountry [China]\n\n\n\n\n-0.141\n0.466\n-0.30\n.762\n\n\nCountry [France]\n\n\n\n\n0.689\n0.308\n2.23\n.026\n\n\nCountry [Germany]\n\n\n\n\n0.557\n0.346\n1.61\n.107\n\n\nCountry [India]\n\n\n\n\n-0.238\n0.436\n-0.55\n.586\n\n\nCountry [Israel]\n\n\n\n\n0.019\n0.555\n0.03\n.973\n\n\nCountry [Singapore]\n\n\n\n\n0.483\n0.433\n1.11\n.266\n\n\nCountry [United Kingdom]\n\n\n\n\n0.143\n0.217\n0.66\n.510\n\n\nNagelkerke's R2\n.047\n\n\n\n.056"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#percentage-increases-1",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#percentage-increases-1",
    "title": "Judging a Pitch by its Cover",
    "section": "5.4 Percentage increases",
    "text": "5.4 Percentage increases\nTo facilitate the interpretation of the results, we calculated the percentage increase in the dependent variables for the high level of visual fluency and substantive quality compared to the respective low level. Table 13 shows these percentage increases for the opening duration, completion percentage, and positive reactions.\n\n\nCode\n# calculate percentage increases based on marginal means\n#\n# compute marginal means using the `marginaleffects` package\n#\n# duration\nm_dur &lt;- aov(duration ~ fluency * quality, d)\nmm_dur_flu &lt;- marginaleffects::predictions(m_dur, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_dur_qual &lt;- marginaleffects::predictions(m_dur, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# completion\nm_compl &lt;- aov(completion ~ fluency * quality, d)\nmm_compl_flu &lt;- marginaleffects::predictions(m_compl, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_compl_qual &lt;- marginaleffects::predictions(m_compl, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# positive reactions\nm_pos &lt;- glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), data = d)\nmm_pos_flu &lt;- marginaleffects::predictions(m_pos, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_pos_qual &lt;- marginaleffects::predictions(m_pos, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n\n# compute effect sizes as percentage increases for high vs. low levels\n#\n# we use the formula: 100 * (high / low - 1)\n# That is, we multiply by 100 to get the percentage increase and then\n# subtract 1 to get the percentage increase relative to the low level.\n#\n# duration: fluency\nperc_incr_dur_flu &lt;- 100 * (mm_dur_flu$estimate[mm_dur_flu$fluency == \"high\"] / mm_dur_flu$estimate[mm_dur_flu$fluency == \"low\"] - 1)\n# duration: quality\nperc_incr_dur_qual &lt;- 100 * (mm_dur_qual$estimate[mm_dur_qual$quality == \"high\"] / mm_dur_qual$estimate[mm_dur_qual$quality == \"low\"] - 1)\n\n# completion: fluency\nperc_incr_compl_flu &lt;- 100 * (mm_compl_flu$estimate[mm_compl_flu$fluency == \"high\"] / mm_compl_flu$estimate[mm_compl_flu$fluency == \"low\"] - 1)\n# completion: quality\nperc_incr_compl_qual &lt;- 100 * (mm_compl_qual$estimate[mm_compl_qual$quality == \"high\"] / mm_compl_qual$estimate[mm_compl_qual$quality == \"low\"] - 1)\n\n# positive: fluency\nperc_incr_pos_flu &lt;- 100 * (mm_pos_flu$estimate[mm_pos_flu$fluency == \"high\"] / mm_pos_flu$estimate[mm_pos_flu$fluency == \"low\"] - 1)\n# positive: quality\nperc_incr_pos_qual &lt;- 100 * (mm_pos_qual$estimate[mm_pos_qual$quality == \"high\"] / mm_pos_qual$estimate[mm_pos_qual$quality == \"low\"] - 1)\n\n# create table\ndata.frame(\n  Measure = c(\"Opening duration\", \"Completion percentage\", \"Positive reaction\"),\n  Fluency = c(sprintf('%.1f%%', perc_incr_dur_flu), sprintf('%.1f%%', perc_incr_compl_flu), sprintf('%.1f%%', perc_incr_pos_flu)),\n  Quality = c(sprintf('%.1f%%', perc_incr_dur_qual), sprintf('%.1f%%', perc_incr_compl_qual), sprintf('%.1f%%', perc_incr_pos_qual))\n) |&gt; kbl() |&gt; kable_styling()\n\n\n\n\nTable 13: Percentage increases for high vs. low levels of visual fluency and substantive quality (healthcare startup)\n\n\n\n\n\n\n\nMeasure\nFluency\nQuality\n\n\n\n\nOpening duration\n34.2%\n32.6%\n\n\nCompletion percentage\n14.6%\n15.1%\n\n\nPositive reaction\n108.9%\n120.6%"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#sankey-plots",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#sankey-plots",
    "title": "Judging a Pitch by its Cover",
    "section": "6.1 Sankey plots",
    "text": "6.1 Sankey plots\nFigure 1 (a) and Figure 1 (b) show the flow of the field experiment for the software and healthcare startups, respectively, as Sankey diagram.\n\n\nCode\n# aesthetics\n#\n# define colors\nblue_main &lt;- \"#297FB8\"\nblue_dark &lt;- \"#2D3E50\"\nblue_light &lt;- \"#A0ABBF\"\n\n# define custom geom\ngeom_sankey_label_richtext &lt;- function (mapping = NULL, data = NULL, position = \"identity\", \n  na.rm = FALSE, show.legend = NA, space = NULL, type = \"sankey\", width = 0.1,\n  inherit.aes = TRUE, family =\n    if (.Platform$OS.type == \"windows\") \"Roboto Condensed\" else \"Roboto Condensed Light\", ...)  # added font family\n{\n  label.aes &lt;- list(...)\n  list(label = ggplot2::layer(stat = ggsankey:::StatSankeyText, data = data, \n    mapping = mapping, geom = \"richtext\", position = position, # changed: geom = \"label\"\n    show.legend = show.legend, inherit.aes = inherit.aes, \n    params = purrr::flatten(list(na.rm = na.rm, width = width, \n      space = space, type = type, label.aes, family = family))))\n}\n\n# change ipsum theme to work for sankey plot\ntheme_sankey_ipsum_rc &lt;- function (base_family = \"Roboto Condensed\", base_size = 11.5, ...)\n{\n  {\n    hrbrthemes::theme_ipsum_rc(base_family = base_family, base_size = base_size, ...) %+replace% \n      ggplot2::theme(panel.border = ggplot2::element_blank(), \n        panel.grid.major = ggplot2::element_blank(), \n        panel.grid.minor = ggplot2::element_blank(), \n        axis.line = ggplot2::element_line(colour = \"black\", \n          linewidth = ggplot2::rel(1)), legend.key = ggplot2::element_blank(), \n        strip.background = ggplot2::element_rect(fill = \"white\", \n          colour = \"transparent\", linewidth = ggplot2::rel(2)), \n        complete = TRUE, axis.line.y = ggplot2::element_blank(), \n        axis.line.x = ggplot2::element_blank(),\n        axis.text.x = ggplot2::element_blank(), \n        axis.text.y = ggplot2::element_blank(), \n        axis.ticks.y = ggplot2::element_blank(),\n        axis.ticks.x = ggplot2::element_blank(),\n        # set background to white for quarto to avoid transparency issues\n        panel.background = ggplot2::element_rect(fill='white', color='white'),\n        plot.background = ggplot2::element_rect(fill='white', color='white')\n      )\n  }\n}\n\n\n# Software startup --------------------------------------------------\n#\n# prepare sankey data (the data is already loaded at the beginning of the script)\n#\n# create binary indicators for the different stages\nsankey_soft |&gt; mutate(\n  `Sent mails` = case_when(\n    hard_bounce == 1 ~ \"Hard bounce\",\n    hard_bounce == 0 ~ \"Delivered\"\n  ),\n  Delivered = case_when(\n    hard_bounce == 0 & is.na(date_first_click) ~ \"Recipient did not open deck\",\n    hard_bounce == 0 & !is.na(date_first_click) ~ \"Recipient opened deck\"\n  ),\n  `Recipient opened deck` = case_when(\n    hard_bounce == 0 & !is.na(date_first_click) & is.na(date_reply) ~ \"Recipient did not reply\",\n    hard_bounce == 0 & !is.na(date_reply) ~ \"Recipient replied\"\n  ),\n  `Recipient replied` = case_when(\n    positive == 0 ~ \"Negative reply\",\n    positive == 1 ~ \"Positive reply\"\n  ),\n  Category = category,\n  `Initial sample` = \"Software&lt;br&gt;startup\"\n) -&gt; sankey_soft\n\n# make data long for sankey graph (for simplicity, just called `df`)\nsankey_soft |&gt; make_long(`Initial sample`, `Sent mails`, Delivered,\n                         `Recipient opened deck`, `Recipient replied`,\n                         Category) -&gt; df\n\n# add group_nodes (needed for calculation of group percentages later)\ndf |&gt; mutate(\n  group_node = case_when(\n    node == \"Hard bounce\" | node == \"Delivered\" ~ \"Software&lt;br&gt;startup\",\n    node == \"Recipient did not open deck\" | node == \"Recipient opened deck\" ~ \"Delivered\",\n    node == \"Recipient did not reply\" | node == \"Recipient replied\" ~ \"Recipient opened deck\",\n    node == \"Negative reply\" | node == \"Positive reply\" ~ \"Recipient replied\",\n    node == \"other\" | node == \"geography\" | node == \"no investments\" | node == \"stage\" | node == \"investment strategy\" | node == \"industry\" | node == \"no specific reason\" ~ \"Negative reply\",\n    node == \"more info/clarification\" | node == \"meeting\" | node == \"formal application\" | node == \"updates\" | node == \"referral\" ~ \"Positive reply\"\n  )  \n) -&gt; df\n\n# add information about node N and group_node N and calculate percentages\ndf |&gt;\n  # count obs per node\n  group_by(node) |&gt; mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  # count obs per group_node\n  group_by(group_node) |&gt; mutate(n_group = ifelse(is.na(group_node), NA, n())) |&gt; \n  ungroup() |&gt;\n  # add percentages\n  mutate(pct = n/n_group) -&gt; df\n\n# manually change order of nodes\ndf |&gt; mutate(\n  node = factor(node,\n                levels = c(\"Software&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  ),\n  next_node = factor(next_node,\n                levels = c(\"Software&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  )\n) -&gt; df\n\n# make sankey plot\n#\n# first change the data so that percentages are displayed in the node texts\ndf |&gt; tidyr::drop_na(node) |&gt;\n  mutate(\n    pct = ifelse(is.na(pct), 9999, pct), # dummy-replace NA with 9999, otherwise later ifelse does not work\n    node_text = ifelse(\n      # group_node and thus pct empty (here: 9999)\n      pct == 9999,\n      # yes, pct empty -&gt; no percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s&lt;/span&gt;\",\n                        node, scales::comma(n)),\n      # no, pct not empty -&gt; add percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s (%s%%)&lt;/span&gt;\",\n                        node,\n                        scales::comma(n),\n                 ifelse(pct&lt;.01, paste0(\"0\",weights::rd(pct*100,2)), weights::rd(pct*100,2)))\n      )) |&gt;\n  # now create the plot\n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node),\n             label = node_text,\n             color = factor(node)\n             )) +\n  geom_sankey(flow.alpha = 0.65, show.legend = FALSE, node.color = blue_dark, node.fill = blue_dark) +\n  geom_sankey_label_richtext(size = 3, color = \"black\", fill = \"white\") +\n  labs(x = element_blank()) +\n  # apply customized theme\n  theme_sankey_ipsum_rc(base_size = 11, plot_margin = margin(5, 5, 5, 5)) -&gt; p\n\n# more customizing\n#\n# now: change the color of the segments\n# to this end, first decompose the plot into its parts using `ggplot_build`\nq &lt;- ggplot_build(p)\n\n# first data layer is for line color of flows\nl1 &lt;- q$data[[1]]$colour\n# second data layer is for line color of nodes\nl2 &lt;- q$data[[2]]$colour\n\n# fill colors\nf1 &lt;- q$data[[1]]$fill # flows\nf2 &lt;- q$data[[2]]$fill # nodes\n\n# relevant flows are all of length 600, and only starting color value is relevant\n# thus, color change points (ccp) are\nccp &lt;- seq(1, length(f1), by = 600)\nq$data[[1]]$fill[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$fill[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$fill[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$fill[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$fill[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$fill[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$fill[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$fill[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$fill[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$fill[ccp[16:20]] &lt;- blue_main # positive categories\n\nq$data[[1]]$colour[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$colour[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$colour[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$colour[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$colour[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$colour[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$colour[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$colour[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$colour[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$colour[ccp[16:20]] &lt;- blue_main # positive categories\n\n# put all back together and plot the modified, final plot\np_mod &lt;- ggplot_gtable(q)\nplot(p_mod)\n\n\n# Healthcare startup ------------------------------------------------\n#\n# prepare sankey data (the data is already loaded at the beginning of the script)\n#\n# create binary indicators for the different stages\nsankey_health |&gt; mutate(\n  `Sent mails` = case_when(\n    hard_bounce == 1 ~ \"Hard bounce\",\n    hard_bounce == 0 ~ \"Delivered\"\n  ),\n  Delivered = case_when(\n    hard_bounce == 0 & is.na(date_first_click) ~ \"Recipient did not open deck\",\n    hard_bounce == 0 & !is.na(date_first_click) ~ \"Recipient opened deck\"\n  ),\n  `Recipient opened deck` = case_when(\n    hard_bounce == 0 & !is.na(date_first_click) & is.na(date_reply) ~ \"Recipient did not reply\",\n    hard_bounce == 0 & !is.na(date_reply) ~ \"Recipient replied\"\n  ),\n  `Recipient replied` = case_when(\n    positive == 0 ~ \"Negative reply\",\n    positive == 1 ~ \"Positive reply\"\n  ),\n  Category = category,\n  `Initial sample` = \"Healthcare&lt;br&gt;startup\"\n) -&gt; sankey_health\n\n# make data long for sankey graph (for simplicity, just called `df`)\nsankey_health |&gt; make_long(`Initial sample`, `Sent mails`, Delivered,\n                         `Recipient opened deck`, `Recipient replied`,\n                         Category) -&gt; df\n\n# add group_nodes (needed for calculation of group percentages later)\ndf |&gt; mutate(\n  group_node = case_when(\n    node == \"Hard bounce\" | node == \"Delivered\" ~ \"Healthcare&lt;br&gt;startup\",\n    node == \"Recipient did not open deck\" | node == \"Recipient opened deck\" ~ \"Delivered\",\n    node == \"Recipient did not reply\" | node == \"Recipient replied\" ~ \"Recipient opened deck\",\n    node == \"Negative reply\" | node == \"Positive reply\" ~ \"Recipient replied\",\n    node == \"other\" | node == \"geography\" | node == \"no investments\" | node == \"stage\" | node == \"investment strategy\" | node == \"industry\" | node == \"no specific reason\" ~ \"Negative reply\",\n    node == \"more info/clarification\" | node == \"meeting\" | node == \"formal application\" | node == \"updates\" | node == \"referral\" ~ \"Positive reply\"\n  )  \n) -&gt; df\n\n# add information about node N and group_node N and calculate percentages\ndf |&gt;\n  # count obs per node\n  group_by(node) |&gt; mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  # count obs per group_node\n  group_by(group_node) |&gt; mutate(n_group = ifelse(is.na(group_node), NA, n())) |&gt; \n  ungroup() |&gt;\n  # add percentages\n  mutate(pct = n/n_group) -&gt; df\n\n# manually change order of nodes\ndf |&gt; mutate(\n  node = factor(node,\n                levels = c(\"Healthcare&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  ),\n  next_node = factor(next_node,\n                levels = c(\"Healthcare&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  )\n) -&gt; df\n\n# make sankey plot\n#\n# first change the data so that percentages are displayed in the node texts\ndf |&gt; tidyr::drop_na(node) |&gt;\n  mutate(\n    pct = ifelse(is.na(pct), 9999, pct), # dummy-replace NA with 9999, otherwise later ifelse does not work\n    node_text = ifelse(\n      # group_node and thus pct empty (here: 9999)\n      pct == 9999,\n      # yes, pct empty -&gt; no percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s&lt;/span&gt;\",\n                        node, scales::comma(n)),\n      # no, pct not empty -&gt; add percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s (%s%%)&lt;/span&gt;\",\n                        node,\n                        scales::comma(n),\n                 ifelse(pct&lt;.01, paste0(\"0\",weights::rd(pct*100,2)), weights::rd(pct*100,2)))\n      )) |&gt;\n  # now create the plot\n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node),\n             label = node_text,\n             color = factor(node)\n             )) +\n  geom_sankey(flow.alpha = 0.65, show.legend = FALSE, node.color = blue_dark, node.fill = blue_dark) +\n  geom_sankey_label_richtext(size = 3, color = \"black\", fill = \"white\") +\n  labs(x = element_blank()) +\n  # apply customized theme\n  theme_sankey_ipsum_rc(base_size = 11, plot_margin = margin(5, 5, 5, 5)) -&gt; p\n\n# more customizing\n#\n# now: change the color of the segments\n# to this end, first decompose the plot into its parts using `ggplot_build`\nq &lt;- ggplot_build(p)\n\n# first data layer is for line color of flows\nl1 &lt;- q$data[[1]]$colour\n# second data layer is for line color of nodes\nl2 &lt;- q$data[[2]]$colour\n\n# fill colors\nf1 &lt;- q$data[[1]]$fill # flows\nf2 &lt;- q$data[[2]]$fill # nodes\n\n# relevant flows are all of length 600, and only starting color value is relevant\n# thus, color change points (ccp) are\nccp &lt;- seq(1, length(f1), by = 600)\nq$data[[1]]$fill[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$fill[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$fill[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$fill[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$fill[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$fill[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$fill[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$fill[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$fill[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$fill[ccp[16:20]] &lt;- blue_main # positive categories\n\nq$data[[1]]$colour[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$colour[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$colour[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$colour[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$colour[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$colour[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$colour[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$colour[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$colour[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$colour[ccp[16:20]] &lt;- blue_main # positive categories\n\n# put all back together and plot the modified, final plot\np_mod &lt;- ggplot_gtable(q)\nplot(p_mod)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Software startup\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Healthcare startup\n\n\n\n\n\n\n\nFigure 1: Sankey diagrams of the field experiment flow"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#main-results",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#main-results",
    "title": "Judging a Pitch by its Cover",
    "section": "6.2 Main results",
    "text": "6.2 Main results\nFigure 2 shows the main results visually. Figure 2 (a) shows the results for the opening duration of the pitch decks, Figure 2 (b) for the percentage of pitch decks slides that were viewed, and Figure 2 (c) for the share of positive investor reactions. Note that the significance brackets represent post-hoc contrasts with Holm (1979) correction (** p &lt; .01; *** p &lt; .001).\n\n\nCode\n# Helper functions and aesthetics\n#\n# pval function\npval &lt;- function(p, stars = TRUE){\n  if(stars){\n    if(p &lt; .001) return(\"***\")\n    if(p &lt; .01) return(\"**\")\n    if(p &lt; .05) return(\"*\")\n    if(p &gt;= .05) return(\"NS\")\n  } else{\n    scales::pvalue(p, prefix = c(\"p &lt; \", \"p = \", \"p &gt; \"))\n  }\n}\n\n# theme settings\nmy_style &lt;- list(hrbrthemes::theme_ipsum_rc(),\n                 scale_fill_manual(values=c(blue_light, blue_dark)))\nmy_theme &lt;- theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = .5),\n        axis.title.y = element_text(size = 12, hjust = .5, margin = margin(t = 0, r = 10, b = 0, l = 0)),\n        axis.title.x = element_text(size = 12, hjust = .5, margin = margin(t = 5, r = 0, b = 0, l = 0)),\n        )\n# set up titles, axis names etc.\nmy_labs &lt;- labs(\n  x = \"Quality\", \n  shape='Fluency'\n)\n\n\n# Main figures: software startup\n#\n# data prep\n#\n# for convenience and to not interfere with later code, we work on a copy of\n# the data\nd_soft &lt;- d_sw\n# convert fluency and quality to factor vars\nd_soft$fluency &lt;- factor(d_soft$fluency, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\nd_soft$quality &lt;- factor(d_soft$quality, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\n\n# create dataset for post-hoc contrasts\n# --Note: We create a special dataset since we need to change the reference\n#         level of the factor variables before switching to effect coding to \n#         keep the direction of the effects as intended\nd_soft_analysis &lt;- d_soft\nd_soft_analysis$fluency &lt;- relevel(d_soft_analysis$fluency, ref = 2)\nd_soft_analysis$quality &lt;- relevel(d_soft_analysis$quality, ref = 2)\n# switch to effect coding\ncontrasts(d_soft_analysis$fluency) &lt;- contr.sum # High = 1, Low = -1\ncontrasts(d_soft_analysis$quality) &lt;- contr.sum\n\n# FIGURE FOR DURATION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(duration ~ fluency * quality, d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\nduration_1 &lt;- ggplot(d_soft, aes(x=quality, y=duration, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Duration (in seconds)\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  coord_cartesian(ylim=c(0, 220)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = 175,\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = c(210),\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR COMPLETION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(completion ~ fluency * quality, d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\ncompletion_1 &lt;- ggplot(d_soft, aes(x=quality, y=completion, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Completion percentage\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, 1)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .8,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .85,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR POSITIVE REACTIONS\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(glm(positive ~ fluency * quality,\n                           family = binomial(link = \"logit\"),\n                           data = d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\npositive_1 &lt;- ggplot(d_soft, aes(x=quality, y=positive, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Positive reactions\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, .25)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .125,\n    tip_length = .002,\n    textsize = 6,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .16,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n# Main figures: healthcare startup\n#\n# data prep\n#\n# for convenience and to not interfere with later code, we work on a copy of\n# the data\nd_health &lt;- d_hc\n# convert fluency and quality to factor vars\nd_health$fluency &lt;- factor(d_health$fluency, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\nd_health$quality &lt;- factor(d_health$quality, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\n\n# create dataset for post-hoc contrasts\n# --Note: We create a special dataset since we need to change the reference\n#         level of the factor variables before switching to effect coding to \n#         keep the direction of the effects as intended\nd_health_analysis &lt;- d_health\nd_health_analysis$fluency &lt;- relevel(d_health_analysis$fluency, ref = 2)\nd_health_analysis$quality &lt;- relevel(d_health_analysis$quality, ref = 2)\n# switch to effect coding\ncontrasts(d_health_analysis$fluency) &lt;- contr.sum # High = 1, Low = -1\ncontrasts(d_health_analysis$quality) &lt;- contr.sum\n\n# FIGURE FOR DURATION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(duration ~ fluency * quality, d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\nduration_2 &lt;- ggplot(d_health, aes(x=quality, y=duration, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Duration (in seconds)\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  coord_cartesian(ylim=c(0, 220)) +\n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = 130,\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = c(155),\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR COMPLETION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(completion ~ fluency * quality, d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\ncompletion_2 &lt;- ggplot(d_health, aes(x=quality, y=completion, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Completion percentage\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, 1)) +\n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .8,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .875,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR POSITIVE REACTIONS\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(glm(positive ~ fluency * quality,\n                           family = binomial(link = \"logit\"),\n                           data = d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\npositive_2 &lt;- ggplot(d_health, aes(x=quality, y=positive, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Positive reactions\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, .25)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .1425,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .225,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n\n# Final (combined) figures\n#\n# duration\nduration_1 + duration_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Opening duration of the pitch decks\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n# completion\ncompletion_1 + completion_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Percentage of pitch deck slides being viewed\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n  # tag_levels = 'A'\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n# positive reactions\npositive_1 + positive_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Share of positive investor reactions\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n  # tag_levels = 'A'\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Opening duration. *** p &lt; .001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Completion percentage. ** p &lt; .01; *** p &lt; .001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Positive reactions. ** p &lt; .01; *** p &lt; .001.\n\n\n\n\n\n\n\nFigure 2: Results of the field experiment"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#descriptives-3",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#descriptives-3",
    "title": "Judging a Pitch by its Cover",
    "section": "7.1 Descriptives",
    "text": "7.1 Descriptives\nAfter having applied the exclusion restrictions, there were 944 replies to the 21,152 emails we sent (21.25%) for the software startup. Of these 944 replies, 346 replies were positive (36.65% of replies).\nFor the healthcare startup, there were 757 replies to the 13,335 emails we sent (31.45%). Of these 757 replies, 240 replies were positive (31.70% of replies).\nTable 14 shows a breakdown of the reply rate and the positive reactions per fluency and quality condition.\n\n\nCode\n# combine data\npos_given_reply_data &lt;- bind_rows(list(\n  Software = d_sw |&gt; select(Fluency = fluency, Quality = quality, reply, positive_given_reply, investments, type, gender, location) |&gt; mutate(n_mails = n_mails_sw),\n  Healthcare = d_hc |&gt; select(Fluency = fluency, Quality = quality, reply, positive_given_reply, investments, type, gender, location) |&gt; mutate(n_mails = n_mails_hc)),\n  .id = \"Startup\")\n# convert Startup to factor, change order of levels\npos_given_reply_data$Startup &lt;- factor(pos_given_reply_data$Startup, levels = c(\"Software\", \"Healthcare\"))\n\n# create table\npos_given_reply_data |&gt; \n  group_by(Startup, Fluency, Quality) |&gt; \n  summarize(\n    `Clicks` = scales::comma(n()),\n    `Replies ` = scales::comma(sum(reply)),\n    `% Reply Rate ` = sprintf('%.2f', mean(reply)*100),\n    `Pos. Reaction ` = scales::comma(sum(positive_given_reply, na.rm=T)),\n    `% Pos. Reaction Rate ` = sprintf('%.2f', mean(positive_given_reply, na.rm=T)*100),\n  ) |&gt; kbl() |&gt; kable_styling() |&gt;\n  row_spec(c(4), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 14: Breakdown of positive email reactions per startup, fluency, and quality condition\n\n\n\n\n\n\n\nStartup\nFluency\nQuality\nClicks\nReplies\n% Reply Rate\nPos. Reaction\n% Pos. Reaction Rate\n\n\n\n\nSoftware\nhigh\nhigh\n1,091\n245\n22.46\n133\n54.29\n\n\nSoftware\nhigh\nlow\n1,080\n238\n22.04\n91\n38.24\n\n\nSoftware\nlow\nhigh\n1,126\n233\n20.69\n94\n40.34\n\n\nSoftware\nlow\nlow\n1,146\n228\n19.90\n28\n12.28\n\n\nHealthcare\nhigh\nhigh\n589\n209\n35.48\n101\n48.33\n\n\nHealthcare\nhigh\nlow\n605\n196\n32.40\n57\n29.08\n\n\nHealthcare\nlow\nhigh\n593\n180\n30.35\n59\n32.78\n\n\nHealthcare\nlow\nlow\n620\n172\n27.74\n23\n13.37"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#results",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#results",
    "title": "Judging a Pitch by its Cover",
    "section": "7.2 Results",
    "text": "7.2 Results\nWe re-estimated the binary logit models (with control variables) reported in Table 7 and Table 12, now predicting whether there was a positive reaction to the emails conditional on investors having replied. Thus, for this analysis, the sample is a subset of the full sample (i.e., only investors who replied to the emails, N = 944 software startup, N = 757 healthcare startup). Table 15 shows the results of these models for the software and healthcare startup.\n\n\nCode\nd &lt;- pos_given_reply_data\n\n# 2 tables witch 2 colmuns each version\n\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$Fluency)\nd$quality &lt;- as.factor(d$Quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n\n# run regressions\nglm_tbl(glm(positive_given_reply ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d |&gt; filter(Startup == \"Software\")), coef_digits = 3) -&gt; temp_positive_given_reply_sw\nglm_tbl(glm(positive_given_reply ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d |&gt; filter(Startup == \"Healthcare\")), coef_digits = 3) -&gt; temp_positive_given_reply_hc\n\n# put results together\ntemp &lt;- bind_cols(temp_positive_given_reply_sw,\n                  temp_positive_given_reply_hc[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\"))\n\n# change order of rows: put interaction after main effects\ntemp |&gt; arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\")) -&gt; temp\n# change column names\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\n\n# create final table\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Software Startup\" = 4, \"Healthcare Startup\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 15: Binary logit models of positive reactions conditional on investors having replied\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Startup\n\n\nHealthcare Startup\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-1.664\n1.145\n-1.45\n.146\n0.328\n1.465\n0.22\n.823\n\n\nFluency\n0.519\n0.077\n6.75\n&lt; .001\n0.407\n0.087\n4.66\n&lt; .001\n\n\nQuality\n0.558\n0.077\n7.26\n&lt; .001\n0.504\n0.087\n5.77\n&lt; .001\n\n\nFluency × Quality\n-0.236\n0.077\n-3.08\n.002\n-0.079\n0.087\n-0.91\n.364\n\n\nInvestment experience\n0.005\n0.016\n0.30\n.766\n-0.038\n0.064\n-0.59\n.555\n\n\nInvestor type [Venture Capital]\n-0.111\n0.187\n-0.59\n.553\n-0.140\n0.198\n-0.71\n.478\n\n\nGender [Male]\n0.063\n0.248\n0.26\n.798\n-0.628\n0.253\n-2.49\n.013\n\n\nCountry [Brazil]\n1.098\n1.231\n0.89\n.372\n-0.263\n1.521\n-0.17\n.863\n\n\nCountry [Canada]\n1.375\n1.830\n0.75\n.452\n-0.766\n1.563\n-0.49\n.624\n\n\nCountry [China]\n0.580\n1.292\n0.45\n.654\n-0.162\n1.491\n-0.11\n.913\n\n\nCountry [France]\n1.295\n1.220\n1.06\n.288\n-0.269\n1.505\n-0.18\n.858\n\n\nCountry [Germany]\n0.644\n1.214\n0.53\n.596\n-0.733\n1.567\n-0.47\n.640\n\n\nCountry [India]\n1.055\n1.247\n0.85\n.397\n0.053\n1.678\n0.03\n.975\n\n\nCountry [Israel]\n1.636\n1.280\n1.28\n.201\n-0.327\n1.538\n-0.21\n.832\n\n\nCountry [Singapore]\n0.818\n1.166\n0.70\n.483\n-0.582\n1.464\n-0.40\n.691\n\n\nCountry [United Kingdom]\n0.956\n1.133\n0.84\n.399\n-0.565\n1.443\n-0.39\n.695\n\n\nTjur's R2\n.102\n\n\n\n.084"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#footnotes",
    "href": "replication_reports/backup/Replication_Report_2_Field_Experiment.html#footnotes",
    "title": "Judging a Pitch by its Cover",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTwo independent raters blind to our hypotheses rated all email replies as to whether the content demonstrates investor interest. For the software startup, the interrater agreement was 97.16% (Cohen’s κ = 0.939). For the healthcare startup, the interrater agreement was 96.45% (Cohen’s κ = 0.919). All cases for which the raters did not agree were resolved by the authors of this paper.↩︎"
  },
  {
    "objectID": "replication_reports/Replication_Report_5_Online_Experiments.html",
    "href": "replication_reports/Replication_Report_5_Online_Experiments.html",
    "title": "Online Experiments",
    "section": "",
    "text": "For both our fictitious startups (Software: PerkSouq; Healthcare: Brachytix), we ran online experiments to determine how the (stated) likelihood to invest depends on the substantive quality and visual fluency of a pitch deck.\nSpecifically, we first ran a 2x2 between-subjects experiment for our software startup. To make sure that the instruction we used when measuring investment likelihood (“Based on the information at hand, what is the probability […]”) did not bias the results, we ran a replication of this first experiment with a modified instruction (“Based on the pitch deck, what is the probability […]”). To ensure generalizability across domains, we then ran the same experiment for our healthcare startup.\nWe ran all online experiments on Qualtrics, hosted the pitch decks on DocSend, and recruited the participants via Prolific. For details, see the corresponding AsPredicted pre-registrations listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registrations\n\n\n\n\n\nStartup\nPre-Reg Date\nAsPredicted #\nData Collection Start\n\n\n\n\nSoftware startup\n28-11-2022\n114380\n30-11-2022\n\n\nSoftware startup (Replication)\n16-12-2022\n116872\n17-12-2022\n\n\nHealthcare startup\n20-12-2022\n117215\n21-12-2022\n\n\n\n\n\n\nIn what follows, we will give an overview of the results and robustness checks, followed by figures that summarize the results of the experiments. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\noptions(knitr.kable.NA = '')\n\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(ggsignif)\nlibrary(ggtext)\nlibrary(patchwork)\nlibrary(kableExtra)\noptions(knitr.kable.NA = '',\n        kable_styling_bootstrap_options = c(\"striped\", \"condensed\", \"responsive\"))\n\n# further packages that are loaded on demand are:\n# - supernova\n# - car\n# - rstatix\n# - forcats\n# - weights\n# - broom\n# - scales\n# - readr\n# - tidyr\n# - hrbrthemes\n# - emmeans\n# - grid\n\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract t-test results and Cohen's d and put the results together as a table\nttest_tbl &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", weights::rd(tres$p.value, 3))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return df\n  df = data.frame(DV = NA, condition=rep(NA, 2), N = NA, Mean = NA, SD = NA, test_statistic = NA, p = NA, d = NA)\n  # fill values\n  df$DV[1] &lt;- stringr::str_to_sentence(dres$`.y.`)\n  df$condition &lt;- c(dres$group1, dres$group2)\n  df$N &lt;- c(dres$n1, dres$n2)\n  df$Mean &lt;- weights::rd(aggregate(formula, data = data, FUN = mean)[,2], 2)\n  df$SD &lt;- weights::rd(aggregate(formula, data = data, FUN = sd)[,2], 3)\n  df$test_statistic[1] &lt;- paste0(\"t(\",\n                                 ifelse(var.equal == TRUE, tres$parameter,\n                                        weights::rd(tres$parameter, 1)),\n                                 \") = \",\n                                 sprintf('%.2f', tres$statistic))\n  df$p[1] &lt;- pval\n  df$d[1] &lt;- weights::rd(dres$effsize, 2)\n  return(df)\n}\n#\n# extract ANOVA results including eta squared and put the results together as a table\n# -Note: needs the supernova library installed\nanova_tbl &lt;- function(formula, data, type = 3, ...){\n  # perform ANOVA\n  d &lt;- supernova::supernova(lm(as.formula(deparse(formula)), data = data), type = type)$tbl\n  # check whether ANOVA is univariate or factorial\n  univariate &lt;- all(d$term %in% c(\"Model\", \"Error\", \"Total\"))\n  # get rows of interest\n  if(univariate) {\n    effect_rows &lt;- d$term %notin% c(\"Error\", \"Total\")\n  } else {\n    effect_rows &lt;- d$term %notin% c(\"Model\", \"Error\", \"Total\")\n  }\n  # extract key parameters\n  effect &lt;- d$term[effect_rows]\n  MSE &lt;- round(d$MS[effect_rows], 2)\n  df &lt;- d$df[effect_rows]\n  df_res &lt;- d$df[d$term == \"Error\"]\n  statistic &lt;- round(d$F[effect_rows], 2)\n  pval &lt;- ifelse(d$p[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$p[effect_rows], 3))\n  eta &lt;- ifelse(d$PRE[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$PRE[effect_rows], 3))\n  # construct return df\n  return(data.frame(effect, MSE, df, df_res, statistic, pval, eta))\n}\n# extract GLM results and put the results together as a table\nglm_tbl &lt;- function(model, coef_digits = 2, coef_bold = TRUE, p_threshold = 0.05, ...){\n  # extract model parameters\n  if(\"tobit\" %in% class(model)){ # tobit model -&gt; broom::tidy does not work\n    res &lt;- parameters::model_parameters(model)\n    res &lt;- res[c(\"Parameter\", \"Coefficient\", \"SE\", \"z\", \"p\")]\n    names(res) &lt;- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n    # res[] &lt;- lapply(res, function(x) { attributes(x) &lt;- NULL; x })\n  } else {\n    res &lt;- broom::tidy(model)\n  }\n  pvals &lt;- res$p.value\n  res$estimate &lt;- sprintf(paste0(\"%.\", coef_digits, \"f\"), res$estimate)\n  res$std.error &lt;- sprintf(\"%.3f\", res$std.error)\n  res$statistic &lt;- sprintf(\"%.2f\", res$statistic)\n  # format p value\n  res$p.value &lt;- ifelse(res$p.value &lt; .001, \" &lt; .001\", weights::rd(res$p.value, 3))\n  # make estimates bold if below critical p value\n  if(coef_bold){\n    res$estimate[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$estimate[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$std.error[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$std.error[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$statistic[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$statistic[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$p.value[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$p.value[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n  }\n  # bind R2 and Adj. R2 to model parameters\n  r2 &lt;- performance::r2(model) # extract R2\n  end &lt;- nrow(res) + seq_len(length(r2))\n  res[end,\"term\"] &lt;- names(r2)\n  res[end,\"estimate\"] &lt;- weights::rd(unlist(r2), digits = 3)\n  # return result\n  return(res)\n}"
  },
  {
    "objectID": "replication_reports/Replication_Report_5_Online_Experiments.html#main-analyses",
    "href": "replication_reports/Replication_Report_5_Online_Experiments.html#main-analyses",
    "title": "Online Experiments",
    "section": "4.1 Main analyses",
    "text": "4.1 Main analyses\nTable 5 shows for each experiment the results of a factorial ANOVA that models stated investment likelihood as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\n\n\nCode\n# combine data\nd &lt;- bind_rows(list(\n  Software = d_sw,\n  `Software Replication` = d_sw_repl,\n  Healthcare = d_hc),\n  .id = \"Startup\")\n\n# convert Startup to factor, change order of levels\nd$Startup &lt;- factor(d$Startup,\n                    levels = c(\"Software\", \"Software Replication\", \"Healthcare\"),\n                    labels = c(\"Software\", \"Software (replication)\", \"Healthcare\"))\n\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n\n# combine results\ntemp &lt;- bind_rows(\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software\")),\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software (replication)\")),\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Healthcare\"))\n  )\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 3)) |&gt;\n  kbl(col.names = c(\"\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling() |&gt; \n  pack_rows(index = c(\"Software Startup\" = 3,\n                      \"Software Startup (Replication)\" = 3,\n                      \"Healthcare Startup\" = 3),\n            label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0,3,6,9), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 5: ANOVA results for stated investment likelihood\n\n\n\n\n\n\n\n\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nSoftware Startup\n\n\nFluency\n559.81\n1\n193\n0.77\n.380\n.004\n\n\nQuality\n3055.37\n1\n193\n4.22\n.041\n.021\n\n\nFluency × Quality\n232.63\n1\n193\n0.32\n.571\n.002\n\n\nSoftware Startup (Replication)\n\n\nFluency\n202.33\n1\n196\n0.27\n.604\n.001\n\n\nQuality\n8433.54\n1\n196\n11.28\n&lt; .001\n.054\n\n\nFluency × Quality\n30.14\n1\n196\n0.04\n.841\n&lt; .001\n\n\nHealthcare Startup\n\n\nFluency\n2451.79\n1\n189\n3.35\n.069\n.017\n\n\nQuality\n3813.02\n1\n189\n5.21\n.024\n.027\n\n\nFluency × Quality\n1104.61\n1\n189\n1.51\n.221\n.008"
  },
  {
    "objectID": "replication_reports/Replication_Report_5_Online_Experiments.html#sec-mc",
    "href": "replication_reports/Replication_Report_5_Online_Experiments.html#sec-mc",
    "title": "Online Experiments",
    "section": "4.2 Manipulation checks",
    "text": "4.2 Manipulation checks\nIn this section, we report the results of the manipulation checks for visual fluency and substantive quality (software startup replication and healthcare startup experiments). We conducted t-tests for each manipulation check, with either perceived fluency or perceived quality as the dependent variable and the visual fluency condition or the substantive quality condition as independent variables. The results are shown in Table 6 (a) for perceived fluency and Table 6 (b) for perceived quality. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\n# perceived fluency\nbind_rows(\n  ttest_tbl(fluency_perception ~ fluency, d |&gt; filter(Startup == \"Software (replication)\")),\n  ttest_tbl(fluency_perception ~ fluency, d |&gt; filter(Startup == \"Healthcare\"))\n) |&gt;\n  mutate(DV = rep(c(\"Perceived fluency\", \"\"), 2)) |&gt;\n  kbl(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"), escape = FALSE) |&gt;\n  kable_styling() |&gt;\n  pack_rows(index = c(\"Software Startup (Replication)\" = 2, \"Healthcare Startup\" = 2), label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0, 2), extra_css = \"border-bottom: 1px solid\")\n# perceived quality\nbind_rows(\n  ttest_tbl(quality_perception ~ quality, d |&gt; filter(Startup == \"Software (replication)\")),\n  ttest_tbl(quality_perception ~ quality, d |&gt; filter(Startup == \"Healthcare\"))\n) |&gt;\n  mutate(DV = rep(c(\"Perceived quality\", \"\"), 2)) |&gt;\n  kbl(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"), escape = FALSE) |&gt;\n  kable_styling() |&gt;\n  pack_rows(index = c(\"Software Startup (Replication)\" = 2, \"Healthcare Startup\" = 2), label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0, 2), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 6: Manipulation checks (software startup replication and healthcare startup)\n\n\n\n\n\n\n\n(a) Perceived fluency\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen's d\n\n\n\n\nSoftware Startup (Replication)\n\n\nPerceived fluency\nhigh\n101\n63.80\n26.711\nt(198) = 2.03\n.043\n.29\n\n\n\nlow\n99\n55.83\n28.749\n\n\n\n\n\nHealthcare Startup\n\n\nPerceived fluency\nhigh\n103\n58.79\n28.051\nt(191) = 1.99\n.048\n.29\n\n\n\nlow\n90\n50.33\n30.871\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Perceived quality\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen's d\n\n\n\n\nSoftware Startup (Replication)\n\n\nPerceived quality\nhigh\n107\n4.73\n1.225\nt(198) = 2.34\n.020\n.33\n\n\n\nlow\n93\n4.33\n1.155\n\n\n\n\n\nHealthcare Startup\n\n\nPerceived quality\nhigh\n97\n5.23\n.952\nt(191) = 3.48\n&lt; .001\n.50\n\n\n\nlow\n96\n4.70\n1.153"
  },
  {
    "objectID": "replication_reports/Replication_Report_5_Online_Experiments.html#sec-robustness",
    "href": "replication_reports/Replication_Report_5_Online_Experiments.html#sec-robustness",
    "title": "Online Experiments",
    "section": "4.3 Robustness checks",
    "text": "4.3 Robustness checks\nAs a robustness check, we estimated linear regression models in which we predicted the stated investment likelihood by visual fluency, substantive quality, age, gender, investment experience and the aesthetic sensitivity (CVPA composite score). In this regression, we again used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1. Table 7 shows the results of this regression.\n\n\nCode\n# -- Note: We comment out Levene's tests of homogeneous group variances here for\n#          readability. However, we adapt the function call of the regression\n#          based on the result of Levene's tests.\n\n\n# Levene's test of homogeneous group variances -&gt; if not, use robust::lmRob() below\n# \n# # software startup\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Software\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Software\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software\")) # not significant\n# # software startup replication\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# # healthcare startup\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n\n# recode Likert text scores into numeric values for the CVPA score\nd |&gt; mutate_at(vars(cvpa_v_1:cvpa_r_3), dplyr::recode,\n                   'Strongly disagree' = 1,\n                   'Disagree' = 2,\n                   'Neither agree nor disagree' = 3,\n                   'Agree' = 4,\n                   'Strongly agree' = 5) -&gt; d\n# create CVPA composite score\nd &lt;- d |&gt; rowwise() |&gt; mutate(CVPA = mean(cvpa_v_1:cvpa_r_3))\n\n# group \"Non-binary\" and \"Prefer not to say\" gender categories into one\nd$gender &lt;- forcats::fct_collapse(d$gender, Female = \"Female\", Male = \"Male\", other_level = \"Other\")\n\n# run regression\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Software\"))) -&gt; lm_rob_sw\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Software (replication)\"))) -&gt; lm_rob_sw_repl\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Healthcare\"))) -&gt; lm_rob_hc\n\n# add empty row for `genderOther` in the first and third regression\nlm_rob_sw |&gt; add_row(.before = 6)  -&gt; lm_rob_sw\nlm_rob_hc |&gt; add_row(.before = 6)  -&gt; lm_rob_hc\n\n\ntemp &lt;- bind_cols(lm_rob_sw, lm_rob_sw_repl[,-1], lm_rob_hc[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Age\",\n               \"Gender [Male]\", \"Gender [Other]\",\n               \"Investment experience\", \"Aesthetic sensitivity\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 3))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Software Startup\" = 4, \"Software Startup (Replication)\" = 4, \"Healthcare Startup\" = 4)) |&gt;\n  row_spec(c(0,9), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 7: Robustness checks: Linear regressions for stated investment likelihood with individual-level control variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Startup\n\n\nSoftware Startup (Replication)\n\n\nHealthcare Startup\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n34.99\n10.432\n3.35\n&lt; .001\n4.86\n10.538\n0.46\n.645\n37.19\n10.670\n3.49\n&lt; .001\n\n\nFluency\n-2.12\n1.781\n-1.19\n.236\n1.54\n1.831\n0.84\n.401\n3.59\n1.875\n1.91\n.057\n\n\nQuality\n4.48\n1.790\n2.50\n.013\n6.18\n1.831\n3.38\n&lt; .001\n4.27\n1.891\n2.26\n.025\n\n\nFluency × Quality\n-0.01\n1.804\n-0.01\n.996\n-0.34\n1.847\n-0.18\n.855\n1.51\n1.883\n0.80\n.423\n\n\nAge\n-0.47\n0.165\n-2.82\n.005\n-0.01\n0.165\n-0.03\n.975\n-0.30\n0.182\n-1.65\n.102\n\n\nGender [Male]\n-3.90\n3.703\n-1.05\n.294\n0.32\n4.029\n0.08\n.938\n-2.06\n4.045\n-0.51\n.611\n\n\nGender [Other]\n\n\n\n\n-2.19\n13.260\n-0.17\n.869\n\n\n\n\n\n\nInvestment experience\n-0.12\n0.299\n-0.39\n.695\n-0.06\n0.294\n-0.21\n.830\n-0.14\n0.399\n-0.34\n.732\n\n\nAesthetic sensitivity\n8.80\n1.981\n4.44\n&lt; .001\n11.79\n2.028\n5.82\n&lt; .001\n8.58\n2.047\n4.19\n&lt; .001\n\n\nR2\n.188\n\n\n\n.202\n\n\n\n.164\n\n\n\n\n\nR2adj.\n.157\n\n\n\n.168\n\n\n\n.133"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_MCs.html",
    "href": "replication_reports/Replication_Report_3_MCs.html",
    "title": "Manipulation Checks",
    "section": "",
    "text": "For both our fictitious startups (Software: PerkSouq; Healthcare: Brachytix), we ran manipulation checks of the proposed pitch decks. Specifically, we ran four online experiments in which either design (i.e., visual fluency) or substantive quality was manipulated and their impact on several measures was tested.\nWe ran all online experiments on Qualtrics, hosted the pitch decks on DocSend, and recruited the participants via Prolific. For details, see the corresponding AsPredicted pre-registrations listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registrations\n\n\n\n\n\nStartup\nManipulation\nPre-Reg Date\nAsPredicted #\nTarget N\nData Collection Start\n\n\n\n\nSoftware\nDesign\n03-11-2022\n111740\n160\n04-11-2022\n\n\n\nQuality\n11-11-2022\n112721\n160\n12-11-2022\n\n\nHealthcare\nDesign\n18-12-2022\n116999\n160\n19-12-2022\n\n\n\nQuality\n18-12-2022\n117000\n160\n19-12-2022\n\n\n\n\n\n\nIn what follows, we will give an overview of the results, separately for each startup. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\noptions(knitr.kable.NA = '')\n\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\n# further packages that are loaded on demand are:\n# - rstatix\n# - weights\n# - stringr\n# - readr\n# - car\n# - tidyr\n# - hrbrthemes\n# - grid\n\n# set option to disable showing the column types when loading data with `readr`\noptions(\"readr.show_col_types\" = FALSE)\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract t-test results and Cohen's d and put the results together as a string\nttest_str &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", paste0(\" = \",weights::rd(tres$p.value, 3)))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return string\n  return(paste0(stringr::str_to_sentence(dv),\n                \"\\nt(\",\n                ifelse(var.equal == TRUE, tres$parameter, weights::rd(tres$parameter, 1)),\n                \") = \", sprintf('%.2f', tres$statistic),\n                \", p\", pval,\n                \"; d = \", weights::rd(dres$effsize, 2)))\n}\n#\n# extract t-test results and Cohen's d and put the results together as a table\nttest_tbl &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", weights::rd(tres$p.value, 3))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return df\n  df = data.frame(DV = NA, condition=rep(NA, 2), N = NA, Mean = NA, SD = NA, test_statistic = NA, p = NA, d = NA)\n  # fill values\n  df$DV[1] &lt;- stringr::str_to_sentence(dres$`.y.`)\n  df$condition &lt;- c(dres$group1, dres$group2)\n  df$N &lt;- c(dres$n1, dres$n2)\n  df$Mean &lt;- weights::rd(aggregate(formula, data = data, FUN = mean)[,2], 2)\n  df$SD &lt;- weights::rd(aggregate(formula, data = data, FUN = sd)[,2], 3)\n  df$test_statistic[1] &lt;- paste0(\"t(\",\n                                 ifelse(var.equal == TRUE, tres$parameter,\n                                        weights::rd(tres$parameter, 1)),\n                                 \") = \",\n                                 sprintf('%.2f', tres$statistic))\n  df$p[1] &lt;- pval\n  df$d[1] &lt;- weights::rd(dres$effsize, 2)\n  return(df)\n}"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_MCs.html#sec-design-sw",
    "href": "replication_reports/Replication_Report_3_MCs.html#sec-design-sw",
    "title": "Manipulation Checks",
    "section": "4.1 Design manipulation (visual fluency)",
    "text": "4.1 Design manipulation (visual fluency)\nIn this between-subjects experiment, we presented participants one of two pitch decks that varied only in their visual fluency. The content (i.e., substantive quality) was held constant across conditions. Specifically, the pitch deck’s design was systematically varied by a design agency with the instruction that four dimensions of processing fluency (contrast, clarity, symmetry, simplicity) should be each either relatively high or relatively low. The goal was to create a high fluency and a low fluency pitch deck.\nIn the online experiment, participants were randomly assigned to one of the two visual fluency conditions, had to open and carefully study the pitch deck, and answer questions on their perceived contrast, clarity, simplicity, symmetry, processing fluency, and venture quality.\n\n4.1.1 Results\nTable 3 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two visual fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- design_sw\n\n# convert fluency_condition to factor\nd$fluency_condition &lt;- as.factor(d$fluency_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Contrast\nres_contr &lt;- ttest_tbl(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 2. Clarity\nres_clar &lt;- ttest_tbl(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 3. Symmetry\nres_sym &lt;- ttest_tbl(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 4. Simplicity\nres_simpl &lt;- ttest_tbl(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 5. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 6. Venture Quality\nres_qual &lt;- ttest_tbl(quality ~ fluency_condition, data = d)\n\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\n\n# put all results together\nbind_rows(res_contr, res_clar, res_sym, res_simpl, res_pf, res_qual) |&gt;\n  kable(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 3: Manipulation checks, visual fluency (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nContrast\nhigh\n49\n4.80\n1.224\nt(90.3) = 5.30\n&lt; .001\n1.06\n\n\n\nlow\n51\n3.22\n1.724\n\n\n\n\n\nClarity\nhigh\n49\n5.45\n1.138\nt(89.2) = 5.07\n&lt; .001\n1.01\n\n\n\nlow\n51\n4.02\n1.643\n\n\n\n\n\nSymmetry\nhigh\n49\n5.76\n.969\nt(80.0) = 5.59\n&lt; .001\n1.11\n\n\n\nlow\n51\n4.22\n1.701\n\n\n\n\n\nSimplicity\nhigh\n49\n4.27\n1.366\nt(98) = -0.03\n.974\n-.01\n\n\n\nlow\n51\n4.27\n1.401\n\n\n\n\n\nProcessing fluency\nhigh\n49\n66.27\n26.701\nt(98) = 3.65\n&lt; .001\n.73\n\n\n\nlow\n51\n46.69\n26.951\n\n\n\n\n\nVenture quality\nhigh\n49\n4.94\n1.069\nt(98) = 1.50\n.136\n.30\n\n\n\nlow\n51\n4.61\n1.133\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Plots\nFigure 1 summarizes the results of this manipulation check visually.\n\n\nCode\n# change factor labels for fluency\nd$fluency_condition &lt;- factor(d$fluency_condition, levels = c(\"high\", \"low\"), labels = c(\"High\", \"Low\"))\n\n# create long dataset for plot\nd_long &lt;- d |&gt; select(contrast:symmetry, fluency, quality, fluency_condition) |&gt; \n  tidyr::pivot_longer(contrast:quality, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_contrast &lt;- ttest_str(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_clarity &lt;- ttest_str(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_symmetry &lt;- ttest_str(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_simplicity &lt;- ttest_str(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ fluency_condition, data = d)\n\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"contrast\", \"clarity\", \"symmetry\", \"simplicity\", \"fluency\", \"quality\"),\n                          labels = c(str_contrast, str_clarity, str_symmetry, str_simplicity, str_fluency, str_quality))\n\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n                 )) -&gt; d_long\n                                  \n# plot result\nggplot(d_long, aes(x=fluency_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +  \n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Visual fluency (software startup)\",\n       subtitle = \"Effect of the low vs. high fluency pitch deck versions on various outcomes\",\n       x = \"Pitch deck visual fluency\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 1: Summary of the fluency manipulation checks for the software startup"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_MCs.html#sec-quality-sw",
    "href": "replication_reports/Replication_Report_3_MCs.html#sec-quality-sw",
    "title": "Manipulation Checks",
    "section": "4.2 Quality manipulation",
    "text": "4.2 Quality manipulation\nIn this between-subjects experiment, we presented participants one of two pitch decks that varied only in their substantive quality. The design (i.e., visual fluency) was held constant across conditions. Participants were randomly assigned to one of the two substantive quality conditions, had to open and carefully study the pitch deck, and rate the startup’s intellectual property, human capital, commercialization opportunity, legitimacy, and venture quality. They further had to rate the perceived processing fluency of the pitch deck.\n\n4.2.1 Results\nTable 3 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two quality conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- quality_sw\n\n# convert quality_condition to factor\nd$quality_condition &lt;- as.factor(d$quality_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Intellectual Property\nres_intell &lt;- ttest_tbl(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 2. Human Capital\nres_hum &lt;- ttest_tbl(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 3. Commercialization opportunity\nres_commerc &lt;- ttest_tbl(commerc ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 4. Organizational legitimacy\nres_legitim &lt;- ttest_tbl(legitim ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 5. Overall Venture Quality / Potential\nres_qual &lt;- ttest_tbl(quality ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 6. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ quality_condition, data = d)\n\nres_intell[1,1] &lt;- stringr::str_replace(res_intell[1,1], \"Intell_prop\", \"Intellectual property\")\nres_hum[1,1] &lt;- stringr::str_replace(res_hum[1,1], \"Hum_cap\", \"Human capital\")\nres_commerc[1,1] &lt;- stringr::str_replace(res_commerc[1,1], \"Commerc\", \"Commercialization opportunity\")\nres_legitim[1,1] &lt;- stringr::str_replace(res_legitim[1,1], \"Legitim\", \"Organizational legitimacy\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\n\n# put all results together\nbind_rows(res_intell, res_hum, res_commerc, res_legitim, res_qual, res_pf) |&gt;\n  kable(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 4: Manipulation checks, substantive quality (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nIntellectual property\nhigh\n56\n4.61\n1.303\nt(111) = 2.70\n.008\n.51\n\n\n\nlow\n57\n3.96\n1.224\n\n\n\n\n\nHuman capital\nhigh\n56\n5.54\n.990\nt(111) = 3.61\n&lt; .001\n.68\n\n\n\nlow\n57\n4.88\n.946\n\n\n\n\n\nCommercialization opportunity\nhigh\n56\n5.11\n1.155\nt(111) = 1.21\n.227\n.23\n\n\n\nlow\n57\n4.86\n1.008\n\n\n\n\n\nOrganizational legitimacy\nhigh\n56\n5.02\n1.104\nt(111) = 2.94\n.004\n.55\n\n\n\nlow\n57\n4.40\n1.116\n\n\n\n\n\nVenture quality\nhigh\n56\n4.80\n1.212\nt(111) = 2.82\n.006\n.53\n\n\n\nlow\n57\n4.21\n1.013\n\n\n\n\n\nProcessing fluency\nhigh\n56\n54.39\n26.222\nt(111) = 1.42\n.158\n.27\n\n\n\nlow\n57\n47.12\n28.041\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Plots\nFigure 2 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(intell_prop:legitim, quality, fluency, quality_condition) |&gt; \n  tidyr::pivot_longer(intell_prop:fluency, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_intell_prop &lt;- ttest_str(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\nstr_hum_cap &lt;- ttest_str(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\nstr_commerc &lt;- ttest_str(commerc ~ quality_condition, data = d) # alternative = \"greater\"\nstr_legitim &lt;- ttest_str(legitim ~ quality_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ quality_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ quality_condition, data = d)\n\nstr_intell_prop &lt;- stringr::str_replace(str_intell_prop, \"Intell_prop\", \"Intellectual property\")\nstr_hum_cap &lt;- stringr::str_replace(str_hum_cap, \"Hum_cap\", \"Human capital\")\nstr_commerc &lt;- stringr::str_replace(str_commerc, \"Commerc\", \"Commercialization opportunity\")\nstr_legitim &lt;- stringr::str_replace(str_legitim, \"Legitim\", \"Organizational legitimacy\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"intell_prop\", \"hum_cap\", \"commerc\", \"legitim\",\"quality\",  \"fluency\"),\n                         labels = c(str_intell_prop, str_hum_cap, str_commerc, str_legitim, str_quality, str_fluency))\n\n# create ymin and ymax for plot\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n)) -&gt; d_long\n\n\n# plot result\nggplot(d_long, aes(x=quality_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Substantive quality (software startup)\",\n       subtitle = \"Effect of the low vs. high quality pitch deck versions on various outcomes\",\n       x = \"Pitch deck substantive quality\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 2: Summary of the quality manipulation checks for the software startup"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_MCs.html#sec-design-hc",
    "href": "replication_reports/Replication_Report_3_MCs.html#sec-design-hc",
    "title": "Manipulation Checks",
    "section": "5.1 Design manipulation (visual fluency)",
    "text": "5.1 Design manipulation (visual fluency)\nAs before, we presented participants one of two pitch decks that varied only in their visual fluency. The content (i.e., substantive quality) was held constant across conditions. Participants were randomly assigned to the conditions. The dependent variables were the same as before (i.e., perceived contrast, clarity, symmetry, simplicity, processing fluency, and venture quality).\n\n5.1.1 Results\nTable 5 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two visual fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\nd &lt;- design_hc\n\n# convert fluency_condition to factor\nd$fluency_condition &lt;- as.factor(d$fluency_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Contrast\nres_contr &lt;- ttest_tbl(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 2. Clarity\nres_clar &lt;- ttest_tbl(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 3. Symmetry\nres_sym &lt;- ttest_tbl(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 4. Simplicity\nres_simpl &lt;- ttest_tbl(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 5. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\n\n# 6. Venture Quality\nres_qual &lt;- ttest_tbl(quality ~ fluency_condition, data = d)\n\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\n\n# put all results together\nbind_rows(res_contr, res_clar, res_sym, res_simpl, res_pf, res_qual) |&gt;\n  kable(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 5: Manipulation checks, visual fluency (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nContrast\nhigh\n53\n4.83\n1.139\nt(95.4) = 4.46\n&lt; .001\n.87\n\n\n\nlow\n52\n3.67\n1.491\n\n\n\n\n\nClarity\nhigh\n53\n5.49\n1.171\nt(92.8) = 5.12\n&lt; .001\n1.00\n\n\n\nlow\n52\n4.08\n1.619\n\n\n\n\n\nSymmetry\nhigh\n53\n5.36\n1.242\nt(103) = 4.96\n&lt; .001\n.97\n\n\n\nlow\n52\n4.02\n1.515\n\n\n\n\n\nSimplicity\nhigh\n53\n3.74\n1.546\nt(103) = 0.68\n.499\n.13\n\n\n\nlow\n52\n3.52\n1.721\n\n\n\n\n\nProcessing fluency\nhigh\n53\n53.26\n30.495\nt(99.8) = 2.14\n.034\n.42\n\n\n\nlow\n52\n41.62\n24.949\n\n\n\n\n\nVenture quality\nhigh\n53\n5.28\n1.007\nt(103) = 0.90\n.371\n.18\n\n\n\nlow\n52\n5.12\n.900\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Plots\nFigure 3 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(contrast:symmetry, fluency, quality, fluency_condition) |&gt; \n  tidyr::pivot_longer(contrast:quality, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_contrast &lt;- ttest_str(contrast ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_clarity &lt;- ttest_str(clarity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_symmetry &lt;- ttest_str(symmetry ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_simplicity &lt;- ttest_str(simplicity ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ fluency_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ fluency_condition, data = d)\n\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"contrast\", \"clarity\", \"symmetry\", \"simplicity\", \"fluency\", \"quality\"),\n                          labels = c(str_contrast, str_clarity, str_symmetry, str_simplicity, str_fluency, str_quality))\n\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n                 )) -&gt; d_long\n\n# plot result\nggplot(d_long, aes(x=fluency_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Visual fluency (healthcare startup)\",\n       subtitle = \"Effect of the low vs. high fluency pitch deck versions on various outcomes\",\n       x = \"Pitch deck visual fluency\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 3: Summary of the fluency manipulation checks for the healthcare startup"
  },
  {
    "objectID": "replication_reports/Replication_Report_3_MCs.html#sec-quality-hc",
    "href": "replication_reports/Replication_Report_3_MCs.html#sec-quality-hc",
    "title": "Manipulation Checks",
    "section": "5.2 Quality manipulation",
    "text": "5.2 Quality manipulation\nAs before, we presented participants one of two pitch decks that varied only in their substantive quality. The design was held constant across conditions. Participants were randomly assigned to the conditions. The dependent variables are the same as before (i.e., intellectual property, human capital, commercialization opportunity, legitimacy, venture quality, and processing fluency).\n\n5.2.1 Results\nTable 5 shows the results of all t-tests that were run. Each t-test compares the group means of the respective dependent variable across the two fluency conditions. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances. \n\n\nCode\nd &lt;- quality_hc\n\n# convert quality_condition to factor\nd$quality_condition &lt;- as.factor(d$quality_condition)\n\n# -- Note: Although for most hypotheses a direction was specified, we do not\n#          specify alternative = \"greater\" in our tests. However, we include\n#          comments in the code where this would have been \"allowed\", so that\n#          an interested reader can divide the resulting p-values by 2.\n\n# 1. Intellectual Property\nres_intell &lt;- ttest_tbl(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 2. Human Capital\nres_hum &lt;- ttest_tbl(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 3. Commercialization opportunity\nres_commerc &lt;- ttest_tbl(commerc ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 4. Organizational legitimacy\nres_legitim &lt;- ttest_tbl(legitim ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 5. Overall Venture Quality / Potential\nres_qual &lt;- ttest_tbl(quality ~ quality_condition, data = d) # alternative = \"greater\"\n\n# 6. Processing Fluency\nres_pf &lt;- ttest_tbl(fluency ~ quality_condition, data = d)\n\nres_intell[1,1] &lt;- stringr::str_replace(res_intell[1,1], \"Intell_prop\", \"Intellectual property\")\nres_hum[1,1] &lt;- stringr::str_replace(res_hum[1,1], \"Hum_cap\", \"Human capital\")\nres_commerc[1,1] &lt;- stringr::str_replace(res_commerc[1,1], \"Commerc\", \"Commercialization opportunity\")\nres_legitim[1,1] &lt;- stringr::str_replace(res_legitim[1,1], \"Legitim\", \"Organizational legitimacy\")\nres_qual[1,1] &lt;- stringr::str_replace(res_qual[1,1], \"Quality\", \"Venture quality\")\nres_pf[1,1] &lt;- stringr::str_replace(res_pf[1,1], \"Fluency\", \"Processing fluency\")\n\n# put all results together\nbind_rows(res_intell, res_hum, res_commerc, res_legitim, res_qual, res_pf) |&gt;\n  kable(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"),\n        align = 'llrrrrrr')\n\n\n\n\nTable 6: Manipulation checks, substantive quality (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen’s d\n\n\n\n\nIntellectual property\nhigh\n66\n5.65\n.903\nt(63.2) = 4.41\n&lt; .001\n.90\n\n\n\nlow\n43\n4.56\n1.452\n\n\n\n\n\nHuman capital\nhigh\n66\n5.86\n.959\nt(107) = 3.63\n&lt; .001\n.71\n\n\n\nlow\n43\n5.09\n1.250\n\n\n\n\n\nCommercialization opportunity\nhigh\n66\n5.52\n1.070\nt(107) = 3.32\n.001\n.65\n\n\n\nlow\n43\n4.74\n1.347\n\n\n\n\n\nOrganizational legitimacy\nhigh\n66\n5.36\n1.047\nt(107) = 3.44\n&lt; .001\n.67\n\n\n\nlow\n43\n4.63\n1.155\n\n\n\n\n\nVenture quality\nhigh\n66\n5.39\n.839\nt(107) = 3.01\n.003\n.59\n\n\n\nlow\n43\n4.84\n1.090\n\n\n\n\n\nProcessing fluency\nhigh\n66\n46.67\n26.018\nt(107) = -1.12\n.264\n-.22\n\n\n\nlow\n43\n52.58\n28.202\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Plots\nFigure 4 summarizes the results of this manipulation check visually.\n\n\nCode\n# create long dataset for plot\nd_long &lt;- d |&gt; select(intell_prop:legitim, quality, fluency, quality_condition) |&gt; \n  tidyr::pivot_longer(intell_prop:fluency, names_to=\"measure\", values_to=\"value\")\n\n# create labels that include statistical inference\nstr_intell_prop &lt;- ttest_str(intell_prop ~ quality_condition, data = d) # alternative = \"greater\"\nstr_hum_cap &lt;- ttest_str(hum_cap ~ quality_condition, data = d) # alternative = \"greater\"\nstr_commerc &lt;- ttest_str(commerc ~ quality_condition, data = d) # alternative = \"greater\"\nstr_legitim &lt;- ttest_str(legitim ~ quality_condition, data = d) # alternative = \"greater\"\nstr_quality &lt;- ttest_str(quality ~ quality_condition, data = d) # alternative = \"greater\"\nstr_fluency &lt;- ttest_str(fluency ~ quality_condition, data = d)\n\nstr_intell_prop &lt;- stringr::str_replace(str_intell_prop, \"Intell_prop\", \"Intellectual property\")\nstr_hum_cap &lt;- stringr::str_replace(str_hum_cap, \"Hum_cap\", \"Human capital\")\nstr_commerc &lt;- stringr::str_replace(str_commerc, \"Commerc\", \"Commercialization opportunity\")\nstr_legitim &lt;- stringr::str_replace(str_legitim, \"Legitim\", \"Organizational legitimacy\")\nstr_quality &lt;- stringr::str_replace(str_quality, \"Quality\", \"Venture quality\")\nstr_fluency &lt;- stringr::str_replace(str_fluency, \"Fluency\", \"Processing fluency\")\n\nd_long$measure &lt;- factor(d_long$measure, levels = c(\"intell_prop\", \"hum_cap\", \"commerc\", \"legitim\",\"quality\",  \"fluency\"),\n                         labels = c(str_intell_prop, str_hum_cap, str_commerc, str_legitim, str_quality, str_fluency))\n\n# create ymin and ymax for plot\nd_long |&gt; mutate(ymin = case_when(measure == \"fluency\" ~ 0,\n                                  .default = 1),\n                 ymax = case_when(measure == \"fluency\" ~ 100,\n                                  .default = 7\n)) -&gt; d_long\n\n\n# plot result\nggplot(d_long, aes(x=quality_condition, y=value)) +\n  geom_point(size = 2.5, alpha = 0.25, position=position_jitter(.1, seed = 42)) +\n  stat_summary(color = \"darkred\", geom = \"errorbar\",\n               fun.min = mean, fun = mean, fun.max = mean,\n               width = .5, linewidth = 0.75) +\n  facet_wrap(vars(measure), ncol = 3, scales = \"free_y\") +\n  scale_x_discrete(limits = rev) +\n  geom_blank(aes(y = ymin)) +\n  geom_blank(aes(y = ymax)) +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(panel.grid.major.x = element_blank(),\n        plot.margin=grid::unit(c(1,0,3,0), \"mm\"),\n        axis.title.x = element_text(hjust=0.5, margin=margin(t=15), size = 12, face = \"bold\"),\n        axis.title.y = element_text(hjust=0.5),\n        plot.caption = element_text(hjust=0, size = 10)\n  ) +\n  labs(title=\"Manipulation check: Substantive quality (healthcare startup)\",\n       subtitle = \"Effect of the low vs. high quality pitch deck versions on various outcomes\",\n       x = \"Pitch deck substantive quality\",\n       y = NULL,\n       caption = paste0(\"Note: (Jittered) raw values and group means are shown (n = \", nrow(d), \").\"))\n\n\n\n\n\n\n\n\nFigure 4: Summary of the quality manipulation checks for the healthcare startup"
  },
  {
    "objectID": "replication_reports/Replication_Report_2_Benchmarking.html",
    "href": "replication_reports/Replication_Report_2_Benchmarking.html",
    "title": "Benchmarking Study",
    "section": "",
    "text": "1 Introduction\nTo ensure that our experimental pitch decks appeared realistic and ecologically valid in terms of design and fluency, we benchmarked their visual fluency against 3,510 real pitch decks submitted to a European venture capital firm specializing in seed-stage investments. Importantly, since these real pitch decks include both successful and unsuccessful funding attempts, this approach mitigates the survivorship bias inherent in publicly available pitch decks, which are typically limited to successful cases and may overrepresent higher visual fluency.\nIn what follows, we will shortly describe the results of the benchmarking study. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\n# setup\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(rlang) # enables `%||%` (default value for NULL)\n\n# further packages that are loaded on demand are:\n# - here\n# - readr\n# - hrbrthemes\n# - scales\n# - stringr\n\n# set option to disable showing the column types when loading data with `readr`\noptions(\"readr.show_col_types\" = FALSE)\n\n# -----------------------------------------------------------------------------\n# custom colors and themes\n# -----------------------------------------------------------------------------\n#\n# colors\ntheme_colors &lt;- list(\n  percentiles = \"black\",\n  fill = \"grey80\",\n  emphasis = \"#297FB8\",\n  emphasis_text = \"black\",\n  axis = \"black\",\n  grid = \"grey80\"\n)\n\n# custom theme\ncustom_theme &lt;- hrbrthemes::theme_ipsum_rc() +\n  theme(\n    panel.grid.major.y = element_line(color = theme_colors$grid),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.line = element_blank(),\n    plot.title = element_text(hjust = 0, size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0, size = 14, family = \"Roboto Condensed\", margin = margin(b = 25)),\n    plot.caption = element_text(hjust = 0.5, family = \"Roboto Condensed\"),\n    axis.title.x = element_text(size = 12, margin = margin(t = 10, b = 0), hjust = 0.5, color = theme_colors$axis),\n    axis.text.x = element_text(angle = 0, hjust = 0.5, size = 10, color = theme_colors$axis),\n    axis.text.y = element_text(size = 10, color = theme_colors$axis),\n    axis.title.y = element_text(size = 12, margin = margin(l = -20, r = 10), hjust = 0.5, color = theme_colors$axis),\n    plot.margin = margin(t = 40, r = 20, b = 40, l = 40, unit = \"pt\")\n  )\n\n\n# -----------------------------------------------------------------------------\n# custom plot function\n# -----------------------------------------------------------------------------\n#\n# function to create density plot with percentiles and benchmarks\ncreate_density_plot &lt;- function(\n\n    data,               # data frame with data for all pitch decks\n    metric,             # metric to plot\n    benchmark_values,   # values for benchmarks\n    benchmark_labels,   # labels for benchmarks\n    title = NULL,       # title for the plot\n    save_plot = FALSE   # save the plot as png\n    \n    ) {\n  \n  # calculate density and percentiles\n  metric_density &lt;- density(data[[metric]])\n  metric_percentiles &lt;- quantile(data[[metric]],\n                                 probs = c(0.01, 0.05, 0.10, 0.25, 0.5,\n                                           0.75, 0.9, 0.95, 0.99))\n  \n  # create percentile lines data\n  perc_lines &lt;- data.frame(\n    x = metric_percentiles,\n    y = approx(metric_density$x, metric_density$y, xout = metric_percentiles)$y,\n    label = c(\"p1\", \"p5\", \"p10\", \"p25\", \"p50\", \"p75\", \"p90\", \"p95\", \"p99\")\n  )\n  \n  # create benchmark data\n  benchmarks &lt;- data.frame(\n    values = benchmark_values,\n    labels = benchmark_labels\n  )\n  \n  # create plot\n  plot &lt;- ggplot(data, aes(!!sym(metric))) +\n    geom_density(fill = theme_colors$fill, alpha = 0.65, color = \"white\", linewidth = .65) +\n    geom_segment(data = perc_lines,\n                aes(x = x, xend = x, y = 0, yend = y),\n                color = theme_colors$percentiles, linetype = \"dashed\") +\n    geom_text(data = perc_lines,\n              aes(x = x, y = 0, label = label),\n              vjust = 1.5, size = 3, color = theme_colors$percentiles, fontface = \"italic\", family = \"Roboto Condensed\") +\n    geom_segment(data = benchmarks,\n                aes(x = values, xend = values, y = 0, yend = max(metric_density$y)),\n                color = theme_colors$emphasis, linewidth = 1) +\n    geom_text(data = benchmarks,\n              aes(x = values, y = max(metric_density$y), label = labels),\n              vjust = -1, size = 4, color = theme_colors$emphasis_text, family = \"Roboto Condensed\") +\n    scale_x_continuous(\n      labels = scales::number_format(accuracy = 0.01),\n      breaks = scales::pretty_breaks(n = 10)\n    ) +\n    custom_theme +\n    labs(\n      title = title %||% paste(\"Distribution of\", stringr::str_to_lower(metric), \"scores\"),\n      x = paste(stringr::str_to_title(metric), \"score\"),\n      y = \"Density\",\n      subtitle = paste(scales::comma(nrow(data)), \"pitch decks\"),\n      caption = \"Note: Dashed lines represent key percentiles. Solid lines represent the average scores of our pitch decks used in the experiments.\"\n    )\n  \n  # save plot if requested\n  if(save_plot){\n    filename &lt;- paste0(\"Benchmark_Plot_\", stringr::str_to_title(metric), \".png\")\n    ggsave(filename, plot, width = 12, height = 8, dpi = 300)\n  }\n  \n  return(plot)\n}\n\n\n\n\n2 Data\nWe provided the VC firm with a script that uses the imagefluency package (Mayer, 2024) to compute simplicity, symmetry, and contrast values for all pitch decks in their database. In the present replication report, we load the data we got back from the VC firm.\n\n\nCode\ndata_dir &lt;- 'replication_reports/data'\n\npitch_data &lt;- readr::read_csv(here::here(data_dir, 'Pitchdeck_Benchmarks.csv'))\n# no further processing needed\n\n\n\n\n3 Plots\nFigure 1 shows the distribution of visual fluency across the VC’s deck database. In particular, Figure 1 (a) shows the distribution of visual contrast, Figure 1 (b) shows the distribution of visual simplicity, and Figure 1 (c) shows the distribution of visual symmetry. In each distribution, we highlight the position of our experimental pitch decks’ scores to illustrate how they compare to the overall range. The density is based on the average of the individual slides of the pitch decks, and the scores for the low and high fluency pitch decks are the average per condition.\n\n\nCode\n# NOTE: below custom plot function is defined at the beginning of this document\n\n# contrast plot\ncreate_density_plot(\n  data = pitch_data,\n  metric = \"contrast\",\n  benchmark_values = c(0.118188, 0.189691),\n  benchmark_labels = c(\"Low Fluency Decks\", \"High Fluency Decks\")\n)\n\n# simplicity plot\ncreate_density_plot(\n  data = pitch_data,\n  metric = \"simplicity\",\n  benchmark_values = c(0.874492, 0.930208),\n  benchmark_labels = c(\"Low Fluency Decks\", \"High Fluency Decks\")\n)\n\n# symmetry plot\ncreate_density_plot(\n  data = pitch_data,\n  metric = \"symmetry\",\n  benchmark_values = c(0.157513, 0.542096),\n  benchmark_labels = c(\"Low Fluency Decks\", \"High Fluency Decks\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Contrast scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Simplicity scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Symmetry scores\n\n\n\n\n\n\n\nFigure 1: Distribution of contrast, simplicity, and symmetry scores in real pitch decks from a European VC firm and comparison with the scores of the low and high fluency pitch decks from our field experiment.\n\n\n\n\nOverall, Figure 1 highlights that our experimental low-fluency decks fall close to the 10th percentile of the distributions, while the high-fluency decks are near the 90th percentiles."
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html",
    "title": "Field Experiment",
    "section": "",
    "text": "In our main study, we ran a field experiment in which we varied the visual fluency and the substantive quality of pitch decks of our two fictitious startups (Software: PerkSouq; Healthcare: Brachytix).\nSpecifically, we sent out a standardized email to 39,977 potential investors (24,961 for the software startup and 15,016 for healthcare startup), and tracked whether the potential investor clicks on the link to the pitch deck. Every investor who clicked the link within 21 days of receiving the email was potentially part of our sample (pending exclusion restrictions). All data that was recorded within 21 days after an investor clicked on the link to the pitch deck was considered for analysis.\nDepending on their previous investment record, investors were matched to either the software startup or healthcare startup, then randomly assigned to one of the four experimental conditions. We pretested all the manipulations. We tracked whether an investor clicked on the link to the pitch deck, the cumulative time the pitch deck remained open, the percentage of slides viewed, and whether there was a positive reaction to the email. A reaction was considered positive if a meeting appointment was scheduled over a link in the deck and / or an email reply has been sent that clearly demonstrated investor interest.1 Participants were not aware that they took part in a scientific study and that the startups were fictitious.\nFor more details, e.g., on the exclusion criteria that we used, see the corresponding AsPredicted pre-registration listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registration\n\n\n\n\n\nStartup\nPre-Reg Date\nAsPredicted #\nData Collection Start\n\n\n\n\nPerkSouq & Brachytix\n13-01-2023\n118675\n16-01-2023\n\n\n\n\n\n\nIn what follows, we will give an overview of the results, separately for each startup, followed by figures that summarize the results of the field experiment. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(ggsankey)\nlibrary(ggsignif)\nlibrary(patchwork)\nlibrary(kableExtra)\noptions(knitr.kable.NA = '',\n        kable_styling_bootstrap_options = c(\"striped\", \"condensed\", \"responsive\"))\n\n# further packages that are loaded on demand are:\n# - supernova\n# - weights\n# - parameters\n# - broom\n# - scales\n# - performance\n# - readr\n# - lubridate\n# - stringr\n# - tidyr\n# - irr\n# - AER\n# - hrbrthemes\n# - grid\n# - gridExtra\n\n\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract ANOVA results including eta squared and put the results together as a table\n# -Note: needs the supernova library installed\nanova_tbl &lt;- function(formula, data, type = 3, ...){\n  # perform ANOVA\n  d &lt;- supernova::supernova(lm(as.formula(deparse(formula)), data = data), type = type)$tbl\n  # check whether ANOVA is univariate or factorial\n  univariate &lt;- all(d$term %in% c(\"Model\", \"Error\", \"Total\"))\n  # get rows of interest\n  if(univariate) {\n    effect_rows &lt;- d$term %notin% c(\"Error\", \"Total\")\n  } else {\n    effect_rows &lt;- d$term %notin% c(\"Model\", \"Error\", \"Total\")\n  }\n  # extract key parameters\n  effect &lt;- d$term[effect_rows]\n  MSE &lt;- round(d$MS[effect_rows], 2)\n  df &lt;- d$df[effect_rows]\n  df_res &lt;- d$df[d$term == \"Error\"]\n  statistic &lt;- round(d$F[effect_rows], 2)\n  pval &lt;- ifelse(d$p[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$p[effect_rows], 3))\n  eta &lt;- ifelse(d$PRE[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$PRE[effect_rows], 3))\n  # construct return df\n  return(data.frame(effect, MSE, df, df_res, statistic, pval, eta))\n}\n# extract GLM results and put the results together as a table\nglm_tbl &lt;- function(model, coef_digits = 2, coef_bold = TRUE, p_threshold = 0.05, ...){\n  # extract model parameters\n  if(\"tobit\" %in% class(model)){ # tobit model -&gt; broom::tidy does not work\n    res &lt;- parameters::model_parameters(model)\n    res &lt;- res[c(\"Parameter\", \"Coefficient\", \"SE\", \"z\", \"p\")]\n    names(res) &lt;- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n    # res[] &lt;- lapply(res, function(x) { attributes(x) &lt;- NULL; x })\n  } else {\n    res &lt;- broom::tidy(model)\n  }\n  pvals &lt;- res$p.value\n  res$estimate &lt;- sprintf(paste0(\"%.\", coef_digits, \"f\"), res$estimate)\n  res$std.error &lt;- sprintf(\"%.3f\", res$std.error)\n  res$statistic &lt;- sprintf(\"%.2f\", res$statistic)\n  # format p value\n  res$p.value &lt;- ifelse(res$p.value &lt; .001, \" &lt; .001\", weights::rd(res$p.value, 3))\n  # make estimates bold if below critical p value\n  if(coef_bold){\n    res$estimate[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$estimate[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$std.error[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$std.error[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$statistic[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$statistic[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$p.value[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$p.value[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n  }\n  # bind R2 and Adj. R2 to model parameters\n  r2 &lt;- performance::r2(model) # extract R2\n  end &lt;- nrow(res) + seq_len(length(r2))\n  res[end,\"term\"] &lt;- names(r2)\n  res[end,\"estimate\"] &lt;- weights::rd(unlist(r2), digits = 3)\n  # return result\n  return(res)\n}"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#descriptives-1",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#descriptives-1",
    "title": "Field Experiment",
    "section": "4.1 Descriptives",
    "text": "4.1 Descriptives\nTable 4 shows a descriptive breakdown of cumulative opening duration, completion percentage, and positive reactions by visual fluency and substantive quality conditions.\n\n\nCode\nd &lt;- d_sw\n\nd |&gt; group_by(fluency, quality) |&gt; summarize(N = n(), Mean = mean(duration),\n      SD = sd(duration)) -&gt; temp_duration\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(completion), SD = sd(completion)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_completion\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(positive), SD = sd(positive)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_positive\n\ntemp &lt;- bind_cols(temp_duration, temp_completion, temp_positive)\nnames(temp) &lt;- c(\"Fluency\", \"Quality\", \"N\", \"Mean\", \"SD\", \"Mean\", \"SD\", \"Mean\", \"SD\")\ntemp |&gt; kbl(digits = 3, format.args = list(decimal.mark = \".\", big.mark = \",\")) |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 3, \"Opening Duration\" = 2, \"Completion Percentage\" = 2, \"Positive Reaction\" = 2))\n\n\n\n\nTable 4: Descriptive statistics (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\nPositive Reaction\n\n\n\nFluency\nQuality\nN\nMean\nSD\nMean\nSD\nMean\nSD\n\n\n\n\nhigh\nhigh\n1,091\n181.742\n167.906\n0.728\n0.278\n0.122\n0.327\n\n\nhigh\nlow\n1,080\n146.706\n127.484\n0.690\n0.272\n0.084\n0.278\n\n\nlow\nhigh\n1,126\n148.829\n131.286\n0.687\n0.283\n0.083\n0.277\n\n\nlow\nlow\n1,146\n94.798\n84.112\n0.613\n0.313\n0.024\n0.154"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#opening-duration-completion-percentage",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#opening-duration-completion-percentage",
    "title": "Field Experiment",
    "section": "4.2 Opening duration, completion percentage",
    "text": "4.2 Opening duration, completion percentage\nTable 5 shows the result of two factorial ANOVAs that model cumulative opening duration and completion percentage as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\nAdditionally, Table 6 shows the results of robustness check regressions in which we included investment experience, investor type, gender, and country as control variables in addition to fluency, quality, and their interaction (significant values with p &lt; .05 are printed in boldface). In all analyses, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n# combine results\ntemp &lt;- bind_rows(`Opening duration` = anova_tbl(duration ~ fluency * quality, d),\n                  `Completion percentage` = anova_tbl(completion ~ fluency * quality, d),\n                  .id = \"Measure\")\n\n# keep only first occurrence of each measure\ntemp$Measure[duplicated(temp$Measure)] &lt;- NA\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 2)) |&gt;\n  kbl(col.names = c(\"Measure\", \"Effect\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling()\n\n\n\n\nTable 5: ANOVA results for opening duration and completion (software startup)\n\n\n\n\n\n\n\nMeasure\nEffect\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nOpening duration\nFluency\n1996756.35\n1\n4439\n117.00\n&lt; .001\n.026\n\n\n\nQuality\n2201670.64\n1\n4439\n129.01\n&lt; .001\n.028\n\n\n\nFluency × Quality\n100126.67\n1\n4439\n5.87\n.015\n.001\n\n\nCompletion percentage\nFluency\n3.89\n1\n4439\n47.15\n&lt; .001\n.011\n\n\n\nQuality\n3.47\n1\n4439\n42.04\n&lt; .001\n.009\n\n\n\nFluency × Quality\n0.35\n1\n4439\n4.28\n.039\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# set reference levels\nd$location &lt;- relevel(as.factor(d$location), \"United States\")\nd$gender &lt;- relevel(as.factor(d$gender), \"female\")\nd$type &lt;- relevel(as.factor(d$type), \"Angel\")\n\n\n# run regressions\nglm_tbl(lm(duration ~ fluency * quality + investments + type + gender + location, d)) -&gt; temp_duration\nglm_tbl(lm(completion ~ fluency * quality + investments + type + gender + location, d), coef_digits = 3) -&gt; temp_completion\n\ntemp &lt;- bind_cols(temp_duration, temp_completion[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Opening Duration\" = 4, \"Completion Percentage\" = 4)) |&gt;\n  row_spec(16, extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 6: Robustness check regressions for opening duration and completion percentage with control variables (software startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n150.54\n6.614\n22.76\n&lt; .001\n0.671\n0.015\n46.15\n&lt; .001\n\n\nFluency\n21.32\n1.961\n10.87\n&lt; .001\n0.030\n0.004\n6.89\n&lt; .001\n\n\nQuality\n22.36\n1.964\n11.38\n&lt; .001\n0.028\n0.004\n6.48\n&lt; .001\n\n\nFluency × Quality\n-4.89\n1.962\n-2.49\n.013\n-0.009\n0.004\n-2.08\n.037\n\n\nInvestment experience\n0.13\n0.545\n0.24\n.809\n0.001\n0.001\n0.83\n.405\n\n\nInvestor type [Venture Capital]\n1.81\n5.206\n0.35\n.728\n-0.002\n0.011\n-0.19\n.852\n\n\nGender [Male]\n-8.22\n6.724\n-1.22\n.221\n0.004\n0.015\n0.30\n.761\n\n\nCountry [Brazil]\n-4.54\n25.722\n-0.18\n.860\n0.039\n0.057\n0.69\n.491\n\n\nCountry [Canada]\n8.39\n13.662\n0.61\n.539\n0.022\n0.030\n0.73\n.465\n\n\nCountry [China]\n-26.29\n28.027\n-0.94\n.348\n-0.050\n0.062\n-0.81\n.416\n\n\nCountry [France]\n-9.67\n15.473\n-0.63\n.532\n0.024\n0.034\n0.69\n.489\n\n\nCountry [Germany]\n-0.27\n13.134\n-0.02\n.983\n0.057\n0.029\n1.96\n.050\n\n\nCountry [India]\n12.18\n10.931\n1.11\n.265\n0.017\n0.024\n0.70\n.487\n\n\nCountry [Israel]\n-23.51\n17.968\n-1.31\n.191\n-0.014\n0.040\n-0.36\n.723\n\n\nCountry [Singapore]\n-43.21\n18.088\n-2.39\n.017\n-0.030\n0.040\n-0.75\n.455\n\n\nCountry [United Kingdom]\n-1.83\n7.581\n-0.24\n.809\n0.003\n0.017\n0.17\n.864\n\n\nR2\n.057\n\n\n\n.023\n\n\n\n\n\nR2adj.\n.054\n\n\n\n.019"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#positive-reactions",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#positive-reactions",
    "title": "Field Experiment",
    "section": "4.3 Positive reactions",
    "text": "4.3 Positive reactions\nA logistic regression model was estimated to analyze the effects of visual fluency, substantive quality, and their interaction on whether there was a positive reaction to the emails. Table 7 shows the result of this regression model, next to several robustness check models that included control variables and / or were specified as Tobit models.\nIn all regressions, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# run regressions\nglm_tbl(glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive\nglm_tbl(glm(positive ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive_controls\nglm_tbl(AER::tobit(positive ~ fluency * quality, data = d), coef_digits = 3) -&gt; temp_positive_tobit\nglm_tbl(AER::tobit(positive ~ fluency * quality + investments + type + gender + location, data = d), coef_digits = 3) -&gt; temp_positive_controls_tobit\n\n\n# add empty rows to models w/o controls to enable column binding\nrows_control &lt;- nrow(temp_positive_controls)\nrows_simple &lt;- nrow(temp_positive)\ntemp_positive[(rows_simple+1):rows_control,] &lt;- NA\ntemp_positive_tobit[(rows_simple+1):rows_control,] &lt;- NA\n# put interaction and R2 row to the end\ntemp_positive[rows_control-1,] &lt;- temp_positive[rows_simple-1,]\ntemp_positive[rows_control,] &lt;- temp_positive[rows_simple,]\ntemp_positive[rows_simple-1,] &lt;- NA\ntemp_positive[rows_simple,] &lt;- NA\ntemp_positive_tobit[rows_control-1,] &lt;- temp_positive_tobit[rows_simple-1,]\ntemp_positive_tobit[rows_control,] &lt;- temp_positive_tobit[rows_simple,]\ntemp_positive_tobit[rows_simple-1,] &lt;- NA\ntemp_positive_tobit[rows_simple,] &lt;- NA\n\n# table binary logit\ntemp &lt;- bind_cols(temp_positive, temp_positive_controls[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Binary Logit\" = 4, \"Binary Logit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n# table tobit\ntemp &lt;- bind_cols(temp_positive_tobit, temp_positive_controls_tobit[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Nagelkerke's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Tobit Model\" = 4, \"Tobit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 7: Binary logit and Tobit regressions for positive reactions (software startup)\n\n\n\n\n\n\n\n(a) Binary logit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logit\n\n\nBinary Logit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.611\n0.066\n-39.82\n&lt; .001\n-2.634\n0.197\n-13.38\n&lt; .001\n\n\nFluency\n0.431\n0.066\n6.57\n&lt; .001\n0.431\n0.066\n6.56\n&lt; .001\n\n\nQuality\n0.426\n0.066\n6.49\n&lt; .001\n0.426\n0.066\n6.49\n&lt; .001\n\n\nFluency × Quality\n-0.220\n0.066\n-3.35\n&lt; .001\n-0.219\n0.066\n-3.34\n&lt; .001\n\n\nInvestment experience\n\n\n\n\n0.014\n0.012\n1.12\n.263\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.031\n0.147\n0.21\n.835\n\n\nGender [Male]\n\n\n\n\n0.013\n0.198\n0.07\n.947\n\n\nCountry [Brazil]\n\n\n\n\n-0.731\n1.026\n-0.71\n.476\n\n\nCountry [Canada]\n\n\n\n\n0.062\n0.379\n0.16\n.869\n\n\nCountry [China]\n\n\n\n\n-0.752\n1.030\n-0.73\n.466\n\n\nCountry [France]\n\n\n\n\n-0.410\n0.522\n-0.79\n.432\n\n\nCountry [Germany]\n\n\n\n\n0.129\n0.358\n0.36\n.718\n\n\nCountry [India]\n\n\n\n\n-0.406\n0.371\n-1.09\n.274\n\n\nCountry [Israel]\n\n\n\n\n0.445\n0.416\n1.07\n.284\n\n\nCountry [Singapore]\n\n\n\n\n0.580\n0.416\n1.39\n.164\n\n\nCountry [United Kingdom]\n\n\n\n\n-0.286\n0.241\n-1.19\n.236\n\n\nTjur's R2\n.017\n\n\n\n.020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tobit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTobit Model\n\n\nTobit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.708\n0.154\n-17.56\n&lt; .001\n-2.746\n0.229\n-11.98\n&lt; .001\n\n\nFluency\n0.370\n0.057\n6.49\n&lt; .001\n0.372\n0.057\n6.51\n&lt; .001\n\n\nQuality\n0.366\n0.057\n6.41\n&lt; .001\n0.364\n0.057\n6.38\n&lt; .001\n\n\nFluency × Quality\n-0.176\n0.055\n-3.18\n.001\n-0.176\n0.055\n-3.19\n.001\n\n\nInvestment experience\n\n\n\n\n0.012\n0.012\n0.98\n.326\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.017\n0.133\n0.13\n.900\n\n\nGender [Male]\n\n\n\n\n0.042\n0.179\n0.23\n.816\n\n\nCountry [Brazil]\n\n\n\n\n-0.631\n0.837\n-0.75\n.451\n\n\nCountry [Canada]\n\n\n\n\n0.084\n0.344\n0.24\n.808\n\n\nCountry [China]\n\n\n\n\n-0.625\n0.847\n-0.74\n.460\n\n\nCountry [France]\n\n\n\n\n-0.379\n0.451\n-0.84\n.400\n\n\nCountry [Germany]\n\n\n\n\n0.193\n0.322\n0.60\n.549\n\n\nCountry [India]\n\n\n\n\n-0.376\n0.320\n-1.18\n.240\n\n\nCountry [Israel]\n\n\n\n\n0.391\n0.404\n0.97\n.333\n\n\nCountry [Singapore]\n\n\n\n\n0.547\n0.402\n1.36\n.173\n\n\nCountry [United Kingdom]\n\n\n\n\n-0.237\n0.210\n-1.13\n.260\n\n\nNagelkerke's R2\n.040\n\n\n\n.044"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#percentage-increases",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#percentage-increases",
    "title": "Field Experiment",
    "section": "4.4 Percentage increases",
    "text": "4.4 Percentage increases\nTo facilitate the interpretation of the results, we calculated the percentage increase in the dependent variables for the high level of visual fluency and substantive quality compared to the respective low level. Table 8 shows these percentage increases for the opening duration, completion percentage, and positive reactions.\n\n\nCode\n# calculate percentage increases based on marginal means\n#\n# compute marginal means using the `marginaleffects` package\n#\n# duration\nm_dur &lt;- aov(duration ~ fluency * quality, d)\nmm_dur_flu &lt;- marginaleffects::predictions(m_dur, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_dur_qual &lt;- marginaleffects::predictions(m_dur, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# completion\nm_compl &lt;- aov(completion ~ fluency * quality, d)\nmm_compl_flu &lt;- marginaleffects::predictions(m_compl, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_compl_qual &lt;- marginaleffects::predictions(m_compl, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# positive reactions\nm_pos &lt;- glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), data = d)\nmm_pos_flu &lt;- marginaleffects::predictions(m_pos, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_pos_qual &lt;- marginaleffects::predictions(m_pos, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n\n# compute effect sizes as percentage increases for high vs. low levels\n#\n# we use the formula: 100 * (high / low - 1)\n# That is, we multiply by 100 to get the percentage increase and then\n# subtract 1 to get the percentage increase relative to the low level.\n#\n# duration: fluency\nperc_incr_dur_flu &lt;- 100 * (mm_dur_flu$estimate[mm_dur_flu$fluency == \"high\"] / mm_dur_flu$estimate[mm_dur_flu$fluency == \"low\"] - 1)\n# duration: quality\nperc_incr_dur_qual &lt;- 100 * (mm_dur_qual$estimate[mm_dur_qual$quality == \"high\"] / mm_dur_qual$estimate[mm_dur_qual$quality == \"low\"] - 1)\n\n# completion: fluency\nperc_incr_compl_flu &lt;- 100 * (mm_compl_flu$estimate[mm_compl_flu$fluency == \"high\"] / mm_compl_flu$estimate[mm_compl_flu$fluency == \"low\"] - 1)\n# completion: quality\nperc_incr_compl_qual &lt;- 100 * (mm_compl_qual$estimate[mm_compl_qual$quality == \"high\"] / mm_compl_qual$estimate[mm_compl_qual$quality == \"low\"] - 1)\n\n# positive: fluency\nperc_incr_pos_flu &lt;- 100 * (mm_pos_flu$estimate[mm_pos_flu$fluency == \"high\"] / mm_pos_flu$estimate[mm_pos_flu$fluency == \"low\"] - 1)\n# positive: quality\nperc_incr_pos_qual &lt;- 100 * (mm_pos_qual$estimate[mm_pos_qual$quality == \"high\"] / mm_pos_qual$estimate[mm_pos_qual$quality == \"low\"] - 1)\n\n# create table\ndata.frame(\n  Measure = c(\"Opening duration\", \"Completion percentage\", \"Positive reaction\"),\n  Fluency = c(sprintf('%.1f%%', perc_incr_dur_flu), sprintf('%.1f%%', perc_incr_compl_flu), sprintf('%.1f%%', perc_incr_pos_flu)),\n  Quality = c(sprintf('%.1f%%', perc_incr_dur_qual), sprintf('%.1f%%', perc_incr_compl_qual), sprintf('%.1f%%', perc_incr_pos_qual))\n) |&gt; kbl() |&gt; kable_styling()\n\n\n\n\nTable 8: Percentage increases for high vs. low levels of visual fluency and substantive quality (software startup)\n\n\n\n\n\n\n\nMeasure\nFluency\nQuality\n\n\n\n\nOpening duration\n34.8%\n36.9%\n\n\nCompletion percentage\n9.1%\n8.6%\n\n\nPositive reaction\n122.8%\n120.7%"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#descriptives-2",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#descriptives-2",
    "title": "Field Experiment",
    "section": "5.1 Descriptives",
    "text": "5.1 Descriptives\nTable 9 shows a descriptive breakdown of cumulative opening duration, completion percentage, and positive reactions by visual fluency and substantive quality conditions.\n\n\nCode\nd &lt;- d_hc\n\nd |&gt; group_by(fluency, quality) |&gt; summarize(N = n(), Mean = mean(duration),\n      SD = sd(duration)) -&gt; temp_duration\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(completion), SD = sd(completion)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_completion\nd |&gt; group_by(fluency, quality) |&gt; summarize(Mean = mean(positive), SD = sd(positive)) |&gt; ungroup() |&gt; select(Mean, SD) -&gt; temp_positive\n\ntemp &lt;- bind_cols(temp_duration, temp_completion, temp_positive)\nnames(temp) &lt;- c(\"Fluency\", \"Quality\", \"N\", \"Mean\", \"SD\", \"Mean\", \"SD\", \"Mean\", \"SD\")\ntemp |&gt; kbl(digits = 3) |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 3, \"Opening Duration\" = 2, \"Completion Percentage\" = 2, \"Positive Reaction\" = 2))\n\n\n\n\nTable 9: Descriptive statistics (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\nPositive Reaction\n\n\n\nFluency\nQuality\nN\nMean\nSD\nMean\nSD\nMean\nSD\n\n\n\n\nhigh\nhigh\n589\n125.431\n90.226\n0.775\n0.228\n0.171\n0.377\n\n\nhigh\nlow\n605\n100.198\n88.423\n0.688\n0.245\n0.094\n0.292\n\n\nlow\nhigh\n593\n99.042\n77.849\n0.691\n0.253\n0.099\n0.300\n\n\nlow\nlow\n620\n69.044\n64.892\n0.586\n0.294\n0.037\n0.189"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#opening-duration-completion-percentage-1",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#opening-duration-completion-percentage-1",
    "title": "Field Experiment",
    "section": "5.2 Opening duration, completion percentage",
    "text": "5.2 Opening duration, completion percentage\nTable 10 shows the result of two factorial ANOVAs that model cumulative opening duration and completion percentage as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\nAdditionally, Table 11 shows the results of robustness check regressions in which we included investment experience, investor type, gender, and country as control variables in addition to fluency, quality, and their interaction (significant values with p &lt; .05 are printed in boldface). In all analyses, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n# combine results\ntemp &lt;- bind_rows(`Opening duration` = anova_tbl(duration ~ fluency * quality, d),\n                  `Completion percentage` = anova_tbl(completion ~ fluency * quality, d),\n                  .id = \"Measure\")\n\n# keep only first occurrence of each measure\ntemp$Measure[duplicated(temp$Measure)] &lt;- NA\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 2)) |&gt;\n  kbl(col.names = c(\"Measure\", \"Effect\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling()\n\n\n\n\nTable 10: ANOVA results for opening duration and completion (healthcare startup)\n\n\n\n\n\n\n\nMeasure\nEffect\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nOpening duration\nFluency\n497944.86\n1\n2403\n76.20\n&lt; .001\n.031\n\n\n\nQuality\n458729.52\n1\n2403\n70.20\n&lt; .001\n.028\n\n\n\nFluency × Quality\n3415.38\n1\n2403\n0.52\n.470\n&lt; .001\n\n\nCompletion percentage\nFluency\n5.21\n1\n2403\n79.11\n&lt; .001\n.032\n\n\n\nQuality\n5.56\n1\n2403\n84.53\n&lt; .001\n.034\n\n\n\nFluency × Quality\n0.05\n1\n2403\n0.83\n.364\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# set reference levels\nd$location &lt;- relevel(as.factor(d$location), \"United States\")\nd$gender &lt;- relevel(as.factor(d$gender), \"female\")\nd$type &lt;- relevel(as.factor(d$type), \"Angel\")\n\n\n# run regressions\nglm_tbl(lm(duration ~ fluency * quality + investments + type + gender + location, d)) -&gt; temp_duration\nglm_tbl(lm(completion ~ fluency * quality + investments + type + gender + location, d), coef_digits = 3) -&gt; temp_completion\n\ntemp &lt;- bind_cols(temp_duration, temp_completion[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Opening Duration\" = 4, \"Completion Percentage\" = 4)) |&gt;\n  row_spec(16, extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 11: Robustness check regressions for opening duration and completion percentage with control variables (healthcare startup)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpening Duration\n\n\nCompletion Percentage\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n95.01\n5.426\n17.51\n&lt; .001\n0.680\n0.017\n39.44\n&lt; .001\n\n\nFluency\n14.32\n1.656\n8.65\n&lt; .001\n0.046\n0.005\n8.83\n&lt; .001\n\n\nQuality\n13.91\n1.655\n8.40\n&lt; .001\n0.048\n0.005\n9.15\n&lt; .001\n\n\nFluency × Quality\n-1.19\n1.651\n-0.72\n.471\n-0.005\n0.005\n-0.86\n.389\n\n\nInvestment experience\n0.50\n0.941\n0.54\n.592\n0.004\n0.003\n1.43\n.153\n\n\nInvestor type [Venture Capital]\n-1.86\n3.812\n-0.49\n.626\n-0.017\n0.012\n-1.44\n.151\n\n\nGender [Male]\n3.21\n5.231\n0.61\n.539\n0.000\n0.017\n0.02\n.986\n\n\nCountry [Brazil]\n28.17\n20.980\n1.34\n.180\n0.056\n0.067\n0.84\n.402\n\n\nCountry [Canada]\n13.84\n11.540\n1.20\n.231\n0.032\n0.037\n0.88\n.377\n\n\nCountry [China]\n-2.59\n12.788\n-0.20\n.839\n-0.031\n0.041\n-0.76\n.449\n\n\nCountry [France]\n-7.84\n10.027\n-0.78\n.434\n-0.012\n0.032\n-0.36\n.716\n\n\nCountry [Germany]\n5.27\n10.580\n0.50\n.618\n0.018\n0.034\n0.55\n.585\n\n\nCountry [India]\n8.41\n10.985\n0.77\n.444\n0.021\n0.035\n0.61\n.541\n\n\nCountry [Israel]\n28.49\n14.909\n1.91\n.056\n0.004\n0.047\n0.08\n.935\n\n\nCountry [Singapore]\n-11.03\n13.430\n-0.82\n.412\n0.056\n0.043\n1.31\n.189\n\n\nCountry [United Kingdom]\n-4.62\n6.178\n-0.75\n.455\n0.023\n0.020\n1.18\n.236\n\n\nR2\n.062\n\n\n\n.068\n\n\n\n\n\nR2adj.\n.056\n\n\n\n.062"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#positive-reactions-1",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#positive-reactions-1",
    "title": "Field Experiment",
    "section": "5.3 Positive reactions",
    "text": "5.3 Positive reactions\nA logistic regression model was estimated to analyze the effects of visual fluency, substantive quality, and their interaction on whether there was a positive reaction to the emails. Table 12 shows the result of this regression model, next to several robustness check models that included control variables and / or were specified as Tobit models.\nIn all regressions, we used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1.\n\n\nCode\n# run regressions\nglm_tbl(glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive\nglm_tbl(glm(positive ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d), coef_digits = 3) -&gt; temp_positive_controls\nglm_tbl(AER::tobit(positive ~ fluency * quality, data = d), coef_digits = 3) -&gt; temp_positive_tobit\nglm_tbl(AER::tobit(positive ~ fluency * quality + investments + type + gender + location, data = d), coef_digits = 3) -&gt; temp_positive_controls_tobit\n\n\n# add empty rows to models w/o controls to enable column binding\nrows_control &lt;- nrow(temp_positive_controls)\nrows_simple &lt;- nrow(temp_positive)\ntemp_positive[(rows_simple+1):rows_control,] &lt;- NA\ntemp_positive_tobit[(rows_simple+1):rows_control,] &lt;- NA\n# put interaction and R2 row to the end\ntemp_positive[rows_control-1,] &lt;- temp_positive[rows_simple-1,]\ntemp_positive[rows_control,] &lt;- temp_positive[rows_simple,]\ntemp_positive[rows_simple-1,] &lt;- NA\ntemp_positive[rows_simple,] &lt;- NA\ntemp_positive_tobit[rows_control-1,] &lt;- temp_positive_tobit[rows_simple-1,]\ntemp_positive_tobit[rows_control,] &lt;- temp_positive_tobit[rows_simple,]\ntemp_positive_tobit[rows_simple-1,] &lt;- NA\ntemp_positive_tobit[rows_simple,] &lt;- NA\n\n# table binary logit\ntemp &lt;- bind_cols(temp_positive, temp_positive_controls[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Binary Logit\" = 4, \"Binary Logit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n# table tobit\ntemp &lt;- bind_cols(temp_positive_tobit, temp_positive_controls_tobit[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Nagelkerke's R&lt;sup&gt;2&lt;/sup&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Tobit Model\" = 4, \"Tobit w/ Controls\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 12: Binary logit and Tobit regressions for positive reactions (healthcare startup)\n\n\n\n\n\n\n\n(a) Binary logit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logit\n\n\nBinary Logit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.324\n0.077\n-30.12\n&lt; .001\n-2.068\n0.214\n-9.65\n&lt; .001\n\n\nFluency\n0.405\n0.077\n5.25\n&lt; .001\n0.401\n0.078\n5.16\n&lt; .001\n\n\nQuality\n0.435\n0.077\n5.64\n&lt; .001\n0.448\n0.078\n5.76\n&lt; .001\n\n\nFluency × Quality\n-0.091\n0.077\n-1.18\n.236\n-0.093\n0.078\n-1.20\n.229\n\n\nInvestment experience\n\n\n\n\n-0.020\n0.049\n-0.41\n.679\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.059\n0.165\n0.36\n.721\n\n\nGender [Male]\n\n\n\n\n-0.360\n0.201\n-1.79\n.074\n\n\nCountry [Brazil]\n\n\n\n\n-0.503\n1.045\n-0.48\n.630\n\n\nCountry [Canada]\n\n\n\n\n0.452\n0.425\n1.06\n.287\n\n\nCountry [China]\n\n\n\n\n-0.205\n0.545\n-0.38\n.707\n\n\nCountry [France]\n\n\n\n\n0.783\n0.328\n2.39\n.017\n\n\nCountry [Germany]\n\n\n\n\n0.608\n0.380\n1.60\n.110\n\n\nCountry [India]\n\n\n\n\n-0.324\n0.532\n-0.61\n.542\n\n\nCountry [Israel]\n\n\n\n\n0.099\n0.624\n0.16\n.874\n\n\nCountry [Singapore]\n\n\n\n\n0.618\n0.462\n1.34\n.181\n\n\nCountry [United Kingdom]\n\n\n\n\n0.131\n0.251\n0.52\n.602\n\n\nTjur's R2\n.025\n\n\n\n.033\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tobit models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTobit Model\n\n\nTobit w/ Controls\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-2.287\n0.162\n-14.15\n&lt; .001\n-2.012\n0.230\n-8.73\n&lt; .001\n\n\nFluency\n0.344\n0.066\n5.23\n&lt; .001\n0.337\n0.066\n5.14\n&lt; .001\n\n\nQuality\n0.370\n0.066\n5.60\n&lt; .001\n0.377\n0.066\n5.70\n&lt; .001\n\n\nFluency × Quality\n-0.063\n0.063\n-0.99\n.323\n-0.062\n0.063\n-0.98\n.326\n\n\nInvestment experience\n\n\n\n\n-0.017\n0.041\n-0.42\n.676\n\n\nInvestor type [Venture Capital]\n\n\n\n\n0.022\n0.142\n0.15\n.878\n\n\nGender [Male]\n\n\n\n\n-0.337\n0.179\n-1.88\n.060\n\n\nCountry [Brazil]\n\n\n\n\n-0.371\n0.836\n-0.44\n.657\n\n\nCountry [Canada]\n\n\n\n\n0.414\n0.382\n1.08\n.279\n\n\nCountry [China]\n\n\n\n\n-0.141\n0.466\n-0.30\n.762\n\n\nCountry [France]\n\n\n\n\n0.689\n0.308\n2.23\n.026\n\n\nCountry [Germany]\n\n\n\n\n0.557\n0.346\n1.61\n.107\n\n\nCountry [India]\n\n\n\n\n-0.238\n0.436\n-0.55\n.586\n\n\nCountry [Israel]\n\n\n\n\n0.019\n0.555\n0.03\n.973\n\n\nCountry [Singapore]\n\n\n\n\n0.483\n0.433\n1.11\n.266\n\n\nCountry [United Kingdom]\n\n\n\n\n0.143\n0.217\n0.66\n.510\n\n\nNagelkerke's R2\n.047\n\n\n\n.056"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#percentage-increases-1",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#percentage-increases-1",
    "title": "Field Experiment",
    "section": "5.4 Percentage increases",
    "text": "5.4 Percentage increases\nTo facilitate the interpretation of the results, we calculated the percentage increase in the dependent variables for the high level of visual fluency and substantive quality compared to the respective low level. Table 13 shows these percentage increases for the opening duration, completion percentage, and positive reactions.\n\n\nCode\n# calculate percentage increases based on marginal means\n#\n# compute marginal means using the `marginaleffects` package\n#\n# duration\nm_dur &lt;- aov(duration ~ fluency * quality, d)\nmm_dur_flu &lt;- marginaleffects::predictions(m_dur, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_dur_qual &lt;- marginaleffects::predictions(m_dur, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# completion\nm_compl &lt;- aov(completion ~ fluency * quality, d)\nmm_compl_flu &lt;- marginaleffects::predictions(m_compl, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_compl_qual &lt;- marginaleffects::predictions(m_compl, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n# positive reactions\nm_pos &lt;- glm(positive ~ fluency * quality, family = binomial(link = \"logit\"), data = d)\nmm_pos_flu &lt;- marginaleffects::predictions(m_pos, by = c(\"fluency\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\nmm_pos_qual &lt;- marginaleffects::predictions(m_pos, by = c(\"quality\"),\n                   newdata = marginaleffects::datagrid(grid_type = \"balanced\"))\n\n# compute effect sizes as percentage increases for high vs. low levels\n#\n# we use the formula: 100 * (high / low - 1)\n# That is, we multiply by 100 to get the percentage increase and then\n# subtract 1 to get the percentage increase relative to the low level.\n#\n# duration: fluency\nperc_incr_dur_flu &lt;- 100 * (mm_dur_flu$estimate[mm_dur_flu$fluency == \"high\"] / mm_dur_flu$estimate[mm_dur_flu$fluency == \"low\"] - 1)\n# duration: quality\nperc_incr_dur_qual &lt;- 100 * (mm_dur_qual$estimate[mm_dur_qual$quality == \"high\"] / mm_dur_qual$estimate[mm_dur_qual$quality == \"low\"] - 1)\n\n# completion: fluency\nperc_incr_compl_flu &lt;- 100 * (mm_compl_flu$estimate[mm_compl_flu$fluency == \"high\"] / mm_compl_flu$estimate[mm_compl_flu$fluency == \"low\"] - 1)\n# completion: quality\nperc_incr_compl_qual &lt;- 100 * (mm_compl_qual$estimate[mm_compl_qual$quality == \"high\"] / mm_compl_qual$estimate[mm_compl_qual$quality == \"low\"] - 1)\n\n# positive: fluency\nperc_incr_pos_flu &lt;- 100 * (mm_pos_flu$estimate[mm_pos_flu$fluency == \"high\"] / mm_pos_flu$estimate[mm_pos_flu$fluency == \"low\"] - 1)\n# positive: quality\nperc_incr_pos_qual &lt;- 100 * (mm_pos_qual$estimate[mm_pos_qual$quality == \"high\"] / mm_pos_qual$estimate[mm_pos_qual$quality == \"low\"] - 1)\n\n# create table\ndata.frame(\n  Measure = c(\"Opening duration\", \"Completion percentage\", \"Positive reaction\"),\n  Fluency = c(sprintf('%.1f%%', perc_incr_dur_flu), sprintf('%.1f%%', perc_incr_compl_flu), sprintf('%.1f%%', perc_incr_pos_flu)),\n  Quality = c(sprintf('%.1f%%', perc_incr_dur_qual), sprintf('%.1f%%', perc_incr_compl_qual), sprintf('%.1f%%', perc_incr_pos_qual))\n) |&gt; kbl() |&gt; kable_styling()\n\n\n\n\nTable 13: Percentage increases for high vs. low levels of visual fluency and substantive quality (healthcare startup)\n\n\n\n\n\n\n\nMeasure\nFluency\nQuality\n\n\n\n\nOpening duration\n34.2%\n32.6%\n\n\nCompletion percentage\n14.6%\n15.1%\n\n\nPositive reaction\n108.9%\n120.6%"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#sankey-plots",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#sankey-plots",
    "title": "Field Experiment",
    "section": "6.1 Sankey plots",
    "text": "6.1 Sankey plots\nFigure 1 (a) and Figure 1 (b) show the flow of the field experiment for the software and healthcare startups, respectively, as Sankey diagram.\n\n\nCode\n# aesthetics\n#\n# define colors\nblue_main &lt;- \"#297FB8\"\nblue_dark &lt;- \"#2D3E50\"\nblue_light &lt;- \"#A0ABBF\"\n\n# define custom geom\ngeom_sankey_label_richtext &lt;- function (mapping = NULL, data = NULL, position = \"identity\", \n  na.rm = FALSE, show.legend = NA, space = NULL, type = \"sankey\", width = 0.1,\n  inherit.aes = TRUE, family =\n    if (.Platform$OS.type == \"windows\") \"Roboto Condensed\" else \"Roboto Condensed Light\", ...)  # added font family\n{\n  label.aes &lt;- list(...)\n  list(label = ggplot2::layer(stat = ggsankey:::StatSankeyText, data = data, \n    mapping = mapping, geom = \"richtext\", position = position, # changed: geom = \"label\"\n    show.legend = show.legend, inherit.aes = inherit.aes, \n    params = purrr::flatten(list(na.rm = na.rm, width = width, \n      space = space, type = type, label.aes, family = family))))\n}\n\n# change ipsum theme to work for sankey plot\ntheme_sankey_ipsum_rc &lt;- function (base_family = \"Roboto Condensed\", base_size = 11.5, ...)\n{\n  {\n    hrbrthemes::theme_ipsum_rc(base_family = base_family, base_size = base_size, ...) %+replace% \n      ggplot2::theme(panel.border = ggplot2::element_blank(), \n        panel.grid.major = ggplot2::element_blank(), \n        panel.grid.minor = ggplot2::element_blank(), \n        axis.line = ggplot2::element_line(colour = \"black\", \n          linewidth = ggplot2::rel(1)), legend.key = ggplot2::element_blank(), \n        strip.background = ggplot2::element_rect(fill = \"white\", \n          colour = \"transparent\", linewidth = ggplot2::rel(2)), \n        complete = TRUE, axis.line.y = ggplot2::element_blank(), \n        axis.line.x = ggplot2::element_blank(),\n        axis.text.x = ggplot2::element_blank(), \n        axis.text.y = ggplot2::element_blank(), \n        axis.ticks.y = ggplot2::element_blank(),\n        axis.ticks.x = ggplot2::element_blank(),\n        # set background to white for quarto to avoid transparency issues\n        panel.background = ggplot2::element_rect(fill='white', color='white'),\n        plot.background = ggplot2::element_rect(fill='white', color='white')\n      )\n  }\n}\n\n\n# Software startup --------------------------------------------------\n#\n# prepare sankey data (the data is already loaded at the beginning of the script)\n#\n# create binary indicators for the different stages\nsankey_soft |&gt; mutate(\n  `Sent mails` = case_when(\n    hard_bounce == 1 ~ \"Hard bounce\",\n    hard_bounce == 0 ~ \"Delivered\"\n  ),\n  Delivered = case_when(\n    hard_bounce == 0 & is.na(date_first_click) ~ \"Recipient did not open deck\",\n    hard_bounce == 0 & !is.na(date_first_click) ~ \"Recipient opened deck\"\n  ),\n  `Recipient opened deck` = case_when(\n    hard_bounce == 0 & !is.na(date_first_click) & is.na(date_reply) ~ \"Recipient did not reply\",\n    hard_bounce == 0 & !is.na(date_reply) ~ \"Recipient replied\"\n  ),\n  `Recipient replied` = case_when(\n    positive == 0 ~ \"Negative reply\",\n    positive == 1 ~ \"Positive reply\"\n  ),\n  Category = category,\n  `Initial sample` = \"Software&lt;br&gt;startup\"\n) -&gt; sankey_soft\n\n# make data long for sankey graph (for simplicity, just called `df`)\nsankey_soft |&gt; make_long(`Initial sample`, `Sent mails`, Delivered,\n                         `Recipient opened deck`, `Recipient replied`,\n                         Category) -&gt; df\n\n# add group_nodes (needed for calculation of group percentages later)\ndf |&gt; mutate(\n  group_node = case_when(\n    node == \"Hard bounce\" | node == \"Delivered\" ~ \"Software&lt;br&gt;startup\",\n    node == \"Recipient did not open deck\" | node == \"Recipient opened deck\" ~ \"Delivered\",\n    node == \"Recipient did not reply\" | node == \"Recipient replied\" ~ \"Recipient opened deck\",\n    node == \"Negative reply\" | node == \"Positive reply\" ~ \"Recipient replied\",\n    node == \"other\" | node == \"geography\" | node == \"no investments\" | node == \"stage\" | node == \"investment strategy\" | node == \"industry\" | node == \"no specific reason\" ~ \"Negative reply\",\n    node == \"more info/clarification\" | node == \"meeting\" | node == \"formal application\" | node == \"updates\" | node == \"referral\" ~ \"Positive reply\"\n  )  \n) -&gt; df\n\n# add information about node N and group_node N and calculate percentages\ndf |&gt;\n  # count obs per node\n  group_by(node) |&gt; mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  # count obs per group_node\n  group_by(group_node) |&gt; mutate(n_group = ifelse(is.na(group_node), NA, n())) |&gt; \n  ungroup() |&gt;\n  # add percentages\n  mutate(pct = n/n_group) -&gt; df\n\n# manually change order of nodes\ndf |&gt; mutate(\n  node = factor(node,\n                levels = c(\"Software&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  ),\n  next_node = factor(next_node,\n                levels = c(\"Software&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  )\n) -&gt; df\n\n# make sankey plot\n#\n# first change the data so that percentages are displayed in the node texts\ndf |&gt; tidyr::drop_na(node) |&gt;\n  mutate(\n    pct = ifelse(is.na(pct), 9999, pct), # dummy-replace NA with 9999, otherwise later ifelse does not work\n    node_text = ifelse(\n      # group_node and thus pct empty (here: 9999)\n      pct == 9999,\n      # yes, pct empty -&gt; no percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s&lt;/span&gt;\",\n                        node, scales::comma(n)),\n      # no, pct not empty -&gt; add percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s (%s%%)&lt;/span&gt;\",\n                        node,\n                        scales::comma(n),\n                 ifelse(pct&lt;.01, paste0(\"0\",weights::rd(pct*100,2)), weights::rd(pct*100,2)))\n      )) |&gt;\n  # now create the plot\n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node),\n             label = node_text,\n             color = factor(node)\n             )) +\n  geom_sankey(flow.alpha = 0.65, show.legend = FALSE, node.color = blue_dark, node.fill = blue_dark) +\n  geom_sankey_label_richtext(size = 3, color = \"black\", fill = \"white\") +\n  labs(x = element_blank()) +\n  # apply customized theme\n  theme_sankey_ipsum_rc(base_size = 11, plot_margin = margin(5, 5, 5, 5)) -&gt; p\n\n# more customizing\n#\n# now: change the color of the segments\n# to this end, first decompose the plot into its parts using `ggplot_build`\nq &lt;- ggplot_build(p)\n\n# first data layer is for line color of flows\nl1 &lt;- q$data[[1]]$colour\n# second data layer is for line color of nodes\nl2 &lt;- q$data[[2]]$colour\n\n# fill colors\nf1 &lt;- q$data[[1]]$fill # flows\nf2 &lt;- q$data[[2]]$fill # nodes\n\n# relevant flows are all of length 600, and only starting color value is relevant\n# thus, color change points (ccp) are\nccp &lt;- seq(1, length(f1), by = 600)\nq$data[[1]]$fill[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$fill[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$fill[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$fill[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$fill[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$fill[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$fill[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$fill[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$fill[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$fill[ccp[16:20]] &lt;- blue_main # positive categories\n\nq$data[[1]]$colour[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$colour[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$colour[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$colour[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$colour[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$colour[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$colour[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$colour[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$colour[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$colour[ccp[16:20]] &lt;- blue_main # positive categories\n\n# put all back together and plot the modified, final plot\np_mod &lt;- ggplot_gtable(q)\nplot(p_mod)\n\n\n# Healthcare startup ------------------------------------------------\n#\n# prepare sankey data (the data is already loaded at the beginning of the script)\n#\n# create binary indicators for the different stages\nsankey_health |&gt; mutate(\n  `Sent mails` = case_when(\n    hard_bounce == 1 ~ \"Hard bounce\",\n    hard_bounce == 0 ~ \"Delivered\"\n  ),\n  Delivered = case_when(\n    hard_bounce == 0 & is.na(date_first_click) ~ \"Recipient did not open deck\",\n    hard_bounce == 0 & !is.na(date_first_click) ~ \"Recipient opened deck\"\n  ),\n  `Recipient opened deck` = case_when(\n    hard_bounce == 0 & !is.na(date_first_click) & is.na(date_reply) ~ \"Recipient did not reply\",\n    hard_bounce == 0 & !is.na(date_reply) ~ \"Recipient replied\"\n  ),\n  `Recipient replied` = case_when(\n    positive == 0 ~ \"Negative reply\",\n    positive == 1 ~ \"Positive reply\"\n  ),\n  Category = category,\n  `Initial sample` = \"Healthcare&lt;br&gt;startup\"\n) -&gt; sankey_health\n\n# make data long for sankey graph (for simplicity, just called `df`)\nsankey_health |&gt; make_long(`Initial sample`, `Sent mails`, Delivered,\n                         `Recipient opened deck`, `Recipient replied`,\n                         Category) -&gt; df\n\n# add group_nodes (needed for calculation of group percentages later)\ndf |&gt; mutate(\n  group_node = case_when(\n    node == \"Hard bounce\" | node == \"Delivered\" ~ \"Healthcare&lt;br&gt;startup\",\n    node == \"Recipient did not open deck\" | node == \"Recipient opened deck\" ~ \"Delivered\",\n    node == \"Recipient did not reply\" | node == \"Recipient replied\" ~ \"Recipient opened deck\",\n    node == \"Negative reply\" | node == \"Positive reply\" ~ \"Recipient replied\",\n    node == \"other\" | node == \"geography\" | node == \"no investments\" | node == \"stage\" | node == \"investment strategy\" | node == \"industry\" | node == \"no specific reason\" ~ \"Negative reply\",\n    node == \"more info/clarification\" | node == \"meeting\" | node == \"formal application\" | node == \"updates\" | node == \"referral\" ~ \"Positive reply\"\n  )  \n) -&gt; df\n\n# add information about node N and group_node N and calculate percentages\ndf |&gt;\n  # count obs per node\n  group_by(node) |&gt; mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  # count obs per group_node\n  group_by(group_node) |&gt; mutate(n_group = ifelse(is.na(group_node), NA, n())) |&gt; \n  ungroup() |&gt;\n  # add percentages\n  mutate(pct = n/n_group) -&gt; df\n\n# manually change order of nodes\ndf |&gt; mutate(\n  node = factor(node,\n                levels = c(\"Healthcare&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  ),\n  next_node = factor(next_node,\n                levels = c(\"Healthcare&lt;br&gt;startup\",\n                           \"Hard bounce\", \"Delivered\",\n                           \"Recipient did not open deck\", \"Recipient opened deck\",\n                           \"Recipient did not reply\", \"Recipient replied\",\n                           \"Negative reply\", \"Positive reply\",\n                           \"other\", \"geography\", \"no investments\", \"stage\", \"investment strategy\", \"industry\", \"no specific reason\",\n                           \"more info/clarification\", \"meeting\", \"formal application\", \"updates\", \"referral\"\n                )\n  )\n) -&gt; df\n\n# make sankey plot\n#\n# first change the data so that percentages are displayed in the node texts\ndf |&gt; tidyr::drop_na(node) |&gt;\n  mutate(\n    pct = ifelse(is.na(pct), 9999, pct), # dummy-replace NA with 9999, otherwise later ifelse does not work\n    node_text = ifelse(\n      # group_node and thus pct empty (here: 9999)\n      pct == 9999,\n      # yes, pct empty -&gt; no percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s&lt;/span&gt;\",\n                        node, scales::comma(n)),\n      # no, pct not empty -&gt; add percentages\n      sprintf(\"&lt;span style='font-weight:bold; font-family:Roboto Condensed'&gt;%s&lt;/span&gt;&lt;br&gt;&lt;span style='color:gray25'&gt;N = %s (%s%%)&lt;/span&gt;\",\n                        node,\n                        scales::comma(n),\n                 ifelse(pct&lt;.01, paste0(\"0\",weights::rd(pct*100,2)), weights::rd(pct*100,2)))\n      )) |&gt;\n  # now create the plot\n  ggplot(aes(x = x, \n             next_x = next_x, \n             node = node, \n             next_node = next_node,\n             fill = factor(node),\n             label = node_text,\n             color = factor(node)\n             )) +\n  geom_sankey(flow.alpha = 0.65, show.legend = FALSE, node.color = blue_dark, node.fill = blue_dark) +\n  geom_sankey_label_richtext(size = 3, color = \"black\", fill = \"white\") +\n  labs(x = element_blank()) +\n  # apply customized theme\n  theme_sankey_ipsum_rc(base_size = 11, plot_margin = margin(5, 5, 5, 5)) -&gt; p\n\n# more customizing\n#\n# now: change the color of the segments\n# to this end, first decompose the plot into its parts using `ggplot_build`\nq &lt;- ggplot_build(p)\n\n# first data layer is for line color of flows\nl1 &lt;- q$data[[1]]$colour\n# second data layer is for line color of nodes\nl2 &lt;- q$data[[2]]$colour\n\n# fill colors\nf1 &lt;- q$data[[1]]$fill # flows\nf2 &lt;- q$data[[2]]$fill # nodes\n\n# relevant flows are all of length 600, and only starting color value is relevant\n# thus, color change points (ccp) are\nccp &lt;- seq(1, length(f1), by = 600)\nq$data[[1]]$fill[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$fill[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$fill[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$fill[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$fill[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$fill[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$fill[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$fill[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$fill[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$fill[ccp[16:20]] &lt;- blue_main # positive categories\n\nq$data[[1]]$colour[ccp[1]] &lt;- blue_light # hard bounce\nq$data[[1]]$colour[ccp[2]] &lt;- blue_main # delivered\nq$data[[1]]$colour[ccp[3]] &lt;- blue_light # deck not opened\nq$data[[1]]$colour[ccp[4]] &lt;- blue_main # deck opened\nq$data[[1]]$colour[ccp[5]] &lt;- blue_light # no reply\nq$data[[1]]$colour[ccp[6]] &lt;- blue_main # reply\nq$data[[1]]$colour[ccp[7]] &lt;- blue_light # negative reply\nq$data[[1]]$colour[ccp[8]] &lt;- blue_main # positive reply\nq$data[[1]]$colour[ccp[9:15]] &lt;- blue_light # negative categories\nq$data[[1]]$colour[ccp[16:20]] &lt;- blue_main # positive categories\n\n# put all back together and plot the modified, final plot\np_mod &lt;- ggplot_gtable(q)\nplot(p_mod)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Software startup\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Healthcare startup\n\n\n\n\n\n\n\nFigure 1: Sankey diagrams of the field experiment flow"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#main-results",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#main-results",
    "title": "Field Experiment",
    "section": "6.2 Main results",
    "text": "6.2 Main results\nFigure 2 shows the main results visually. Figure 2 (a) shows the results for the opening duration of the pitch decks, Figure 2 (b) for the percentage of pitch decks slides that were viewed, and Figure 2 (c) for the share of positive investor reactions. Note that the significance brackets represent post-hoc contrasts with Holm (1979) correction (** p &lt; .01; *** p &lt; .001).\n\n\nCode\n# Helper functions and aesthetics\n#\n# pval function\npval &lt;- function(p, stars = TRUE){\n  if(stars){\n    if(p &lt; .001) return(\"***\")\n    if(p &lt; .01) return(\"**\")\n    if(p &lt; .05) return(\"*\")\n    if(p &gt;= .05) return(\"NS\")\n  } else{\n    scales::pvalue(p, prefix = c(\"p &lt; \", \"p = \", \"p &gt; \"))\n  }\n}\n\n# theme settings\nmy_style &lt;- list(hrbrthemes::theme_ipsum_rc(),\n                 scale_fill_manual(values=c(blue_light, blue_dark)))\nmy_theme &lt;- theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = .5),\n        axis.title.y = element_text(size = 12, hjust = .5, margin = margin(t = 0, r = 10, b = 0, l = 0)),\n        axis.title.x = element_text(size = 12, hjust = .5, margin = margin(t = 5, r = 0, b = 0, l = 0)),\n        )\n# set up titles, axis names etc.\nmy_labs &lt;- labs(\n  x = \"Quality\", \n  shape='Fluency'\n)\n\n\n# Main figures: software startup\n#\n# data prep\n#\n# for convenience and to not interfere with later code, we work on a copy of\n# the data\nd_soft &lt;- d_sw\n# convert fluency and quality to factor vars\nd_soft$fluency &lt;- factor(d_soft$fluency, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\nd_soft$quality &lt;- factor(d_soft$quality, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\n\n# create dataset for post-hoc contrasts\n# --Note: We create a special dataset since we need to change the reference\n#         level of the factor variables before switching to effect coding to \n#         keep the direction of the effects as intended\nd_soft_analysis &lt;- d_soft\nd_soft_analysis$fluency &lt;- relevel(d_soft_analysis$fluency, ref = 2)\nd_soft_analysis$quality &lt;- relevel(d_soft_analysis$quality, ref = 2)\n# switch to effect coding\ncontrasts(d_soft_analysis$fluency) &lt;- contr.sum # High = 1, Low = -1\ncontrasts(d_soft_analysis$quality) &lt;- contr.sum\n\n# FIGURE FOR DURATION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(duration ~ fluency * quality, d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\nduration_1 &lt;- ggplot(d_soft, aes(x=quality, y=duration, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Duration (in seconds)\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  coord_cartesian(ylim=c(0, 220)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = 175,\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = c(210),\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR COMPLETION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(completion ~ fluency * quality, d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\ncompletion_1 &lt;- ggplot(d_soft, aes(x=quality, y=completion, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Completion percentage\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, 1)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .8,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .85,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR POSITIVE REACTIONS\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(glm(positive ~ fluency * quality,\n                           family = binomial(link = \"logit\"),\n                           data = d_soft_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\npositive_1 &lt;- ggplot(d_soft, aes(x=quality, y=positive, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Positive reactions\", title = \"Software startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, .25)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .125,\n    tip_length = .002,\n    textsize = 6,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .16,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n# Main figures: healthcare startup\n#\n# data prep\n#\n# for convenience and to not interfere with later code, we work on a copy of\n# the data\nd_health &lt;- d_hc\n# convert fluency and quality to factor vars\nd_health$fluency &lt;- factor(d_health$fluency, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\nd_health$quality &lt;- factor(d_health$quality, levels = c(\"low\", \"high\"), labels = c(\"Low\", \"High\"))\n\n# create dataset for post-hoc contrasts\n# --Note: We create a special dataset since we need to change the reference\n#         level of the factor variables before switching to effect coding to \n#         keep the direction of the effects as intended\nd_health_analysis &lt;- d_health\nd_health_analysis$fluency &lt;- relevel(d_health_analysis$fluency, ref = 2)\nd_health_analysis$quality &lt;- relevel(d_health_analysis$quality, ref = 2)\n# switch to effect coding\ncontrasts(d_health_analysis$fluency) &lt;- contr.sum # High = 1, Low = -1\ncontrasts(d_health_analysis$quality) &lt;- contr.sum\n\n# FIGURE FOR DURATION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(duration ~ fluency * quality, d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\nduration_2 &lt;- ggplot(d_health, aes(x=quality, y=duration, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Duration (in seconds)\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  coord_cartesian(ylim=c(0, 220)) +\n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = 130,\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = c(155),\n    textsize = 6,\n    tip_length = .001,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR COMPLETION\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(lm(completion ~ fluency * quality, d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\ncompletion_2 &lt;- ggplot(d_health, aes(x=quality, y=completion, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Completion percentage\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, 1)) +\n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .8,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .875,\n    textsize = 6,\n    tip_length = .01,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n# FIGURE FOR POSITIVE REACTIONS\n#\n# post-hoc contrasts (using Holm's correction) via the `emmeans` package\npairs(emmeans::emmeans(glm(positive ~ fluency * quality,\n                           family = binomial(link = \"logit\"),\n                           data = d_health_analysis),\n                       ~ quality * fluency), adjust = \"holm\") |&gt;\n  broom::tidy() |&gt; select(adj.p.value) |&gt; pull() -&gt; temp\n\n# contrast: low quality: high vs. low fluency\np1 &lt;- temp[5]\n# contrast: high quality: high vs. low fluency\np2 &lt;- temp[2]\n\n# plot\npositive_2 &lt;- ggplot(d_health, aes(x=quality, y=positive, fill=fluency)) +\n  stat_summary(fun = \"mean\", geom=\"bar\", position=position_dodge(.91)) +\n  stat_summary(fun.data = mean_cl_normal, fun.args=list(mult=2), geom=\"errorbar\", width=.08, linewidth=.9,\n               position=position_dodge(.91)) + \n  my_style + my_theme + my_labs +\n  labs(y=\"Positive reactions\", title = \"Healthcare startup\", fill = \"Fluency\") +\n  # theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent) +\n  coord_cartesian(ylim=c(0, .25)) +  \n  guides(fill = guide_legend(reverse=TRUE)) +\n  geom_signif(\n    # p1\n    xmin = c(0.775), xmax = c(1.225),\n    y_position = .1425,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p1),\n    color = \"gray45\",\n    vjust = .5\n    ) +\n  geom_signif(\n    # p2\n    xmin = c(1.775), xmax = c(2.225),\n    y_position = .225,\n    textsize = 6,\n    tip_length = .002,\n    annotations = pval(p2),\n    color = \"gray45\",\n    vjust = .5\n  )\n\n\n\n# Final (combined) figures\n#\n# duration\nduration_1 + duration_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Opening duration of the pitch decks\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n# completion\ncompletion_1 + completion_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Percentage of pitch deck slides being viewed\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n  # tag_levels = 'A'\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n# positive reactions\npositive_1 + positive_2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n  title = \"Share of positive investor reactions\",\n  subtitle = \"as a function of quality and fluency (Study 1)\",\n  caption = \"Note: Error bars indicate 95% confidence intervals around the mean. Significance brackets represent post-hoc contrasts with Holm (1979) correction.\",\n  # tag_levels = 'A'\n) & theme(\n  plot.title = element_text(size = 18, family = \"Roboto Condensed\", face = \"bold\"),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(hjust=.5, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 0)),\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Opening duration. *** p &lt; .001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Completion percentage. ** p &lt; .01; *** p &lt; .001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Positive reactions. ** p &lt; .01; *** p &lt; .001.\n\n\n\n\n\n\n\nFigure 2: Results of the field experiment"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#descriptives-3",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#descriptives-3",
    "title": "Field Experiment",
    "section": "7.1 Descriptives",
    "text": "7.1 Descriptives\nAfter having applied the exclusion restrictions, there were 944 replies to the 21,152 emails we sent (21.25%) for the software startup. Of these 944 replies, 346 replies were positive (36.65% of replies).\nFor the healthcare startup, there were 757 replies to the 13,335 emails we sent (31.45%). Of these 757 replies, 240 replies were positive (31.70% of replies).\nTable 14 shows a breakdown of the reply rate and the positive reactions per fluency and quality condition.\n\n\nCode\n# combine data\npos_given_reply_data &lt;- bind_rows(list(\n  Software = d_sw |&gt; select(Fluency = fluency, Quality = quality, reply, positive_given_reply, investments, type, gender, location) |&gt; mutate(n_mails = n_mails_sw),\n  Healthcare = d_hc |&gt; select(Fluency = fluency, Quality = quality, reply, positive_given_reply, investments, type, gender, location) |&gt; mutate(n_mails = n_mails_hc)),\n  .id = \"Startup\")\n# convert Startup to factor, change order of levels\npos_given_reply_data$Startup &lt;- factor(pos_given_reply_data$Startup, levels = c(\"Software\", \"Healthcare\"))\n\n# create table\npos_given_reply_data |&gt; \n  group_by(Startup, Fluency, Quality) |&gt; \n  summarize(\n    `Clicks` = scales::comma(n()),\n    `Replies ` = scales::comma(sum(reply)),\n    `% Reply Rate ` = sprintf('%.2f', mean(reply)*100),\n    `Pos. Reaction ` = scales::comma(sum(positive_given_reply, na.rm=T)),\n    `% Pos. Reaction Rate ` = sprintf('%.2f', mean(positive_given_reply, na.rm=T)*100),\n  ) |&gt; kbl() |&gt; kable_styling() |&gt;\n  row_spec(c(4), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 14: Breakdown of positive email reactions per startup, fluency, and quality condition\n\n\n\n\n\n\n\nStartup\nFluency\nQuality\nClicks\nReplies\n% Reply Rate\nPos. Reaction\n% Pos. Reaction Rate\n\n\n\n\nSoftware\nhigh\nhigh\n1,091\n245\n22.46\n133\n54.29\n\n\nSoftware\nhigh\nlow\n1,080\n238\n22.04\n91\n38.24\n\n\nSoftware\nlow\nhigh\n1,126\n233\n20.69\n94\n40.34\n\n\nSoftware\nlow\nlow\n1,146\n228\n19.90\n28\n12.28\n\n\nHealthcare\nhigh\nhigh\n589\n209\n35.48\n101\n48.33\n\n\nHealthcare\nhigh\nlow\n605\n196\n32.40\n57\n29.08\n\n\nHealthcare\nlow\nhigh\n593\n180\n30.35\n59\n32.78\n\n\nHealthcare\nlow\nlow\n620\n172\n27.74\n23\n13.37"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#results",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#results",
    "title": "Field Experiment",
    "section": "7.2 Results",
    "text": "7.2 Results\nWe re-estimated the binary logit models (with control variables) reported in Table 7 and Table 12, now predicting whether there was a positive reaction to the emails conditional on investors having replied. Thus, for this analysis, the sample is a subset of the full sample (i.e., only investors who replied to the emails, N = 944 software startup, N = 757 healthcare startup). Table 15 shows the results of these models for the software and healthcare startup.\n\n\nCode\nd &lt;- pos_given_reply_data\n\n# 2 tables witch 2 colmuns each version\n\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$Fluency)\nd$quality &lt;- as.factor(d$Quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n\n# run regressions\nglm_tbl(glm(positive_given_reply ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d |&gt; filter(Startup == \"Software\")), coef_digits = 3) -&gt; temp_positive_given_reply_sw\nglm_tbl(glm(positive_given_reply ~ fluency * quality + investments + type + gender + location, family = binomial(link = \"logit\"), d |&gt; filter(Startup == \"Healthcare\")), coef_digits = 3) -&gt; temp_positive_given_reply_hc\n\n# put results together\ntemp &lt;- bind_cols(temp_positive_given_reply_sw,\n                  temp_positive_given_reply_hc[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Investment experience\",\n               \"Investor type [Venture Capital]\", \"Gender [Male]\", \"Country [Brazil]\",\n               \"Country [Canada]\", \"Country [China]\", \"Country [France]\",\n               \"Country [Germany]\", \"Country [India]\", \"Country [Israel]\",\n               \"Country [Singapore]\", \"Country [United Kingdom]\",\n               \"Fluency × Quality\", \"Tjur's R&lt;sup&gt;2&lt;/sup&gt;\"))\n\n# change order of rows: put interaction after main effects\ntemp |&gt; arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\")) -&gt; temp\n# change column names\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"z\", \"p\"), 2))\n\n# create final table\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccc\") |&gt; kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Software Startup\" = 4, \"Healthcare Startup\" = 4)) |&gt;\n  row_spec(c(4,16), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 15: Binary logit models of positive reactions conditional on investors having replied\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Startup\n\n\nHealthcare Startup\n\n\n\n\nCoeff.\nSE\nz\np\nCoeff.\nSE\nz\np\n\n\n\n\nIntercept\n-1.664\n1.145\n-1.45\n.146\n0.328\n1.465\n0.22\n.823\n\n\nFluency\n0.519\n0.077\n6.75\n&lt; .001\n0.407\n0.087\n4.66\n&lt; .001\n\n\nQuality\n0.558\n0.077\n7.26\n&lt; .001\n0.504\n0.087\n5.77\n&lt; .001\n\n\nFluency × Quality\n-0.236\n0.077\n-3.08\n.002\n-0.079\n0.087\n-0.91\n.364\n\n\nInvestment experience\n0.005\n0.016\n0.30\n.766\n-0.038\n0.064\n-0.59\n.555\n\n\nInvestor type [Venture Capital]\n-0.111\n0.187\n-0.59\n.553\n-0.140\n0.198\n-0.71\n.478\n\n\nGender [Male]\n0.063\n0.248\n0.26\n.798\n-0.628\n0.253\n-2.49\n.013\n\n\nCountry [Brazil]\n1.098\n1.231\n0.89\n.372\n-0.263\n1.521\n-0.17\n.863\n\n\nCountry [Canada]\n1.375\n1.830\n0.75\n.452\n-0.766\n1.563\n-0.49\n.624\n\n\nCountry [China]\n0.580\n1.292\n0.45\n.654\n-0.162\n1.491\n-0.11\n.913\n\n\nCountry [France]\n1.295\n1.220\n1.06\n.288\n-0.269\n1.505\n-0.18\n.858\n\n\nCountry [Germany]\n0.644\n1.214\n0.53\n.596\n-0.733\n1.567\n-0.47\n.640\n\n\nCountry [India]\n1.055\n1.247\n0.85\n.397\n0.053\n1.678\n0.03\n.975\n\n\nCountry [Israel]\n1.636\n1.280\n1.28\n.201\n-0.327\n1.538\n-0.21\n.832\n\n\nCountry [Singapore]\n0.818\n1.166\n0.70\n.483\n-0.582\n1.464\n-0.40\n.691\n\n\nCountry [United Kingdom]\n0.956\n1.133\n0.84\n.399\n-0.565\n1.443\n-0.39\n.695\n\n\nTjur's R2\n.102\n\n\n\n.084"
  },
  {
    "objectID": "replication_reports/Replication_Report_4_Field_Experiment.html#footnotes",
    "href": "replication_reports/Replication_Report_4_Field_Experiment.html#footnotes",
    "title": "Field Experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTwo independent raters blind to our hypotheses rated all email replies as to whether the content demonstrates investor interest. For the software startup, the interrater agreement was 97.16% (Cohen’s κ = 0.939). For the healthcare startup, the interrater agreement was 96.45% (Cohen’s κ = 0.919). All cases for which the raters did not agree were resolved by the authors of this paper.↩︎"
  },
  {
    "objectID": "replication_reports/Replication_Report_1_Survey.html",
    "href": "replication_reports/Replication_Report_1_Survey.html",
    "title": "VC and BA Survey",
    "section": "",
    "text": "1 Introduction\nTo investigate investors’ perceptions of visual design, we conducted a survey with 188 venture capitalists (VCs) and business angels (BAs). They were first asked to independently assess the importance of content quality and visual design. Subsequently, they were required to directly compare the relative importance of the two factors.\nIn what follows, we will shortly describe the results of the survey. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\n# setup\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n# further packages that are loaded on demand are:\n# - here\n# - readr\n\n# set option to disable showing the column types when loading data with `readr`\noptions(\"readr.show_col_types\" = FALSE)\n\n\n\n\n2 Data\nNote that in this report, we load the de-identified and anonmyzed dataset. Please consult the online repository for the code that processed the raw data.\n\n\nCode\ndata_dir &lt;- 'replication_reports/data'\n\n# -----------------------------------------------------------------------------\n#\n# Getting and preparing the datasets\nd &lt;- readr::read_csv(here::here(data_dir, 'Survey.csv'))\n\n# make variable names more coding friendly and easier to understand\nd &lt;- d |&gt;\n  rename(duration_study = `Duration (in seconds)`,\n         relative = relative_1,\n         investment_experience = inv_exp,\n         gender = `gender `\n         )\n\n# convert gender to factor\nd$gender &lt;- factor(d$gender)\n\n\n\n\n3 Descriptives\nTable 1 gives a demographic overview of the dataset, and the importance ratings are shown in Table 2.\n\n\nCode\nd |&gt; \n  summarize(N = n(),\n            Age = round(mean(age, na.rm = T), 1),\n            `% Female`= round(prop.table(table(gender))[\"Female\"]*100, 1),\n            `Years Invest. Exp.` = round(mean(d$investment_experience), 1)\n            ) |&gt; kbl(align = rep('c', 4))\n\n\n\n\nTable 1: Demographic overview of survey data\n\n\n\n\n\n\nN\nAge\n% Female\nYears Invest. Exp.\n\n\n\n\n188\n35.7\n36.2\n5.8\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nd |&gt; summarize(N = n(), Mean = mean(content), SD = sd(content), Min = min(content), Max = max(content)) -&gt; imp_content\nd |&gt; summarize(N = n(), Mean = mean(design), SD = sd(design), Min = min(design), Max = max(design)) -&gt; imp_design\nd |&gt; summarize(N = n(), Mean = mean(relative), SD = sd(relative), Min = min(relative), Max = max(relative)) -&gt; imp_relative\n\nbind_rows(imp_content, imp_design, imp_relative, .id = \"Outcome\") -&gt; res_importance\nres_importance$Outcome &lt;- c(\"Content\", \"Design\", \"Relative\")\n\nres_importance |&gt; kbl(digits = 2, format.args = list(decimal.mark = \".\", big.mark = \",\")) |&gt; kable_styling()\n\n\n\n\nTable 2: Importance ratings for content quality, visual design, and their relative tradeoff\n\n\n\n\n\n\nOutcome\nN\nMean\nSD\nMin\nMax\n\n\n\n\nContent\n188\n6.32\n0.71\n3\n7\n\n\nDesign\n188\n5.77\n1.04\n2\n7\n\n\nRelative\n188\n74.64\n22.71\n0\n100\n\n\n\n\n\n\n\n\n\n\nWhen asked to rate the importance of content quality and visual design independently, the VCs and BAs rated their importance similarly high (content quality: M = 6.32, SD = 0.71; visual design: M = 5.77, SD = 1.04). However, when forced to make a direct comparison of the relative importance, the investors rated content to be about 3 times more important than design (74.64% vs. 25.36%; SD = 22.71).\n\n\n4 Plot\nFigure 1 summarizes the results of the survey.\n\n\nCode\n# define colors\nblue_main &lt;- \"#297FB8\"\nblue_dark &lt;- \"#2D3E50\"\nblue_light &lt;- \"#A0ABBF\"\n\n# create new dataset for plotting which has transformed means that add up to 100\n# (to be used in a stacked bar plot)\n\n# calculate means\nd |&gt; summarise(mean_content = mean(content), mean_design = mean(design)) -&gt; means\n# transform means\nmeans |&gt; mutate(mean_content_100 = mean_content / (mean_content + mean_design) * 100,\n                mean_design_100 = mean_design / (mean_content + mean_design) * 100) -&gt; means\n# reshape data for plotting\ndata.frame(category = factor(c(\"Content Quality\", \"Visual Design\")),\n           importance = c(means$mean_content_100, means$mean_design_100),\n           importance_raw = c(means$mean_content, means$mean_design)) -&gt; plot_data\n# change reference level of category\nplot_data$category &lt;- relevel(plot_data$category, ref = \"Visual Design\")\n\n# create plot (importance ratings of content and design)\np1 &lt;- ggplot(plot_data, aes(x = \"\", y = importance_raw, fill = category)) +\n  geom_bar(stat = \"identity\", width = .5, position = \"fill\") +\n  geom_text(aes(label = round(importance_raw, 2)), position = position_fill(vjust = 0.75), size = 5.5, color = c(\"white\", \"black\"), family = \"Roboto Condensed\") +\n  geom_text(aes(label = category), position = position_fill(vjust = 0.35), size = 5.5, color = c(\"white\", \"black\"), family = \"Roboto Condensed\") +\n  scale_fill_manual(values = c(blue_light, blue_dark)) +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Importance Ratings\") +\n  coord_flip()\n\n\n# create second dataset for plotting relative importance\ndata.frame(category = c(\"Priority of Content over Design\", \"\"),\n           priority = c(mean(d$relative), 100 - mean(d$relative))) -&gt; plot_data2\n# create plot (relative importance)\np2 &lt;- ggplot(plot_data2, aes(x = \"\", y = priority, fill = category)) +\n  geom_bar(stat = \"identity\", width = .5, position = \"fill\") +\n  geom_text(aes(label = c(round(priority, 2)[1], \"\")), position = position_fill(vjust = 0.785), size = 5.5, color = \"white\", family = \"Roboto Condensed\") +\n  geom_text(aes(label = category), position = position_fill(vjust = 0.385), size = 5.5, color = \"white\", family = \"Roboto Condensed\") +\n  scale_fill_manual(values = c(\"gray90\", blue_main)) +\n  theme_void() +  \n  theme(legend.position = \"none\") +\n  labs(title = \"Relative Importance\") +\n  coord_flip()\n\n# combine plots (needs package patchwork loaded)\np1 / p2 + plot_layout(guides = 'collect') +\n  plot_annotation(\n    caption = paste0(\"Data:\\nN = \",\n                     nrow(d),\n                     \" VCs and BAs rated how important content quality and visual design are for their initial evaluation\\n\", \n                     \"(scale 1–7). They were also asked about the relative importance of content over design (scale 0–100).\")\n) & theme(\n  plot.title = element_text(hjust = .055, size = 18, family = \"Roboto Condensed\", face = \"bold\", margin = margin(t = 18, r = 0, b = -35, l = 0)),\n  plot.subtitle = element_text(size = 14, family = \"Roboto Condensed\"),\n  plot.caption = element_text(size = 12, hjust= 0, family = \"Roboto Condensed\", margin = margin(t = -10, r = 0, b = 0, l = 38)),\n  aspect.ratio = 0.2\n)\n\n\n\n\n\n\n\n\nFigure 1: Survey results about the importance of content quality and visual design for investors’ initial evaluation"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_3_Online_Experiments.html",
    "href": "replication_reports/backup/Replication_Report_3_Online_Experiments.html",
    "title": "Judging a Pitch by its Cover",
    "section": "",
    "text": "For both our fictitious startups (Software: PerkSouq; Healthcare: Brachytix), we ran online experiments to determine how the (stated) likelihood to invest depends on the substantive quality and visual fluency of a pitch deck.\nSpecifically, we first ran a 2x2 between-subjects experiment for our software startup. To make sure that the instruction we used when measuring investment likelihood (“Based on the information at hand, what is the probability […]”) did not bias the results, we ran a replication of this first experiment with a modified instruction (“Based on the pitch deck, what is the probability […]”). To ensure generalizability across domains, we then ran the same experiment for our healthcare startup.\nWe ran all online experiments on Qualtrics, hosted the pitch decks on DocSend, and recruited the participants via Prolific. For details, see the corresponding AsPredicted pre-registrations listed in Table 1.\n\n\n\nTable 1: Overview Pre-Registrations\n\n\n\n\n\nStartup\nPre-Reg Date\nAsPredicted #\nData Collection Start\n\n\n\n\nSoftware startup\n28-11-2022\n114380\n30-11-2022\n\n\nSoftware startup (Replication)\n16-12-2022\n116872\n17-12-2022\n\n\nHealthcare startup\n20-12-2022\n117215\n21-12-2022\n\n\n\n\n\n\nIn what follows, we will give an overview of the results and robustness checks, followed by figures that summarize the results of the experiments. As this report is dynamically created with R and Quarto, we also report all code. However, for the sake of readability, the code itself is folded by default and only the results relevant for the questions at hand are shown.\n\n\nCode\noptions(knitr.kable.NA = '')\n\n# setup\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(ggsignif)\nlibrary(ggtext)\nlibrary(patchwork)\nlibrary(kableExtra)\noptions(knitr.kable.NA = '',\n        kable_styling_bootstrap_options = c(\"striped\", \"condensed\", \"responsive\"))\n\n# further packages that are loaded on demand are:\n# - supernova\n# - car\n# - rstatix\n# - forcats\n# - weights\n# - broom\n# - scales\n# - readr\n# - tidyr\n# - hrbrthemes\n# - emmeans\n# - grid\n\n\n# Custom functions\n#\n# negate %in%\n`%notin%` &lt;- Negate(`%in%`)\n#\n# extract t-test results and Cohen's d and put the results together as a table\nttest_tbl &lt;- function(formula, data, alternative = \"two.sided\", ...){\n  # first, check for homogeneous group variances using Levene's test\n  # --&gt; if significant, use Welch's t-test (i.e., var.equal = FALSE)\n  # note that we use a significance level of .05 for Levene's test, as pre-registered\n  # we check if the p-value is not significant (i.e., p &gt;= .05) and save this\n  # information var.equal --&gt; thus, we can use 'var.equal = var.equal' in the t-test\n  var.equal &lt;- car::leveneTest(formula, data = data)$`Pr(&gt;F)`[1] &gt;= .05\n  # perform t-test\n  tres &lt;- t.test(formula, data = data, var.equal = var.equal, alternative = alternative)\n  # extract Cohen's d\n  dres &lt;- rstatix::cohens_d(formula, data = data, var.equal = var.equal)\n  # construct p-value\n  pval &lt;- ifelse(tres$p.value &lt; .001, \" &lt; .001\", weights::rd(tres$p.value, 3))\n  # extract dependent variable\n  dv &lt;- stringr::str_match(deparse(formula), '[^ ~]*')\n  # construct return df\n  df = data.frame(DV = NA, condition=rep(NA, 2), N = NA, Mean = NA, SD = NA, test_statistic = NA, p = NA, d = NA)\n  # fill values\n  df$DV[1] &lt;- stringr::str_to_sentence(dres$`.y.`)\n  df$condition &lt;- c(dres$group1, dres$group2)\n  df$N &lt;- c(dres$n1, dres$n2)\n  df$Mean &lt;- weights::rd(aggregate(formula, data = data, FUN = mean)[,2], 2)\n  df$SD &lt;- weights::rd(aggregate(formula, data = data, FUN = sd)[,2], 3)\n  df$test_statistic[1] &lt;- paste0(\"t(\",\n                                 ifelse(var.equal == TRUE, tres$parameter,\n                                        weights::rd(tres$parameter, 1)),\n                                 \") = \",\n                                 sprintf('%.2f', tres$statistic))\n  df$p[1] &lt;- pval\n  df$d[1] &lt;- weights::rd(dres$effsize, 2)\n  return(df)\n}\n#\n# extract ANOVA results including eta squared and put the results together as a table\n# -Note: needs the supernova library installed\nanova_tbl &lt;- function(formula, data, type = 3, ...){\n  # perform ANOVA\n  d &lt;- supernova::supernova(lm(as.formula(deparse(formula)), data = data), type = type)$tbl\n  # check whether ANOVA is univariate or factorial\n  univariate &lt;- all(d$term %in% c(\"Model\", \"Error\", \"Total\"))\n  # get rows of interest\n  if(univariate) {\n    effect_rows &lt;- d$term %notin% c(\"Error\", \"Total\")\n  } else {\n    effect_rows &lt;- d$term %notin% c(\"Model\", \"Error\", \"Total\")\n  }\n  # extract key parameters\n  effect &lt;- d$term[effect_rows]\n  MSE &lt;- round(d$MS[effect_rows], 2)\n  df &lt;- d$df[effect_rows]\n  df_res &lt;- d$df[d$term == \"Error\"]\n  statistic &lt;- round(d$F[effect_rows], 2)\n  pval &lt;- ifelse(d$p[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$p[effect_rows], 3))\n  eta &lt;- ifelse(d$PRE[effect_rows] &lt; .001, \" &lt; .001\", weights::rd(d$PRE[effect_rows], 3))\n  # construct return df\n  return(data.frame(effect, MSE, df, df_res, statistic, pval, eta))\n}\n# extract GLM results and put the results together as a table\nglm_tbl &lt;- function(model, coef_digits = 2, coef_bold = TRUE, p_threshold = 0.05, ...){\n  # extract model parameters\n  if(\"tobit\" %in% class(model)){ # tobit model -&gt; broom::tidy does not work\n    res &lt;- parameters::model_parameters(model)\n    res &lt;- res[c(\"Parameter\", \"Coefficient\", \"SE\", \"z\", \"p\")]\n    names(res) &lt;- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n    # res[] &lt;- lapply(res, function(x) { attributes(x) &lt;- NULL; x })\n  } else {\n    res &lt;- broom::tidy(model)\n  }\n  pvals &lt;- res$p.value\n  res$estimate &lt;- sprintf(paste0(\"%.\", coef_digits, \"f\"), res$estimate)\n  res$std.error &lt;- sprintf(\"%.3f\", res$std.error)\n  res$statistic &lt;- sprintf(\"%.2f\", res$statistic)\n  # format p value\n  res$p.value &lt;- ifelse(res$p.value &lt; .001, \" &lt; .001\", weights::rd(res$p.value, 3))\n  # make estimates bold if below critical p value\n  if(coef_bold){\n    res$estimate[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$estimate[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$std.error[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$std.error[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$statistic[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$statistic[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n    res$p.value[pvals &lt; p_threshold] &lt;- paste0(\"&lt;b&gt;\", res$p.value[pvals &lt; p_threshold], \"&lt;/b&gt;\")\n  }\n  # bind R2 and Adj. R2 to model parameters\n  r2 &lt;- performance::r2(model) # extract R2\n  end &lt;- nrow(res) + seq_len(length(r2))\n  res[end,\"term\"] &lt;- names(r2)\n  res[end,\"estimate\"] &lt;- weights::rd(unlist(r2), digits = 3)\n  # return result\n  return(res)\n}"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_3_Online_Experiments.html#main-analyses",
    "href": "replication_reports/backup/Replication_Report_3_Online_Experiments.html#main-analyses",
    "title": "Judging a Pitch by its Cover",
    "section": "4.1 Main analyses",
    "text": "4.1 Main analyses\nTable 5 shows for each experiment the results of a factorial ANOVA that models stated investment likelihood as a function of the visual fluency condition and the substantive quality condition. Note that type III sum of squares were used, and the proportional reduction in error is reported as effect size (ηp2 in the table).\n\n\nCode\n# combine data\nd &lt;- bind_rows(list(\n  Software = d_sw,\n  `Software Replication` = d_sw_repl,\n  Healthcare = d_hc),\n  .id = \"Startup\")\n\n# convert Startup to factor, change order of levels\nd$Startup &lt;- factor(d$Startup,\n                    levels = c(\"Software\", \"Software Replication\", \"Healthcare\"),\n                    labels = c(\"Software\", \"Software (replication)\", \"Healthcare\"))\n\n# convert conditions into factors\nd$fluency &lt;- as.factor(d$fluency)\nd$quality &lt;- as.factor(d$quality)\n\n# relevel factor levels\nd$quality &lt;- relevel(d$quality, 'high')\nd$fluency &lt;- relevel(d$fluency, 'high')\n\n# convert factor into effect coding\ncontrasts(d$fluency) &lt;- contr.sum\ncontrasts(d$quality) &lt;- contr.sum\n\n\n# combine results\ntemp &lt;- bind_rows(\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software\")),\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software (replication)\")),\n  anova_tbl(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Healthcare\"))\n  )\n\ntemp |&gt;\n  mutate(effect = rep(c(\"Fluency\", \"Quality\", \"Fluency × Quality\"), 3)) |&gt;\n  kbl(col.names = c(\"\", \"MSE\", \"df\", \"df&lt;sub&gt;res&lt;/sub&gt;\", \"F\", \"p\", \"η&lt;sub&gt;p&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;\"),\n        align = \"lcccccc\", escape = FALSE) |&gt;\n  kable_styling() |&gt; \n  pack_rows(index = c(\"Software Startup\" = 3,\n                      \"Software Startup (Replication)\" = 3,\n                      \"Healthcare Startup\" = 3),\n            label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0,3,6,9), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 5: ANOVA results for stated investment likelihood\n\n\n\n\n\n\n\n\nMSE\ndf\ndfres\nF\np\nηp2\n\n\n\n\nSoftware Startup\n\n\nFluency\n559.81\n1\n193\n0.77\n.380\n.004\n\n\nQuality\n3055.37\n1\n193\n4.22\n.041\n.021\n\n\nFluency × Quality\n232.63\n1\n193\n0.32\n.571\n.002\n\n\nSoftware Startup (Replication)\n\n\nFluency\n202.33\n1\n196\n0.27\n.604\n.001\n\n\nQuality\n8433.54\n1\n196\n11.28\n&lt; .001\n.054\n\n\nFluency × Quality\n30.14\n1\n196\n0.04\n.841\n&lt; .001\n\n\nHealthcare Startup\n\n\nFluency\n2451.79\n1\n189\n3.35\n.069\n.017\n\n\nQuality\n3813.02\n1\n189\n5.21\n.024\n.027\n\n\nFluency × Quality\n1104.61\n1\n189\n1.51\n.221\n.008"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_3_Online_Experiments.html#sec-mc",
    "href": "replication_reports/backup/Replication_Report_3_Online_Experiments.html#sec-mc",
    "title": "Judging a Pitch by its Cover",
    "section": "4.2 Manipulation checks",
    "text": "4.2 Manipulation checks\nIn this section, we report the results of the manipulation checks for visual fluency and substantive quality (software startup replication and healthcare startup experiments). We conducted t-tests for each manipulation check, with either perceived fluency or perceived quality as the dependent variable and the visual fluency condition or the substantive quality condition as independent variables. The results are shown in Table 6 (a) for perceived fluency and Table 6 (b) for perceived quality. Note that we ran either Student’s or Welch’s t-test based on the result of Levene’s test for homogeneous group variances.\n\n\nCode\n# perceived fluency\nbind_rows(\n  ttest_tbl(fluency_perception ~ fluency, d |&gt; filter(Startup == \"Software (replication)\")),\n  ttest_tbl(fluency_perception ~ fluency, d |&gt; filter(Startup == \"Healthcare\"))\n) |&gt;\n  mutate(DV = rep(c(\"Perceived fluency\", \"\"), 2)) |&gt;\n  kbl(col.names = c(\"Outcome\", \"Fluency Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"), escape = FALSE) |&gt;\n  kable_styling() |&gt;\n  pack_rows(index = c(\"Software Startup (Replication)\" = 2, \"Healthcare Startup\" = 2), label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0, 2), extra_css = \"border-bottom: 1px solid\")\n# perceived quality\nbind_rows(\n  ttest_tbl(quality_perception ~ quality, d |&gt; filter(Startup == \"Software (replication)\")),\n  ttest_tbl(quality_perception ~ quality, d |&gt; filter(Startup == \"Healthcare\"))\n) |&gt;\n  mutate(DV = rep(c(\"Perceived quality\", \"\"), 2)) |&gt;\n  kbl(col.names = c(\"Outcome\", \"Quality Condition\", \"N\", \"Mean\", \"SD\", \"t-test\", \"p\", \"Cohen's d\"), escape = FALSE) |&gt;\n  kable_styling() |&gt;\n  pack_rows(index = c(\"Software Startup (Replication)\" = 2, \"Healthcare Startup\" = 2), label_row_css = \"text-align: center;\") |&gt;\n  row_spec(c(0, 2), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 6: Manipulation checks (software startup replication and healthcare startup)\n\n\n\n\n\n\n\n(a) Perceived fluency\n\n\n\n\n\n\nOutcome\nFluency Condition\nN\nMean\nSD\nt-test\np\nCohen's d\n\n\n\n\nSoftware Startup (Replication)\n\n\nPerceived fluency\nhigh\n101\n63.80\n26.711\nt(198) = 2.03\n.043\n.29\n\n\n\nlow\n99\n55.83\n28.749\n\n\n\n\n\nHealthcare Startup\n\n\nPerceived fluency\nhigh\n103\n58.79\n28.051\nt(191) = 1.99\n.048\n.29\n\n\n\nlow\n90\n50.33\n30.871\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Perceived quality\n\n\n\n\n\n\nOutcome\nQuality Condition\nN\nMean\nSD\nt-test\np\nCohen's d\n\n\n\n\nSoftware Startup (Replication)\n\n\nPerceived quality\nhigh\n107\n4.73\n1.225\nt(198) = 2.34\n.020\n.33\n\n\n\nlow\n93\n4.33\n1.155\n\n\n\n\n\nHealthcare Startup\n\n\nPerceived quality\nhigh\n97\n5.23\n.952\nt(191) = 3.48\n&lt; .001\n.50\n\n\n\nlow\n96\n4.70\n1.153"
  },
  {
    "objectID": "replication_reports/backup/Replication_Report_3_Online_Experiments.html#sec-robustness",
    "href": "replication_reports/backup/Replication_Report_3_Online_Experiments.html#sec-robustness",
    "title": "Judging a Pitch by its Cover",
    "section": "4.3 Robustness checks",
    "text": "4.3 Robustness checks\nAs a robustness check, we estimated linear regression models in which we predicted the stated investment likelihood by visual fluency, substantive quality, age, gender, investment experience and the aesthetic sensitivity (CVPA composite score). In this regression, we again used effect coding for the two factors visual fluency and substantive quality. That is, for both variables, we coded low as -1 and high as +1. Table 7 shows the results of this regression.\n\n\nCode\n# -- Note: We comment out Levene's tests of homogeneous group variances here for\n#          readability. However, we adapt the function call of the regression\n#          based on the result of Levene's tests.\n\n\n# Levene's test of homogeneous group variances -&gt; if not, use robust::lmRob() below\n# \n# # software startup\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Software\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Software\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software\")) # not significant\n# # software startup replication\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Software (replication)\")) # not significant\n# # healthcare startup\n# car::leveneTest(invest_prob ~ fluency, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n# car::leveneTest(invest_prob ~ quality, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n# car::leveneTest(invest_prob ~ fluency * quality, d |&gt; filter(Startup == \"Healthcare\")) # not significant\n\n# recode Likert text scores into numeric values for the CVPA score\nd |&gt; mutate_at(vars(cvpa_v_1:cvpa_r_3), dplyr::recode,\n                   'Strongly disagree' = 1,\n                   'Disagree' = 2,\n                   'Neither agree nor disagree' = 3,\n                   'Agree' = 4,\n                   'Strongly agree' = 5) -&gt; d\n# create CVPA composite score\nd &lt;- d |&gt; rowwise() |&gt; mutate(CVPA = mean(cvpa_v_1:cvpa_r_3))\n\n# group \"Non-binary\" and \"Prefer not to say\" gender categories into one\nd$gender &lt;- forcats::fct_collapse(d$gender, Female = \"Female\", Male = \"Male\", other_level = \"Other\")\n\n# run regression\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Software\"))) -&gt; lm_rob_sw\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Software (replication)\"))) -&gt; lm_rob_sw_repl\nglm_tbl(lm(invest_prob ~ fluency * quality + age + gender + invest_experience + CVPA,\n             d |&gt; filter(Startup == \"Healthcare\"))) -&gt; lm_rob_hc\n\n# add empty row for `genderOther` in the first and third regression\nlm_rob_sw |&gt; add_row(.before = 6)  -&gt; lm_rob_sw\nlm_rob_hc |&gt; add_row(.before = 6)  -&gt; lm_rob_hc\n\n\ntemp &lt;- bind_cols(lm_rob_sw, lm_rob_sw_repl[,-1], lm_rob_hc[,-1]) |&gt;\n  mutate(term = c(\"Intercept\", \"Fluency\", \"Quality\", \"Age\",\n               \"Gender [Male]\", \"Gender [Other]\",\n               \"Investment experience\", \"Aesthetic sensitivity\",\n               \"Fluency × Quality\", \"R&lt;sup&gt;2&lt;/sup&gt;\", \"R&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;adj.&lt;/sub&gt;\")) |&gt;\n  arrange(term %notin% c(\"Intercept\", \"Fluency\", \"Quality\", \"Fluency × Quality\"))\nnames(temp) &lt;- c(\"\", rep(c(\"Coeff.\", \"SE\", \"t\", \"p\"), 3))\ntemp |&gt; \n  kbl(escape = FALSE, align = \"lcccccccccccc\") |&gt; \n  kable_styling() |&gt;\n  add_header_above(c(\" \" = 1, \"Software Startup\" = 4, \"Software Startup (Replication)\" = 4, \"Healthcare Startup\" = 4)) |&gt;\n  row_spec(c(0,9), extra_css = \"border-bottom: 1px solid\")\n\n\n\n\nTable 7: Robustness checks: Linear regressions for stated investment likelihood with individual-level control variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Startup\n\n\nSoftware Startup (Replication)\n\n\nHealthcare Startup\n\n\n\n\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\nCoeff.\nSE\nt\np\n\n\n\n\nIntercept\n34.99\n10.432\n3.35\n&lt; .001\n4.86\n10.538\n0.46\n.645\n37.19\n10.670\n3.49\n&lt; .001\n\n\nFluency\n-2.12\n1.781\n-1.19\n.236\n1.54\n1.831\n0.84\n.401\n3.59\n1.875\n1.91\n.057\n\n\nQuality\n4.48\n1.790\n2.50\n.013\n6.18\n1.831\n3.38\n&lt; .001\n4.27\n1.891\n2.26\n.025\n\n\nFluency × Quality\n-0.01\n1.804\n-0.01\n.996\n-0.34\n1.847\n-0.18\n.855\n1.51\n1.883\n0.80\n.423\n\n\nAge\n-0.47\n0.165\n-2.82\n.005\n-0.01\n0.165\n-0.03\n.975\n-0.30\n0.182\n-1.65\n.102\n\n\nGender [Male]\n-3.90\n3.703\n-1.05\n.294\n0.32\n4.029\n0.08\n.938\n-2.06\n4.045\n-0.51\n.611\n\n\nGender [Other]\n\n\n\n\n-2.19\n13.260\n-0.17\n.869\n\n\n\n\n\n\nInvestment experience\n-0.12\n0.299\n-0.39\n.695\n-0.06\n0.294\n-0.21\n.830\n-0.14\n0.399\n-0.34\n.732\n\n\nAesthetic sensitivity\n8.80\n1.981\n4.44\n&lt; .001\n11.79\n2.028\n5.82\n&lt; .001\n8.58\n2.047\n4.19\n&lt; .001\n\n\nR2\n.188\n\n\n\n.202\n\n\n\n.164\n\n\n\n\n\nR2adj.\n.157\n\n\n\n.168\n\n\n\n.133"
  }
]